[{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/introduction/introduction/","title":"Introduction to MeVisLab","summary":"Tutorial Introduction Welcome to MeVisLab!\nMore than 20 years of experience and the continuous implementation of adaptations made MeVisLab one of the most powerful development platforms for medical image processing. Several applications and their prototypes are based on and could be realized because of MeVisLab, including software assistants for neuro-imaging, dynamic image analysis, surgery planning, and cardiovascular analysis.\nMeVisLab is a development environment for rapid prototyping and product development of medical and industrial imaging applications. It includes a Software Development Kit (SDK) and an ApplicationBuilder for deploying your applications to end-customers.\n","content":"Tutorial Introduction Welcome to MeVisLab!\nMore than 20 years of experience and the continuous implementation of adaptations made MeVisLab one of the most powerful development platforms for medical image processing. Several applications and their prototypes are based on and could be realized because of MeVisLab, including software assistants for neuro-imaging, dynamic image analysis, surgery planning, and cardiovascular analysis.\nMeVisLab is a development environment for rapid prototyping and product development of medical and industrial imaging applications. It includes a Software Development Kit (SDK) and an ApplicationBuilder for deploying your applications to end-customers.\nIn turn, the MeVisLab SDK consists of an Integrated Development Environment (IDE) for visual programming and the advanced text editor MATE for Python scripting, providing code completion, debugging, profiling, and automated test development as well as execution.\nYou can reuse thousands of predefined Modules for image processing (2D up to 6D images) and visualization, combine them, or even build your own.\nA quick introduction on available modules and example networks will be given in the following tutorials.\nStructure and Usage of Provided Tutorials This tutorial is a hands-on training. You will learn about basic mechanics and features of MeVisLab.\nStarting with this introduction, we will lead you through all relevant aspects of the user interface, commonly used functionalities, and provide you with all the basic knowledge you need to build your own web applications.\nAdditional information is accessible through embedded links, forwarding you to a related glossary entry or tutorial and shortcuts, advice and hints will be highlighted as shown here.\nThe tutorials are divided into chapters by their topic and each chapter contains at least one example for you to try. You find them at the end of the tutorial or, also sorted by chapters, under the menu entry Examples. The examples under the designated menu entry are more suitable if you already have a little experience and rather search for inspiration than for explanations.\nStarting MeVisLab for the First Time Right after installation of MeVisLab, you will find some new icons on your Desktop (if selected during setup).\nMeVisLab Desktop Icons (Windows) Use the top middle icon to start the MeVisLab IDE. You can also start the integrated text editor MATE or the ToolRunner. For this tutorial, you will generally require the IDE.\nWarning:\u0026nbsp; Maybe postpone the usage of the QuickStart icons as they can cause created packages not to be loaded. MeVisLab IDE User Interface First, start the MeVisLab IDE. After showing a Welcome Screen, the standard user interface opens.\nMeVisLab IDE User Interface Workspace By default, MeVisLab starts with an empty workspace.\nThis is where you will develop and edit networks. Essentially, networks form the base of all processing and visualization pipelines, so the workspace is where the visual programming is done.\nViews Area The standard Views Area contains the Output Inspector and Module Inspector. With the help of the Output Inspector, you can visualize the modules output.\nInfo:\u0026nbsp; Further information on each module, e.g., about module parameters, can be found using the Module Inspector. Debug Output Debugging information can be found using the Debug Output.\nThe MeVisLab IDE and its layout are completely configurable. You can rearrange the items and add new views via [ Main Menu \u0026rarr; View \u0026rarr; Views ].\nFile Types Used in, for, and With MeVisLab Extension Description .mlab Network file, includes all information about the networks modules, their settings, their connections, and module groups. Networks developed using the MeVisLab SDK are stored as .mlab files and can only be opened having a valid SDK license. .def Module definition file, necessary for a module to be added to the common MeVisLab module database. May also include all MDL script parts (if they are not sourced out to the .script file). .script MDL script file, typically includes the user interface definition of panels. See Chapter GUI Development for an example on GUI programming. .mlimage MeVisLab internal image format for 6D images saved with all DICOM tags, lossless compression, and in all data types. .mhelp File with descriptions of all fields and possible use cases of a module, edit- and creatable by using MATE. See Help files for details. .py Python file, used for scripting in macro modules. See Python scripting for an example on macro programming. .dcm DCM part of the imported DICOM file, see Importing DICOM Data. Module Types Info:\u0026nbsp; Modules are the basic entities the MeVisLab concept is built upon. They provide the functionalities to process, display, and interact with images. The three existing module types (ML, Open Inventor, and macro module) can be distinguished by their colors:\nType Appearance Characteristics ML module (blue) ML module Page-based, demand-driven processing of voxels. Open Inventor module (green) Open Inventor module Visual scene graphs (3D). Usually starting with So (for Scene object) as a naming convention. Macro module (brown) Macro module Combination of other module types, allowing the implementation of hierarchies and scripted interaction. Invalid Modules If a module is invalid, it is displayed in bright red. This might happen if the module itself is not available for your system.\nAppearance Explanation Invalid module Invalid module Macro State Invalid Macro containing an invalid module As you can see, the number of warning and error messages that are being printed to the debug console are listed in the upper right corner of the module. This is intentional, as it enables the developer to quickly find the module causing the errors.\nCheck:\u0026nbsp; Once the debug console is cleared, the warning and error indicators next to the module are also cleared. Informational messages are indicated in a similar manner on the same spot, but in a subtle gray color.\nModule Interactions Through the Context Menu Each module has a context menu, providing the following options:\nContext Menu of a module Show Internal Network: Macro modules provide an entry to open the internal network. You can see what happens inside a macro module. The internal network may also contain other macro modules. Show Window: If a module does not provide a user interface, you will see the automatic panel showing the module\u0026rsquo;s name. Modules may additionally have one or more windows that can be opened. You can also open the Scripting Console of a module to integrate Python. Instance Name: You can edit or copy the instance name. Renaming can be useful if the same module appears more than once in one network and/or if you want to access and distinguish the modules in your Python script. Help: The menu entry Help provides access to the Module Help pages and to an example network where the module is used. This example network often helps to understand which additional modules can be added to create your desired effect. Extras: Automated tests written for the specific module can be executed here. You can also run this module in a separate process. Reload Definition: In the case you are currently working on a module, you may need to reload the definition, so that your changes are applied on the module (for example, attached Python scripts). Related Files: Related files allows a quick access to the modules .script or .py files. The files are automatically opened in MATE for editing. Show Enclosing Folder: This entry opens the directory where your module is stored. Grouping: Multiple modules can be clustered and the groups can be named. This adds clarity to the structure of your network. In addition to that, grouped modules can be converted to local or global macro modules easily. Input and Output Connectors As the creation of a network requires connected modules, each module has input and output connectors, located on their top and bottom side. Data is generally transmitted from the output connector on the top side of one module to the input connector on another module\u0026rsquo;s bottom side.\nOnce again, three types can be distinguished:\nAppearance Shape Definition Triangle - ML Image triangle ML images Circle - Inventor Scene half-circle Inventor scene Square - Base Object square Base objects: Pointers to data structures Info:\u0026nbsp; A connection can be established by dragging one module close to the other. Some modules even contain hidden connectors in addition to the ones displayed on the module\u0026rsquo;s surface. Click on the workspace and press SPACE to see the hidden connectors as well as internal networks of each macro module. You can now also use the hidden connectors for building connections.\nFor more information about connectors and different types of connections, click here . If you want to know more about establishing, removing, moving, and replacing connections, have a look at this. Parameter Connections Besides through a module\u0026rsquo;s input and output connectors, connections can also be established between parameters in the module\u0026rsquo;s panel.\nCheck:\u0026nbsp; An exemplary use case for a parameter connection is synchronization. Have a look here. Macro Modules Info:\u0026nbsp; The creation of macros is explained in more detail in Tutorial Chapter I - Example 2.2 Adding Modules to Your Workspace There are several ways to add a module to your current network:\nvia the menu bar entry [ Modules ] via [ Quick Search ] via the View Module Search via the View Module Browser via copy and paste from another network by scripting, see the Scripting Reference Both the menu entry[ Modules ] and the Module Browser display all available modules. The modules are sorted hierarchically by topic and name, as defined in the file Genre.def.\nTherefore, both places are a good starting point when in need of a specific function, like an ImageLoad module.\nModules Menu and Module Browser The advantage of the Module Browser is that you can right-click the entries, open the context menu and, for example, open the help (in your default Internet browser) or the module files (in MATE, the built-in text editor).\nCheck:\u0026nbsp; For a module to be listed, it has to be available in the SDK or in your self-defined packages. A detailed tutorial on how to create packages can be found here. If in doubt or missing something, check out the loaded packages in the preferences. Usually the quickest way to add modules to a network is the quick search in the menu bar. It offers the possibility to search for modules by module name. By default, the search will also be extended to keywords and substrings and is case-insensitive. To change these settings, click the magnifier button for the search options.\nQuick Search Options Info:\u0026nbsp; Any time you enter something in the MeVisLab GUI while not focussing a dialog window, your entry will be put into the quick search automatically. Use the \u0026uarr; ArrowUp and \u0026darr; ArrowDown keys on your keyboard to move to one of the listed modules. The module\u0026rsquo;s decription will appear next to it, allowing you to decide if this is the right module for your use case.\nQuick Search Results Tip:\u0026nbsp; For a more complex search, use the Module Search View. ","tags":["Tutorial","Introduction","Glossary","Modules","ML Module","Filetypes","UI","Workspace","Search"],"section":"introduction"},{"date":"1655276093","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/","title":"Chapter I: Basic Mechanisms of MeVisLab","summary":"Basic Mechanisms of MeVisLab (Example: Building a Contour Filter) In this chapter you will learn the basic mechanisms of the MeVisLab IDE. You will learn how to reuse existing modules to load and view data, and you will build your first processing pipeline.\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Extra Infos:\u0026nbsp; Additional information on the basics of MeVisLab are explained here Loading Data First, we need to load the data we would like to work on, e.g., a CT scan. In MeVisLab, modules are used to perform their associated specific task: they are the basic entities you will be working with. Each module has a different functionality for processing, visualization, and interaction. Connecting modules enables the development of complex processing pipelines. You will get to know different types of modules throughout the course of this tutorial.\n","content":"Basic Mechanisms of MeVisLab (Example: Building a Contour Filter) In this chapter you will learn the basic mechanisms of the MeVisLab IDE. You will learn how to reuse existing modules to load and view data, and you will build your first processing pipeline.\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Extra Infos:\u0026nbsp; Additional information on the basics of MeVisLab are explained here Loading Data First, we need to load the data we would like to work on, e.g., a CT scan. In MeVisLab, modules are used to perform their associated specific task: they are the basic entities you will be working with. Each module has a different functionality for processing, visualization, and interaction. Connecting modules enables the development of complex processing pipelines. You will get to know different types of modules throughout the course of this tutorial.\nStarting off, we will add the module ImageLoad to our network to load our data. The module can be found by typing its name into the search bar on the top-right corner and is added to your network by clicking it.\nSearch for ImageLoad Next, we select and load the data we\u0026rsquo;d like to process. Double-click the module ImageLoad to open its panel. You can browse through your folders to select the data you\u0026rsquo;d like to open. Example data can be found in the MeVisLab DemoData directory $(InstallDir)/Packages/MeVisLab/Resources/DemoData located in the MeVisLab installation path. Select a file, for example, an MRI scan of a shoulder Shoulder_Fracture.tif. The image is loaded immediately and basic information of the loaded image can be seen in the Panel.\nExtra Infos:\u0026nbsp; There also are modules to load multiple other formats of data. These are the most common ones:\nDicomImport to load DICOM Images LocalImage to load any image format For a more detailed description on loading DICOM images, see here\nThe Output Inspector and the Module Inspector To inspect and visualize the loaded data, we can use the Output Inspector located in the Views area. You can already interact with the image using the mouse wheel and mouse buttons / . To preview the image, click on the triangle on the top side of the module ImageLoad, which offers the module\u0026rsquo;s output. All module outputs can be found at the top side of the respective module.\nYou can now inspect your image in 2D:\nOutput Inspector Your image does not look like this? One reason might be that the slice of the image you are looking at has no information. Click on the Output Inspector and scroll through the slices (this process is called \u0026ldquo;Slicing\u0026rdquo;) by using the mouse wheel . Still not seeing anything? Then, try to adjust the contrast of the given image by keeping the right mouse button pressed while moving the mouse.\nYou are not restricted to 2D. The Output Inspector offers a 3D View of most loaded images. Try to click on the 3D tab located in the Output Inspector. The 3D display of the image can be rotated by left-clicking on the image and moving the courser around. The little cube in the lower right corner of the viewer shows the orientation of the image.\nNotation:\u0026nbsp; A = anterior, front P = posterior, back R = right side L = left side H = head F = feet Below the Output Inspector, you\u0026rsquo;ll find the Module Inspector. The Module Inspector displays properties and parameters of the selected module. Parameters are stored in so called Fields. Using the Module Inspector, you can examine different fields of your ImageLoad module. The module has, for example, the fields filename (the path the loaded image is stored in), as well as sizeX, sizeY, and sizeZ (the extent of the loaded image).\nModule Inspector Viewer Instead of using the Output Inspector to inspect images, we\u0026rsquo;d suggest to add another viewer to the network. Search for the module View2D and add it to your workspace. Most modules have different connector options. Data is generally transmitted from the top side of a module to another modules bottom side.\nThe module View2D has one input connector for voxel images (triangle-shaped) and three other possible input connectors (shaped like half-circles) on the bottom. The half-circle-shaped input connectors will be explained later on. Generally, module outputs can be connected to module inputs with the same symbol and thus transmit information and data between those modules.\n2D Viewer You can now display the loaded image in the newly added viewer module by connecting the output of the module ImageLoad to the input connector of the module View2D. Follow these steps to do so:\nClick the output connector of ImageLoad.\nKeep the left mouse button pressed while dragging the connection to the input connector of View2D (white line).\nCheck if the connection is well-defined (green line).\nRelease the mouse button on the input connector of your View2D module to establish the connection.\nEstablish connection Although the connection is established, no image rendering has started yet. To initialize rendering, open the View2D panel by double-clicking on the module. Similar to the Output Inspector, you can scroll through the slices and set different levels of contrast. The amount of displayed annotations is altered by pressing A on the keyboard (annotation-mode).\nView2D Panel By dragging the connection away from either the input or the output connector, the connection is interrupted.\nConnections between compatible outputs and inputs are established automatically if two modules get close enough to each other.\nExtra Infos:\u0026nbsp; Connecting, Disconnecting, Moving, and Replacing Connections is explained in more detail here Image Processing An average kernel will be used to smooth the image as our next step will be to actually process our image. Add the Convolution module to your workspace and disconnect the View2D module from the ImageLoad module by clicking on the connection and pressing DEL . Now, you can build new connections from the module ImageLoad to the module Convolution and the Convolution module to View2D.\nConvolution Module Open the panel of the Convolution module by double-clicking it. The panel allows configuration of the module. You can adjust parameters or select a kernel. We will be using the 3x3 Average Kernel for now.\nSelect a Kernel The module View2D is now displaying the smoothed image.\nTo compare the processed and unprocessed image, click on the output connector of the module ImageLoad to display the original image in the Output Inspector. The Output Inspectors greatest advantage is that it\u0026rsquo;s able to display the output of any connector in the process chain (as long as an interpretable format is used). Simply click the connector or connection to find out more about the module\u0026rsquo;s output.\nYou can also inspect changes between processed (output connector) and unprocessed (input connector) images by adding a second or even third viewer to your network. \u0026ldquo;Layers\u0026rdquo; of applied changes can be inspected next to each other using more than one viewer and placing as well as connecting them accordingly. We will be using a second View2D module. Notice how the second viewer is numbered for you to be able to distinguish them better. It might be important to know at this point that numerous connections can be established from one output connector but an input connector can only receive one stream of data. Connect the module ImageLoad to the second viewer to display the images twice. You can now scroll through the slices of both viewers and inspect the images.\nMultiple Viewers Parameter Connection for Synchronization You\u0026rsquo;re now able to scroll through the slices of the image in two separate windows. To examine the effect of the filter even better, we will now synchronize both viewers.\nWe already know data connections between module inputs and outputs. Besides module connections, it is also possible to connect the fields within the panels of the modules via parameter connection. The values of connected fields are synchronized, which means that the changing value of one field will be adapted to all other connected fields.\nIn order to practice establishing parameter connections, add the SyncFloat module to your workspace.\nSyncFloat Module We will be synchronizing the startSlice fields of our viewers to be able to directly compare the effect our processing module has on the slices: Right-click the viewer View2D to open its context menu and select [ Show Window \u0026rarr; Automatic Panel ].\nAutomatic Panel View2D Doing so shows all parameter fields of the module View2D.\nSearch for the field startSlice. The field indicates which slice is currently shown in the viewer. If you scroll through the slices of an image, the value of startSlice changes.\nNow, double-click the module SyncFloat to open its panel.\nClick on the label startSlice in the automatic panel of the module View2D, keep the button pressed, and drag the connection to the label Float1 in the panel of the module SyncFloat.\nSynchronize StartSlice The connection is drawn as a thin gray arrow between both modules with the arrowhead pointing to the module that receives the field value as input. The value of the field startSlice is now transmitted to the field Float1. Changing startSlice automatically changes Float1, but not the other way round.\nParameter Connection StartSlice We will now establish a connection from the module SyncFloat to the second viewer, Viewer2D1. In order to do that, open the automatic panel View2D1. Draw a connection from the label Float2 of the panel of the module SyncFloat to the label startSlice in the automatic panel of the module View2D1. Lastly, implement a connection between the parameter fields startSlice of both viewers. Draw the connection from View2D1 to View2D.\nSynchronize both directions As a result, scrolling through the slices with the mouse wheel in one of the viewers synchronizes the rendered slice in the second viewer. In this case, you can inspect the differences between smoothed and unsmoothed data on every single slice.\nYour final Network It is also possible to use the predefined module SynchroView2D to accomplish a similar result.(SynchroView2D\u0026rsquo;s usage is described in more detail in this chapter ).\nGrouping Modules A contour filter can be created based on our previously created network. To finalize the filter, add the modules Arithmetic2 and Morphology to your workspace and connect the modules as shown below. Double-click the module Arithmetic2 to open its panel. Change the field Function of the module Arithmetic2 to use the function subtract in the panel of the module. The contour filter is done now. You can inspect each processing step using the Output Inspector by clicking on the input and output connectors of the respective modules. The final results can be displayed using the viewer modules. If necessary, adjust the contrast by pressing the right mouse button and moving the cursor.\nGrouping modules If you\u0026rsquo;d like to know more about specific modules, search for help. You can do this by right-clicking the module and select help, which offers an example network and further information about the selected module in particular.\nModule Help To be able to better distinguish the image processing pipeline, you can encapsulate it in a group: select the three modules, for example, by dragging a selection rectangle around them. Then, right-click the selection to open the context menu and select [ Add to New Group ].\nAdd to new group Enter a name for the new group, for example, Filter. The new group is created and displayed as a green rectangle. The group allows for quick interactions with all its modules.\nYour Filter Group Your network got very complex and you lost track? No problem. Let MeVisLab arrange your modules automatically via [ Mein Menu \u0026rarr; Edit \u0026rarr; Auto Arrange Selection ] (or via keyboard shortcut CTRL + 1 ).\nNow, it is time to save your first network. Open the tab [ File \u0026rarr; Save ] to save the network in an .mlab file.\nExtra Infos:\u0026nbsp; More information on module groups can be found here Macro Modules You have probably already noticed how the modules differ in color. Each color represents another type of module:\nBlue modules are called ML modules: they process voxel images. Green modules are Open Inventor modules: they enable visual 3D scene graphs. Brown modules are called macro modules. Macro modules encapsulate a whole network in a single module. To condense our filter into one single module, we will now be creating a macro module out of it. To do that, right-click on the group\u0026rsquo;s title and select Convert To Local Macro. Name your new macro module and finish. You just created a local macro module. Local macros can only be used from networks in the same or any parent directory.\nConvert to local macro Your first local macro Right-click the macro module and select Show Internal Network to inspect and change the internal network. You can change the properties of the new macro module by changing the properties in the internal network. You can, for example, right-click the module Convolution and change the kernel. These changes will be preserved.\nInternal Network of your local macro \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Extra Infos:\u0026nbsp; Module handling is explained here\nMore information on macro modules can be found here\nSummary MeVisLab provides predefined modules you can reuse and connect for building more or less complex networks. Each module\u0026rsquo;s output can be previewed using the Output Inspector. Each module provides example networks to explain their usage. Parameters of each module can be changed in the Module Inspector or automatic panel of the module. Parameter connections can be established to synchronize the values of these parameters. Modules can be clustered. Clustered modules can be encapsulated into local or global macro modules. Macro modules encapsulate networks. Internal networks can be shown and modified. Any changes of the internal network are applied to the macro module on-the-fly, changes in the .mlab file change the permanent behavior of your module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Macro","Macro modules","Local Macro"],"section":"tutorials"},{"date":"1655276093","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/dataimport/","title":"Example 1: Data Import in MeVisLab","summary":"Example 1: Data Import in MeVisLab MeVisLab provides several predefined modules to import data for processing in your networks.\nExtra Infos:\u0026nbsp; The easiest way to load data in MeVisLab is to drop the file onto the MeVisLab workspace. MeVisLab will try to find a module that is capable of loading your file automatically. These chapters explain the data formats and modules related to this example:\n","content":"Example 1: Data Import in MeVisLab MeVisLab provides several predefined modules to import data for processing in your networks.\nExtra Infos:\u0026nbsp; The easiest way to load data in MeVisLab is to drop the file onto the MeVisLab workspace. MeVisLab will try to find a module that is capable of loading your file automatically. These chapters explain the data formats and modules related to this example:\nImages DICOM Data Segmentations / 2D Contours 3D Data / Meshes Example files and images can be found in your MeVisLab installation directory under Packages \u0026gt; MeVisLab \u0026gt; Resources \u0026gt; DemoData\nExtra Infos:\u0026nbsp; Detailed explanations on loading images onto your MeVisLab workspace can be found here Images A good option to load images is the ImageLoad module. ImageLoad Module The ImageLoad module can import the following formats:\nDICOM TIFF DICOM/TIFF RAW LUMISYS PNM Analyze PNG JPEG MLImageFileFormat Basic information of the imported images is available on the panel that opens via double-click.\nDICOM Data Extra Infos:\u0026nbsp; Additional information about Digital Imaging and Communications in Medicine (DICOM) can be found at Wikipedia Even if the above explained ImageLoad is able to import DICOM data, a much better way is to use one of the specialized modules for DICOM images, such as DicomImport.\nThe DicomImport module allows to define a directory containing DICOM files to import as well as a list of files that can be dropped to the UI and imported. After import, the volumes are shown in a patient tree providing the following patient, study, series, and volume information (depending on the availability in the DICOM file(s)):\nPATIENT LEVEL Patient Name (0010,0010) - Patient Birthdate (0010,0030) STUDY LEVEL Study Date (0008,0020) - Study Description (0008,1030) SERIES/VOLUME LEVEL Modality (0008,0060) - Series Description (0008,103e) - Rows (0028,0010) - Columns (0028,0011) - number of slices in volume - number of timepoints in volume DicomImport Module Configuration The DicomImport module generates volumes based on the Dicom Processor Library (DPL) that allows to define sorting and partitioning options.\nDicomImport Sort Part Configuration DicomTree Information In order to get all DICOM tags from your currently imported and selected volume, you can connect the DicomImport module to a DicomTagBrowser.\nDicomTagBrowser Module In MeVisLab versions later than 4.2.0, the Output Inspector provides the option to show the DICOM tags of the currently selected output directly. You do not need to add a separate DicomTagBrowser module anymore.\nDICOM Information in Output Inspector Segmentations / 2D Contours Two-dimensional contours in MeVisLab are handled via CSOs (Contour Segmentation Objects).\nExtra Infos:\u0026nbsp; Tutorials for CSOs are available here\nDetailed explanations about CSOs can be found here\nThe CSO library provides data structures and modules for an interactive or automatic generation of contours in voxel images. Furthermore, these contours can be analyzed, maintained, grouped, and converted into a voxel image or a set of markers.\nCSOs can be created by the existing SoCSO*Editor modules. The following modules are available:\nSoCSOPointEditor SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor SoCSODistancePolylineEditor SoCSOEllipseEditor SoCSORectangleEditor SoCSOIsoEditor SoCSOSplineEditor SoCSOPolygonEditor SoCSOLiveWireEditor For saving and loading existing CSOs, the modules CSOSave and CSOLoad can be used.\n3D Data / Meshes Winged Edge Mesh (WEM) Three-dimensional meshes in MeVisLab are handled via WEMs (Winged Edge Mesh).\nThe module WEMLoad loads different 3D mesh file formats, for example:\nObject File Format (.off .geom) Wavefront (.obj) Polygon File Format (.ply) Standard Tessellation Language (.stl) VRML (.wrl) Winged Edge Mesh (.wem) WEMLoad Module WEMs can be rendered via Open Inventor by using the modules SoExaminerViewer or SoRenderArea and SoCameraInteraction.\nBefore visualizing a WEM, it needs to be converted to a scene object via SoWEMRenderer.\nSoWEMRenderer Module Extra Infos:\u0026nbsp; Tutorials for WEMs are available here. Loading Arbitrary 3D Files The SoSceneLoader module is able to load external 3D formats. MeVisLab uses the integrated assimp third-party library that is able to import most common 3D file types. The currently integrated assimp version can be found here Extra Infos:\u0026nbsp; Supported file formats of the assimp library are documented on their website. SoSceneLoader Module The SoSceneLoader module generates a 3D scene from your loaded files that can be rendered via SoExaminerViewer or SoRenderArea and SoCameraInteraction Extra Infos:\u0026nbsp; Example usage is explained in the tutorials for Open Inventor. ","tags":["Beginner","Tutorial","Data Import","DICOM"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/coordinatesystems/coordinatesystems/","title":"Example 1.1: MeVisLab Coordinate Systems","summary":"Example 1.1: MeVisLab Coordinate Systems Three coordinate systems exist next to each other:\nWorld coordinates Voxel coordinates Device coordinates World coordinate systems in MeVisLab are always right handed.\nThe blue rectangle shows the same region in the three coordinate systems.\nCoordinate Systems in MeVisLab World Coordinates World coordinates are:\nGlobal: Combine several objects in a view Isotropic: All directions are equivalent Orthogonal: Coordinate axes are orthogonal to each other The origin of the world coordinate system can be anywhere and is not clearly defined. Origins of the other coordinate systems can always be mapped to the world coordinate system. In the case of DICOM images, this mapping is defined by DICOM tags.\n","content":"Example 1.1: MeVisLab Coordinate Systems Three coordinate systems exist next to each other:\nWorld coordinates Voxel coordinates Device coordinates World coordinate systems in MeVisLab are always right handed.\nThe blue rectangle shows the same region in the three coordinate systems.\nCoordinate Systems in MeVisLab World Coordinates World coordinates are:\nGlobal: Combine several objects in a view Isotropic: All directions are equivalent Orthogonal: Coordinate axes are orthogonal to each other The origin of the world coordinate system can be anywhere and is not clearly defined. Origins of the other coordinate systems can always be mapped to the world coordinate system. In the case of DICOM images, this mapping is defined by DICOM tags.\nWorld Coordinates in MeVisLab You can show the world coordinates in MeVisLab by using the following example network:\nWorld Coordinates in MeVisLab The ConstantImage module generates an artificial image with a certain size, data type, and a constant fill value. The origin of the image is at the origin of the world coordinate system; therefore, the SoCoordinateSystem module shows the world coordinate system. In order to have a larger z-axis, open the panel of the ConstantImage module and set IMage Size for Z to 256.\nConstantImage Info Placing an object into the Open Inventor scene of the SoExaminerViewer, in this case a SoCube with width, height, and depth of 10, places the object to the origin of the world coordinate system.\nSoCube in world coordinate system Translations You can move an object in your scene, for example, by using a SoTranslation module. Update your network and add the module before your cube. Defining a translation vector (50, 0, 0) moves your cube by 50 in the x-direction based on the origin of the world coordinate system.\nSoTranslation Transformations More complex transformations can be done by using the SoTransform module. Not only can you translate an existing object, but you can also rotate, scale, and shear.\nSoTransform \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Voxel Coordinates Voxel coordinates are:\nRelative to an image Continuous from [0..x, 0..y, 0..z], voxel center at 0.5 Direct relation to voxel location in memory Voxel Coordinates in MeVisLab You can show the voxel coordinates in MeVisLab by using the following example network:\nVoxel Coordinates Load the file Liver1_CT_venous.small.tif. The Info module shows detailed information about the image loaded by the LocalImage. Opening the SoExaminerViewer shows the voxel coordinate system of the loaded image. You may have to change the LUT in SoGVRVolumeRenderer, so that the image looks better.\nVoxel coordinates of the loaded image The Advanced tab of the Info module shows the world coordinates of the image. In this case, the origin of the voxel coordinate system is located at (-186.993, -173.993, -249.993).\nIn addition to that, you can see a scaling that has been done on the image. The voxel sizes are shown in the diagonal values of the matrix as 3.985792, 3.985792, 3.985798.\nWorld coordinates of the loaded image You can change the scaling to 1 by adding a Resample3D module to the network: set the voxel size to (1, 1, 1) and inspect the Info module.\nResample3D Image Info after Resampling The voxel size is now 1.\nYou can add this network to the world coordinate system network developed above and see both coordinate systems.\nWarning:\u0026nbsp; Replace the SoGroup module from the World Group in your network by a SoSeparator. Additional details about the difference can be found here. World coordinates of the loaded image Opening the SoExaminerViewer shows the world coordinate system in white and the voxel coordinate system in yellow.\nWorld and Voxel coordinates On the yellow axis, we can see that the coordinate systems are located as already seen in the Info module Advanced tab. On the x-axis, the voxel coordinate origin is translated by -186.993 and on the y-axis, it is translated by -173.993.\nYou can also add a SoVertexProperty and a SoLineSet module and configure a line from the origin of the world coordinate system (0, 0, 0) to the origin of the voxel coordinate system as defined by the image (-186.993, -173.993, -249.993).\nSoVertexProperty \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Device Coordinates Device coordinates are:\n2D coordinates in OpenGL viewport Measured in pixel Have their origin (0, 0) in the top left corner of the device (with x-coordinates increasing to the right and y-coordinates increasing downward) The viewport is the rectangle in pixels on your screen you want to render to. Affine transformations map abstract coordinates from your scene to physical pixels on your device.\nAll triangular vertices go through a projection matrix and end in a normalized range from -1 to 1 representing your field of view. To find which pixels the triangles actually cover on screen, those coordinates get linearly remapped from [−1, 1] to the range of the viewport rectangle in pixels. Technically, that kind of mapping is called an affine transformation.\n","tags":["Beginner","Tutorial","Data Import","DICOM","Coordinate Systems"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/coordinatesystems/coordinatesystems2/","title":"Example 1.2: DICOM Coordinate Systems","summary":"Example 1.2: DICOM Coordinate Systems General Coordinate systems in DICOM are basically the same as world coordinates in MeVisLab (except for the 0.5 voxel offset). World coordinates also refer to the patient axes. They are:\nBased on the patient\u0026rsquo;s main body axes (transverse, coronal, sagittal) Measured as 1 coordinate unit = 1 millimeter Right-handed Not standardized regarding their origin World Coordinates in Context of the Human Body ","content":"Example 1.2: DICOM Coordinate Systems General Coordinate systems in DICOM are basically the same as world coordinates in MeVisLab (except for the 0.5 voxel offset). World coordinates also refer to the patient axes. They are:\nBased on the patient\u0026rsquo;s main body axes (transverse, coronal, sagittal) Measured as 1 coordinate unit = 1 millimeter Right-handed Not standardized regarding their origin World Coordinates in Context of the Human Body The Digital Imaging and Communications in Medicine (DICOM) standard defines a data format that groups information into data sets. This way, the image data is always kept together with all meta information like patient ID, study time, series time, acquisition data, etc. The image slice is represented by another tag with pixel information.\nDICOM tags have unique numbers, encoded as two 16-bit numbers, usually shown in hexadecimal notation as two four-digit numbers (xxxx,xxxx). These numbers are the data group number and the data element number.\nInfo:\u0026nbsp; Although DICOM is a standard, often the data that is received/recorded does not follow the standard. Wrongly used tags or missing mandatory tags may cause problems in data processing. Some typical modules for DICOM handling:\nDirectDicomImport is a module for DICOM import that generates 3D or 4D images (as ML images) from a list of DICOM files which can directly be used by other modules. It has a lot of options to control the import process, which can, e.g., determine which slices are combined into an image stack. DicomImport is a fast and more lightweight module for DICOM import. Its main advantage is that the import process is faster and happens asynchronously. You can view the the DICOM tags of a DICOM image or a processed ML image with the module DicomTagBrowser. You can view and cut out frame-specific tags with the module DicomFrameSelect. You can modify DICOM tags with the module DicomTagModify. You can also create a new DICOM header for an image file with the ImageSave module, tab Options, Save DICOM header file only. Saving of loaded DICOM data to the filesystem or sending to a Picture Archiving and Communication System (PACS) is possible with the DicomTool macro module. Basic support for querying and receiving DICOM data from a PACS is available via the DicomQuery and DicomReceiver modules. Info:\u0026nbsp; For handling and manipulating DICOM data in C++, the DICOM toolkit DCMTK (DICOM@offis) is recommended. Parts of this toolkit are also used in MeVisLab.\nAnother option for Python is pydicom.\nOrthogonal Views The module OrthoView2D provides a 2D view displaying the input image in three orthogonal viewing directions. By default, the view is configured as Cube where the transverse view is placed in the top right segment, sagittal in bottom left, and coronal in bottom right segment. Use the left mouse button to set a position in the data set. This position will be displayed in all available views and is available as field worldPosition.\nOrthoView2D As already learned in the previous example 1.1: MeVisLab Coordinate Systems, world and voxel positions are based on different coordinate systems. Selecting the top left corner of any of your views will not show a world position of (0, 0, 0). You can move the mouse cursor to the voxel position (0, 0, 0) as seen in the image information of the viewers in brackets (x, y, z). The field worldPosition then shows the location of the image in world coordinate system (see Info module).\nOrthoView2D Voxel- and World Position Another option is to use the module OrthoReformat3 that transforms the input image (by rotating and/or flipping) into the three main views commonly used:\nOutput 0: Sagittal view Output 1: Coronal view Output 2: Transverse view (aka Axial view) OrthoReformat3 The general View2D always uses the original view from the image data without reconstructing another view. In the case of ProbandT1, this is the sagittal view.\n","tags":["Beginner","Tutorial","Data Import","DICOM","Coordinate Systems"],"section":"tutorials"},{"date":"1747612800","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/","title":"Example 2: Macro Modules and Module Interaction","summary":"Example 2: Macro Modules What is a Macro Module? A macro module can be used to develop your own functionality in MeVisLab.\nLike all other standard MeVisLab modules, macro modules have a defined interface with inputs, outputs, and parameters (fields). This interface allows it to interact with other modules in a larger network. They hide the complexity of the internal network or Python code, presenting a more simplified and manageable unit to the user.\n","content":"Example 2: Macro Modules What is a Macro Module? A macro module can be used to develop your own functionality in MeVisLab.\nLike all other standard MeVisLab modules, macro modules have a defined interface with inputs, outputs, and parameters (fields). This interface allows it to interact with other modules in a larger network. They hide the complexity of the internal network or Python code, presenting a more simplified and manageable unit to the user.\nMacro modules are primarily defined using the MeVisLab Definition Language (MDL) and often incorporate Python scripting for added functionality, especially for dynamic user interfaces. Importantly, you don\u0026rsquo;t need to write C++ code to create them.\nThe internal network of a macro module is saved in an .mlab file, often referred to as the macro network. The interface and other definitions are stored in .def and .script files.\nYou have two main options for developing a macro module:\nWith Internal Networks: Use a macro module to reuse a network of modules. For example, if you build a network that applies a specific image filter and you want to use this setup in multiple projects, you can wrap the entire network into a single macro module. This way, you don’t need to manually reconnect all the individual modules each time — you just use your macro module. You can also add inputs and outputs to connect your internal network with other modules. An example can be found in chapter Basic Mechanics of MeVisLab (Example: Building a Contour Filter).\nWithout Internal Networks: Use a macro module to add your own Python code. If MeVisLab is missing a specific functionality, you can write your own Python script and execute it in a macro module. This allows you to extend the software with custom behavior that integrates smoothly into your project. You can see the Python code at the right side of the image below. The image shows these two options. The left side encapsulates a network of three modules into one macro module. The right side shows a macro module without an internal network, only containing your custom Python script.\nA typical example for macro modules without an internal network is the execution of a pretrained AI model on an input ML image, see Example 2: Brain Parcellation using PyTorch for details.\nIt is also possible to combine both approaches. You can add internal networks and additionally write Python code for user interaction and processing.\nInternal Processing and Python Interaction Benefits of Macro Modules Encapsulation: Macro modules take an existing network of modules or Python code. To the user interacting with the macro module, it appears as a single entity with its own defined inputs, outputs, and parameters. You don\u0026rsquo;t need to know or interact with the internal functionality unless you specifically open the macro module for editing. Reusability: Once created, macro modules can be easily added to different networks, saving time and effort in rebuilding common processing pipelines. See below for the scope of a macro module. It can be local or global. Organization and Clarity: Macro modules help to structure complex networks, making them easier to understand, maintain, and debug. By encapsulating a set of modules into one, you reduce visual clutter and focus on the higher-level workflow. Simplified Interface: The interface of a macro module can be more compact and user-friendly than the combined interfaces of all the modules within it. You can expose only the necessary parameters and inputs/outputs. Abstraction of Complexity: Users of a macro module don\u0026rsquo;t need to know the details of its internal workings. They interact with it through its well-defined interface. Custom Functionality: Macro modules allow you to create custom modules tailored to your specific needs by combining and configuring existing MeVisLab functionalities. GUI Development: They are often used to encapsulate dynamic user interfaces built with scripting, sometimes without any underlying image processing network. Scope of Macro Modules Local Macro Module A Local Macro module in MeVisLab exists within the context of the current network document - i.e., it’s defined locally rather than being installed into the global module database. It does not require a package. It lives inside the directory of the current network file (.mlab) you’re working on.\nA local macro is visible and editable in the directory of your current network. A local macro is not listed in the Modules panel and module search. A local macro can only be reused elsewhere by copying it into another folder with your network file. Global Macro Module A global macro module is stored in a central location within your MeVisLab installation. The directory is called Package. Once a global macro module is created, it appears in the module browser and can be used in any MeVisLab network you open. See Package creation for details about Packages.\nLocal macro modules can be converted to global macro modules. MeVisLab then adds a definition file containing the name and package of the module and copies the content to your selected package directory. Package directories are loaded automatically when you start MeVisLab in the case they have been added to your user packages via main menu [ Edit \u0026rarr; Preferences \u0026rarr; Packages ].\nA global macro can be used in any MeVisLab network. A global macro is listed in the Modules panel and module search. Info:\u0026nbsp; Packages are the way MeVisLab organizes different development projects. You can organize your own modules, test cases, or C++ modules in a package. Inputs, Outputs, and Parameter Fields Macro modules can have input and output connectors that receive data and/or provide the results of the processing performed by their internal networks or Python scripts.\nThey are typically defined in the macro module\u0026rsquo;s .script file.\nInputs Input connectors accept data from other modules in the network. These inputs define what information the encapsulated network or Python script within the macro module receives and processes.\nData input connectors, represented by triangles for ML images, half-circles for Open Inventor scenes, or squares for Base objects, receive data objects from other modules. The type of data an input accepts is determined by the modules within the macro that are connected to this input.\nOutputs Output connectors provide the results of the processing performed by their internal networks. These outputs can then be connected to the inputs of other modules.\nData Outputs (triangle, half-circle, square) provide the processed data from the internal network or Python file. The type of data an output provides depends on the outputs of the modules within the macro that are connected to this output.\nParameter Fields Parameter Fields allow users to control the behavior of the internal network. They can be connected to the parameters/fields of other modules or manually adjusted by the user. They also allow other modules to read values or states from within the encapsulated network or Python file.\nYou have two options when adding fields to your macro module:\nDefine your own fields: You can define your own fields by specifying their name, type, and default value in the .script file. This allows you to provide custom parameters for your macro module, tailored to your specific needs. These parameters can be use as input from the user or output from the modules processing. Reuse fields from the internal network: Instead of defining your own field, you can expose an existing field from one of the modules of your internal network. To do this, you reference the internalName of the internal field you want to reuse. This makes the internal field accessible at the macro module level, allowing users to interact with it directly without duplicating parameters. Changes of the field value are automatically applied in your internal network. Inputs, Outputs, and Fields Files Associated with a Macro Module Macro modules typically need the following files:\nDefinition file (.def): The module definition file contains the definition and information about the module like name, author, or package. Definition files are only available for global macro modules. Script file (.script): The script file defines inputs, outputs, parameter fields, and the user interface of the macro module. In the case you want to add Python code, it includes the reference to the Python file. The .script file allows you to define short Python functions to be called on field changes and user interactions. user interface and the internal interface Python file (.py): (Optional) The Python file contains the Python code that is used by the module. See section Python functions and Script files for different options to add Python functions to user interactions. Internal network file (.mlab): (Optional) Stores the internal network of the module if available. This file essentially defines the macro module\u0026rsquo;s internal structure and connections. Macro module help file (.mhelp): (Optional) Provides help documentation for the macro module. This file is used to display information to users about the module’s functionality, usage, and any specific instructions. Additionally, a macro module may provide an additional Python (.py) and network (.mlab) that defines your automated test(s). Both files are also stored in your Package and can only be added for global macro modules.\nPython Functions and Script Files Python functions can be executed on any user interaction with your macro module. Examples are:\nModule initialization: You can add the initCommand to the Commands section and the given Python function is called whenever the module is added to the workspace or reloaded. Window creation: You can add the initCommand to the Window section and the given Python function is called whenever the panel of the module is opened. User interaction: You can add commands to any user interface element like Buttons to call Python functions on user interactions with this element. The image below shows you the user interface and the internal interface. Field changes: You can also react on any changes of fields in your module and create field listeners. See section Field Listeners for details. Field Listeners Field listeners are mechanisms to execute Python code automatically any time the value of a field changes. This allows you to create dynamic responses to user interactions in the module\u0026rsquo;s parameter panel.\nYou can define field listeners within the Commands sections of the .script file. You get a reference to the field object and then use a method to add a callback function that will be executed when the field\u0026rsquo;s value is modified.\nFor an example see Example 2.5.2: Module interactions via Python scripting.\nSummary Macro modules allow you to add your own functionality to MeVisLab. You can add inputs and outputs and connect existing modules to your new macro module. Macro modules may contain an internal network to encapsulate this functionality or Python code to implement your own functionalities to MeVisLab. Benefits are encapsulation, reusability, abtraction of complexity, and the possibility to add your own functionality to MeVisLab. There are different types of macro modules: Local macro modules are only available in the directory of your current network. Global macro modules are available in all projects but must be part of a package. ","tags":["Beginner","Tutorial","Macro","Macro modules"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/package/","title":"Example 2.1: Package Creation","summary":"Example 2.1: Package Creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Packages are the way MeVisLab organizes different development projects.\nMacro modules and projects are stored in packages. If you like to create a global macro module, you need a package in which this macro module can be stored in. In this chapter, we will create our own package. We start our package creation by creating a package group, because every package needs to be stored in a package group. You can find detailed information about packages and package groups here and in the package documentation .\n","content":"Example 2.1: Package Creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Packages are the way MeVisLab organizes different development projects.\nMacro modules and projects are stored in packages. If you like to create a global macro module, you need a package in which this macro module can be stored in. In this chapter, we will create our own package. We start our package creation by creating a package group, because every package needs to be stored in a package group. You can find detailed information about packages and package groups here and in the package documentation .\nSteps to Do To create packages and package groups, we will use the Project Wizard. Open the Project Wizard via [ File \u0026rarr; Run Project Wizard ... ]. Then, select [ Package \u0026rarr; New Package ] and Run Wizard.\nThe Project Wizard Next you need to:\nFind a name for your package group, for example, your company name or in our example the name MyPackageGroup.\nFind a name for your package, in our example we call it General.\nSelect the path your package group is supposed to be stored in. If you like to add a package to an existing package group, select its name and chose the path the package group is stored in.\nIf you now create the package, you can find a folder structure in the desired directory. The folder of your package group contains the folder of your package. We have now successfully created a package in which we can store our global macro module.\nPackage creation Summary Packages are needed to store global macro modules and projects. Package groups contain packages. Packages and package groups can be created using the Project Wizard. Detailed information about packages can be found in the package documentation . ","tags":["Beginner","Tutorial","Package"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/globalmacromodules/","title":"Example 2.2: Creation of Global Macro Modules","summary":"Example 2.2: Global Macro Modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter you will learn how to create global macro modules. There are many ways to do this. You can convert local macros into global macro modules or you can directly create global macro modules using the Project Wizard. In contrast to local macro modules, global macro modules are commonly available throughout projects and can be found via module search and under [ Modules ].\n","content":"Example 2.2: Global Macro Modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter you will learn how to create global macro modules. There are many ways to do this. You can convert local macros into global macro modules or you can directly create global macro modules using the Project Wizard. In contrast to local macro modules, global macro modules are commonly available throughout projects and can be found via module search and under [ Modules ].\nSteps to Do Transform a Local Macro Module into a Global Macro Module To transform our local macro module Filter from Chapter I into a global macro module, right-click the macro module to open the context menu and select [ Extras \u0026rarr; Convert To Global Module... ]\nConvert local macro to global macro Define Module Properties Choose a unique module name.\nState the module\u0026rsquo;s author.\nSelect the genre of the module. For this, browse through the module genres to select the appropriate genre. In our case, as our macro module contains a contour filter, we will choose the genre Filters.\nThe Genre defines the location where your module will be shown in MeVisLab [ Modules ] menu.\nTick the box Add reference to example network to directly create the template for an example network for your macro module.\nSelect the package you like to store the module in. We choose the package we created before. Your module is saved in the .mlab format and can be found in \\MyPackageGroup\\General\\Modules\\Macros\\MyProject.\nInfo:\u0026nbsp; If you are working with MeVisLab versions before 5.0, make sure to choose Directory Structure as self-contained. This makes sure that all files of your module are stored in a single directory. Later versions always use self-contained.\nAlso keep in mind that Python files are only created automatically if selected in the Project Wizard. Converting a local macro to a global macro does NOT create a Python file automatically.\nCreate global macro module Use the Project Wizard to Create Global Macro Modules Instead of converting a local macro module into a global macro module, you can also use the Project Wizard to create new macro modules. Open the Project Wizard via [ File \u0026rarr; Run Project Wizard ... ]. Then, select [ Modules (Scripting) \u0026rarr; Macro module ] and Run Wizard.\nDefine Module Properties Choose a unique module name.\nState the module\u0026rsquo;s author.\nSelect the genre of the module. For this, browse through the module genres to select the appropriate genre. In our case, as our macro module contains a contour filter, we will choose the genre Filters.\nTick the box Add reference to example network to directly create the template for an example network for your macro module.\nSelect the package you like to store the module in. We choose the package we created before. Your module is saved in the .mlab format and can be found in \\MyPackageGroup\\General\\Modules\\Macros\\MyProject.\nInfo:\u0026nbsp; Make sure to choose Directory Structure as self-contained. This ensures that all files of your module are stored in a single directory. Press Next \u0026gt; to edit further properties. You have the opportunity to directly define the internal network of the macro module, for example, by copying an existing network. In this case, we could copy the network of the local macro module Filter we already created. In addition, you have the opportunity to directly create a Python file. Python scripting can be used for the implementation of module interactions and other module functionalities. More information about Python scripting can be found here.\nProjectWizard1 ProjectWizard2 Structure of Global Macro Modules After creating your global macro module, you can find the created project MyProject in your package. This project contains your macro module Filter. For the macro module exist three files:\nFilter.def: module definition file Filter.mlab: network file that contains the internal network of your macro module Filter.script: MDL script file that defines inputs and outputs of your macro module as well as fields. This file defines the module panel, as well as references to Python scripts. In addition, two folders may be created:\nmhelp: contains the help files of all modules of this project network: contains the example networks of all modules of this project Structure of global macro modules How to Find Global Macro Modules All available modules are categorized and can be found via [ Modules ] in the respective genre. After creating a global macro, the new module can be found via [ Modules \u0026rarr; Filters ]. In addition, you can now find your macro module via module search.\nFind module in menu Hint:\u0026nbsp; If you do not find your new global macro module, try to reload the module database. Reload module database Summary Via right-click [ Extras \u0026rarr; Convert To Global Module... ], global macro modules can be created out of local macro modules. You can use the Project Wizard to create new macro modules. You need to have a package structure to store your global macro module. Global macro modules are available throughout projects and can be found via Module Search and under menu item [ Modules ]. ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/helpfiles/","title":"Example 2.3: Creation of Module Help","summary":"Example 2.3: Creation of Module Help Generating help of a macro module is part of the video about macro modules from Example 2: Creation of global macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter, you will learn how to create a help page and an example network. For hands-on training, we will use the macro module Filter, which was created in the previous chapter.\n","content":"Example 2.3: Creation of Module Help Generating help of a macro module is part of the video about macro modules from Example 2: Creation of global macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter, you will learn how to create a help page and an example network. For hands-on training, we will use the macro module Filter, which was created in the previous chapter.\nDepending on the way the macro module was created, the default help page and example network might or might not exist. In the case they exist, the help page only contains information about module inputs and outputs as well as module fields. The example network only contains the macro module itself. Both, the help page and the example network, can be created and edited after module creation.\nSteps to Do Creation of Help Files Using MeVisLab MATE We will start by creating a help file using the built-in text editor MeVisLab MATE (MeVisLab Advanced Text Editor). If you open the context menu of your global macro module and select [ Help ], it might be that no help page is given. We will start to create a help file by selecting [ Help \u0026rarr; Create Help ]. If a help page already exists, select [ Help \u0026rarr; Edit Help ].\nCreation of module help MeVisLab MATE opens. An .mhelp file (Filter.mhelp) is created automatically and stored in the folder your macro module Filter is stored in. You can find the folder structure in MATE on the left side. Editing the text field, you can edit the help file.\nEdit module help file via MATE When creating the help file of a module, all important information of the module down to the field specifications are extracted and created automatically. Thus, the basic module information is always available in the module help. Additional documentation should be added by the module\u0026rsquo;s author. On the left side, you can find the outline of the help file. Each section can be edited. In this example, we added the purpose of the module to the help file.\nEdit module help file via MATE MATE offers the possibility to format the text. By using the button M, module names can be formatted in such a way that links to the respective help file of the modules are created.\nEdit module help file via MATE After finishing your documentation, you can click Generate Help or F7 and your final help file is generated.\nExtra Infos:\u0026nbsp; More information on MeVisLab MATE can be found here\nThe Module Help Editor is explained here\nThe result can be seen when opening the help file via context menu in MeVisLab IDE (or by pressing F1 ).\nHelp file of the module Watch out:\u0026nbsp; Depending on the way the macro module was created, more or less features are automatically given in the help file and the example network. All missing features can be added manually. Creation of an Example Network To add an example network to your module, you need to add a reference to the respective .mlab file to the module definition file (.def). Open the file Filter.def. You can find the line exampleNetwork = \u0026ldquo;$(LOCAL)/networks/FilterExample.mlab\u0026rdquo;, which defines the reference to the .mlab file containing the example network. By default, the name of the example network is ModulenameExample.mlab. An .mlab file containing only the module Filter is created inside the folder networks.\nIt is possible that the reference to the example network or the file FilterExample.mlab is missing. One reason could be that its creation was not selected when creating the macro module. In this case, add the reference and the file manually.\nReference to Example Network To create the example network, open the file FilterExample.mlab in MeVisLab and create an appropriate example.\nExample Network Summary MeVisLab MATE is a built-in text editor that can be used to create module help files and module panels, or to create module interactions via Python scripting. You can create help files via the module context menu using MeVisLab\u0026rsquo;s MATE. You can add an example network to your macro module via the .def file. ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro","Help"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/guidesign/","title":"Example 2.4: GUI Development","summary":"Example 2.4: Building a Panel Layout: Interactions with Macro Modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This chapter will give you an introduction into the creation of module panels and user interfaces. For the implementation, you will need to use the MeVisLab Definition Language (MDL) .\nExtra Infos:\u0026nbsp; More information about GUI design in MeVisLab can be found here Creating a Panel for the Macro Module Filter Creation of a Module Panel In Example 2.2 we created the global macro module Filter. By now, this module does not have a proper panel. When double-clicking the module, the Automatic Panel is shown.\n","content":"Example 2.4: Building a Panel Layout: Interactions with Macro Modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This chapter will give you an introduction into the creation of module panels and user interfaces. For the implementation, you will need to use the MeVisLab Definition Language (MDL) .\nExtra Infos:\u0026nbsp; More information about GUI design in MeVisLab can be found here Creating a Panel for the Macro Module Filter Creation of a Module Panel In Example 2.2 we created the global macro module Filter. By now, this module does not have a proper panel. When double-clicking the module, the Automatic Panel is shown.\nThe Automatic Panel contains fields, as well as module inputs and outputs. In this case, no fields exists except the instanceName. Accordingly, there is no possibility to interact with the module. Only the input and the output of the module are given.\nAutomatic Panel To add and edit a panel, open the context menu and select [ Related Files \u0026rarr; Filter.script ]. The text editor MATE opens. You can see the file Filter.script, which you can edit to define a custom user interface for the module.\nModule script file Module Interface By default, the .script file contains the interface of the module. In the interface section (everything inside the curled brackets behind the name Interface) you can define the module inputs, the module outputs, and also all module fields (or Parameters).\nFilter.script\nInterface { Inputs { Field input0 { internalName = Convolution.input0 } } Outputs { Field output0 { internalName = Arithmetic2.output0 } } } Module Inputs and Outputs To create an input/output, you need to define a Field in the respective input/output section. Each input/output gets a name (here input0/output0) that you can use to reference this field. The module input maps to an input of the internal network. You need to define this mapping. In this case, the input of the macro module Filter maps to the input of the module Convolution of the internal network (internalName = Convolution.input0). Similarly, you need to define which output of the internal network maps to the output of the macro module Filter. In this example, the output of the internal module Arithmethic2 maps to the output of our macro module Filter (internalName = Arithmetic2.output0).\nCreating an input/output causes:\nInput/output connectors are added to the module. You can find placeholders for the input and output in the internal network (see image). Input/output fields are added to the automatic panel. A description of the input/output fields is automatically added to the module help file, when opening the .mhelp file after input/output creation. Helpfile creation is explained in Example 2.3. Internal Network of your macro module Module Fields In the Parameters section, you can define fields of your macro module. These fields may map to existing fields of the internal network (internalName = \u0026hellip; ), but they do not need to and can also be completely new. You can reference these fields when creating a panel, to allow interactions with these fields. All fields appear in the Automatic Panel.\nModule Panel Layout To create your own user interface, we need to create a Window . A window is one of the layout elements that exist in MDL. These layout elements are called controls . The curled brackets define the window section, in which you can define properties of the window and insert further controls like a Box .\nInitially, we call the window MyWindowTitle, which can be used to reference this window.\nDouble-clicking on your module now opens your first self-developed user interface.\nFilter.script\nInterface { Inputs { Field input0 { internalName = Convolution.input0 } } Outputs { Field output0 { internalName = Arithmetic2.output0 } } Parameters { } } Window MyWindowName { title = MyWindowTitle Box MyBox { } } Module Panel You can define different properties of your control. For a window, you can, for example, define a title, or whether the window should be shown in full screen (fullscreen = True).\nThese properties are called tags and are individually different for each control. Which tags exist for the control window can be found here . The control box has different tags. You can, for example, define a title for the box, but you can not define whether to present the box in full screen.\nIf you like to add more than one control to your window, for example, one box and one label, you can specify their design like in the following examples:\nFilter.script\nWindow MyWindowName { title = MyWindowTitle w = 100 h = 50 Vertical { Box MyBox { title = \u0026#34;Title of my Box\u0026#34; } Label MyLabel { title = \u0026#34;This is a label below the box\u0026#34; } } } Vertical layout of Box and Text Filter.script\nWindow MyWindowName { title = MyWindowTitle w = 100 h = 50 Horizontal { Box MyBox { title = \u0026#34;Title of my Box\u0026#34; } Label MyLabel { title = \u0026#34;This is a label beside the box\u0026#34; } } } Horizontal layout of Box and Text There are much more controls that can be used. For example, a CheckBox, a Table, a Grid, or a Button. To find out more, take a look into the MDL Reference .\nModule Interactions Until now, we learned how to create the layout of a panel. As a next step, we like to get an overview over interactions.\nExtra Infos:\u0026nbsp; You can add the module GUIExample to your workspace and play around with is. Access to Existing Fields of the Internal Network To interact with fields of the internal network in your user interface, we need to access these fields. To access the field of the internal module Convolution, which defines the kernel, we need to use the internal network name. To find the internal field name, open the internal network of the macro module Filter (click on the module using the middle mouse button ).\nThen, open the panel of the module Convolution and right-click the field title Use of the box Predefined Kernel and select Copy Name. You now copied the internal network name of the field to your clipboard. The name is made up of ModuleName.FieldName, in this case Convolution.predefKernel.\nConvolution Module In the panel of the module Convolution, you can change this variable Kernel via a drop-down menu. In MDL, a drop-down menu is called a ComboBox . We can take over the field predefKernel, its drop-down menu and all its properties by creating a new field in our panel and reference to the internal field Convolution.predefKernel, which already exist in the internal network.\nChanges of the properties of this field can be done in the curled brackets using tags (here, we changed the title).\nFilter.script\nWindow MyWindowName { title = MyWindowTitle Field Convolution.predefKernel { title = Kernel } } Selecting the kernel As an alternative, you can define the field kernel in the Parameters section, and reference the defined field by its name. The result in the panel is the same. You can see a difference in the automatic panel. All fields that are defined in the interface in the Parameters section appear in the automatic panel. Fields of the internal network, which are used but not declared in the section Parameters of the module interface, do not appear in the automatic panel.\nFilter.script\nInterface { Inputs { Field input0 { internalName = Convolution.input0 } } Outputs { Field output0 { internalName = Arithmetic2.output0 } } Parameters { Field kernel { internalName = Convolution.predefKernel title = Kernel: } } } Window MyWindowName { title = MyWindowTitle Field kernel {} } Commands We not only can use existing functionalities, but also add new interactions via Python scripting.\nIn the example below, we added a wakeupCommand to the Window and a simple command to the Button.\nFilter.script\nWindow MyWindowName { title = MyWindowTitle wakeupCommand = myWindowCommand Button MyButton { command = myButtonAction } } The wakeupCommand defines a Python function that is executed as soon as the Window is opened. The Button command is executed when the user clicks on the Button.\nBoth commands reference a Python function that is executed whenever both actions (open the Window or click the Button) are executed.\nIf you like to learn more about Python scripting, take a look at Example 2.5.\nWe need to define the Python script that contains our Python functions. In order to do this, add a Command section outside your window and define the tag source.\nExample: Filter.script\nCommands { source = $(LOCAL)/Filter.py } Infos:\u0026nbsp; The section Source should already be available and generated automatically in the case you enable the Wizard to add a Python file to your module. You can right-click on the command (myWindowCommand or myButtonAction) in your .script file and select [ Create Python Funtion...... ]. The text editor MATE opens automatically and generates an initial Python function for you. You can simply add a logging function or implement complex logic here.\nExample: Filter.py\ndef myWindowCommand: MLAB.log(\u0026#34;Window opened\u0026#34;) def myButtonAction: MLAB.log(\u0026#34;Button clicked\u0026#34;) Available Examples MeVisLab provides a lot of example modules for GUI development. All of these examples provides the .script file for UI development and the .py file containing the Python script.\nLayouting Examples TestVerticalLayout Module TestHorizontalLayout Module TestTableLayout Module TestGridLayout Module TestSplitterLayout Module TestBoxLayout Module TestTabViewLayout Module Other Examples TestHyperText Module TestListBox Module TestListView Module TestIconView Module TestPopupMenu Module TestViewers Module TestEventFilter Module TestStyles Module TestButtonGroups Module TestImageMap Module Please use the Module Search with the prefix Test for more examples.\nSummary User interfaces and several module panels can be created for each macro module. You can create a panel, define inputs and outputs as well as interactions, in your .script file in MATE by using the MeVisLab Definition Language (MDL) . Module interactions can be implemented using commands, which are linked to Python functions. You can implement field listeners, which trigger actions after a field value changes. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro","User Interface","GUI"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/pythonscripting/","title":"Example 2.5: Interactions via Python Scripting","summary":"Example 2.5: Module Interactions Using Python Scripting Introduction This chapter will give you an overview over Python scripting in MeVisLab. Here, no introduction into Python will be given. However, basic knowledge in Python is helpful. Instead, we will show how to integrate and use Python in the MeVisLab SDK.\nIn fact, nearly everything in MeVisLab can be done via Python scripting: You can add modules to your network, or remove modules, you can dynamically establish and remove connections, and so on. But, much more important: You can access module inputs and outputs, as well as module fields to process their parameters and data. You can equip user interfaces and panel with custom functionalities. Python can be used to implement module interactions. When you open a panel or you press a button in a panel, the executed actions are implemented via Python scripting.\n","content":"Example 2.5: Module Interactions Using Python Scripting Introduction This chapter will give you an overview over Python scripting in MeVisLab. Here, no introduction into Python will be given. However, basic knowledge in Python is helpful. Instead, we will show how to integrate and use Python in the MeVisLab SDK.\nIn fact, nearly everything in MeVisLab can be done via Python scripting: You can add modules to your network, or remove modules, you can dynamically establish and remove connections, and so on. But, much more important: You can access module inputs and outputs, as well as module fields to process their parameters and data. You can equip user interfaces and panel with custom functionalities. Python can be used to implement module interactions. When you open a panel or you press a button in a panel, the executed actions are implemented via Python scripting.\nBasics To see how to access modules, fields, and so on, open the Scripting Console via [ Scripting \u0026rarr; Show Scripting Console ].\nInternal Field Names You can find the internal name of one module field in the respective network. Open a panel, for example, the automatic panel and right-click the field\u0026rsquo;s title to open the field\u0026rsquo;s context menu. Now, you can select Copy Name, to copy the internal name of the field. This name can be used to access the field via scripting.\nScripting Context When entering ctx to the console, you can see the context you are working with. In the context of the Scripting Console, you have access to your workspace, meaning the whole network, its modules, and the modules\u0026rsquo; fields.\nScripting context Editing the Workspace In the Scripting Console, you can add and connect modules using the following commands:\nctx.addModule(\u0026quot;\u0026lt; ModuleName \u0026gt;\u0026quot;) : Add the desired module to your workspace. ctx.field(\u0026quot; \u0026lt; ModuleName.FieldName\u0026gt; \u0026quot;) : Access a field of a module. ctx.field(\u0026quot; \u0026lt; ModuleInput \u0026gt; \u0026quot;).connectFrom(\u0026quot; \u0026lt; ModuleOutput \u0026gt; \u0026quot;) : Draw a connection from one module\u0026rsquo;s output to another module\u0026rsquo;s input. In this case, we added the modules DicomImport and View2D to the workspace and connected both modules.\nAdd and connect modules via scripting It is also possible to add notes to your workspace.\nAdd a note to your workspace Access Modules and Module Fields You can access modules via ctx.module(\u0026quot; \u0026lt; ModuleName \u0026gt; \u0026quot;). From this object, you can access module fields, module inputs and outputs, and everything in context of this module.\nYou can also directly access a module field via ctx.field(\u0026quot; \u0026lt; ModuleName.FieldName \u0026gt; \u0026quot;). Different methods can be called on this object. Have a look at the Scripting Reference to find out which methods can be called for which object or class. You can, for example, access the value of the respective field.\nAccess modules and module fields Python Scripting Reference Here you can find the Scripting Reference. In the Scripting Reference you can find information about different Python classes used in MeVisLab and their methods.\nWhere and How to Use Python Scripting Scripting View Under [ View \u0026rarr; Views \u0026rarr; Scripting ] you can find the View Scripting. The view offers a standard Python console, without any meaningful network or module context. This means only general Python functionalities can be tested and used. Access to modules or your network is not possible.\nScripting Console You can open the Scripting Console via [ Scripting \u0026rarr; Show Scripting Console ]. In the context of your workspace, you can access your network and modules.\nScripting Console of Modules Every module offers a scripting console. Open the context menu of a module and select [ Show Window \u0026rarr; Scripting Console ]. You can work in the context (ctx.) of this module.\nModule RunPythonScript The module RunPythonScript allows to execute Python scripts from within a MeVisLab network. You can draw parameter connection from modules to RunPythonScript and back, to process parameter fields using Python scripting. An example for the usage of RunPythonScript can be found here.\nModule Interactions via Python Scripting You can reference to a Python function in a .script file of a macro module. With this, you can, for example, execute a Python function whenever you open a panel, or define the action that is executed when pressing a button or specify the command triggered by a field listener. An example for module interactions via Python scripting is given in the same example.\nPython Scripting in Network Files (.mlab) If you do not want to create a macro module, you can also execute Python scripts in a network file (.mlab). Save your network using a defined name, for example, mytest.mlab. Then, create a .script and a .py file in the same directory, using the same names (mytest.script and mytest.py).\nOpen the .script file and add a Commands section defining the name of the Python file.\nIsoCSOs.script\nCommands { source = $(LOCAL)/mytest.py } Now, you can enter your Python code to the file mytest.py, for example:\nIsoCSOs.py\n# Optional, only necessary if you want to call functions in your network # from mevis import * print(\u0026#34;Hello\u0026#34;) If you now use the menu item [ Scripting \u0026rarr; Start Network Script ], the script can be executed inside your network. You can also use the keyboard shortcut ctrl\u0026#43;R .\nTips and Tricks Scripting Assistant Under [ View \u0026rarr; Views \u0026rarr; Scripting Assistant ] you can find the view Scripting Assistant. In this view, the actions you execute in the workspace are translated into Python script.\nFor example: Open the Scripting Assistant. Add the module WEMInitialize to your workspace. You can select a Model, for example, the cube. In addition, you can change the Translation and press Apply. All these actions can be seen in the Scripting Assistant translated into Python code. Therefore, the Scripting Assistant is a powerful tool to help you to script you actions.\nScripting Assistant Examples See the following examples for Python scripting:\nThe module RunPythonScript Module interactions via Python scripting Summary Python can be used to access, create, and process networks, modules, fields, and panels. You can use Python via different scripting consoles. You can also define custom module interactions by referencing to Python functions from the .script file. ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro","Python","Scripting"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/scriptingexample1/","title":"Example 2.5.1: The Module RunPythonScript","summary":"Example 2.5.1: The Module RunPythonScript \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The module RunPythonScript allows to execute Python scripts from within a MeVisLab network. You can draw parameter connection from modules to RunPythonScript and back to process parameter fields using Python scripting.\nSteps to Do Develop Your Network In this example, we like to dynamically change the color of a cube in an Open Inventor scene. For that, add and connect the following modules as shown.\n","content":"Example 2.5.1: The Module RunPythonScript \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The module RunPythonScript allows to execute Python scripts from within a MeVisLab network. You can draw parameter connection from modules to RunPythonScript and back to process parameter fields using Python scripting.\nSteps to Do Develop Your Network In this example, we like to dynamically change the color of a cube in an Open Inventor scene. For that, add and connect the following modules as shown.\nRunPythonScript Scripting Using the Module RunPythonScript Open the panel of RunPythonScript. There is an option to display input and output fields. For that, tick the box Fields on the top left side of the panel.\nYou can also name these fields individually by ticking the box Edit field titles. Call the first input field TimeCounter and draw a parameter connection from the field Value of the panel of TimeCounter to the input field TimeCounter of the module RunPythonScript. We can name the first output field DiffuseColor and draw a parameter connection from this field to the field Diffuse Color in the panel of the module SoMaterial.\nTimeCounter The module TimeCounter counts in a defined Frequency. We like to randomly change the color of the cube in the frequency the TimeCounter counts. Add this code:\nIsoCSOs.py\nimport random red = TimeCounter * random.randrange(0,52)/255 green = TimeCounter * random.randrange(0,52)/255 blue = TimeCounter * random.randrange(0,52)/255 updateOutputValue(\u0026#34;DiffuseColor\u0026#34;, str(red) + \u0026#34; \u0026#34; + str(green) + \u0026#34; \u0026#34; + str(blue)) To update the output field DiffuseColor, it is important to use the methods updateOutputValue(name, value) or setOutputValue(name, value) instead of simply assigning a value to the output field.\nYou can now see a color change in the viewer SoExaminerViewer every time the TimeCounter counts.\nTriggered color change Summary The module RunPythonScript can be used to process module fields in your network using Python scripting. Use the methods updateOutputValue(name, value) or setOutputValue(name, value) to update output fields of RunPythonScript. ","tags":["Beginner","Tutorial","Python","Scripting","RunPythonScript"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/scriptingexample2/","title":"Example 2.5.2: Module Interactions via Python Scripting","summary":"Example 2.5.2: Module Interactions Via Python Scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to add Python scripting to your user interface. The network used in Chapter V will be used for creating the macro module.\nSteps to Do Creating the Macro Module First, we condense the example network into a macro module and then we create a panel for that module. To create a macro module, use the Project Wizard, which you find under [ File \u0026rarr; Run Project Wizard ]. Select Macro module and press Run.\n","content":"Example 2.5.2: Module Interactions Via Python Scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to add Python scripting to your user interface. The network used in Chapter V will be used for creating the macro module.\nSteps to Do Creating the Macro Module First, we condense the example network into a macro module and then we create a panel for that module. To create a macro module, use the Project Wizard, which you find under [ File \u0026rarr; Run Project Wizard ]. Select Macro module and press Run.\nNow, you have to edit:\nName: The name of your module Package: Select the package you like to save the macro module in Directory Structure: Change to Self-contained (this setting is only available in MeVisLab versions before 5.0.0, later versions always use self-contained) Project: Select your project name Press Next and edit the following:\nCopy existing network: Select the example network Check the box: Add Python file Now, create your macro module and reload MeVisLab. You can find your module via search in MeVisLab.\nCreating macro module Enable Python scripting To design a panel and create a user interface for the macro module, open the .script file. You can see that a Commands section exists, which defines the Python file as source for all commands.\nOpen the script file Script file Creating a Panel With Tabs and Viewers First, we create a Window with two Tabs : the Main tab, in which both viewers of the network are represented, and the tab for Settings. For generating tabs, we can use the control TabView , with its items TabViewItem . The control TabView enables to add a command, which is executed when opening the tab. For adding the viewers to the panel, we use the control Viewer.\nIsoCSOs.script\nWindow { TabView { TabViewItem Main { Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 } Viewer SoExaminerViewer.self { type = SoExaminerViewer pw = 400 ph = 400 } } } TabViewItem Settings { } } } Panel with Tabs and Viewers Edit Viewer Settings in the Panel You may want to change the design setting of the right viewer. This is still possible via the internal network of the macro module. Open the internal network either via the context menu or using the middle mouse button and click on the module. After that, open the automatic panel of the module SoExaminerViewer via context menu [ Show Windows \u0026rarr; Automatic Panel ] and change the field decoration to False. Keep in mind, as we did not create CSOs by now, the right viewer stays black.\nChange viewer settings Changed viewer settings Selection of Images Next, we like to add the option to browse through the folders and select the image, we like to create CSOs from. This functionality is already given in the internal network in the module LocalImage. We can copy this functionality from LocalImage and add this option to the panel above both viewers. But, how should we know, which field name we reference to? To find this out, open the internal network of your macro module. Now you are able to open the panel of the module LocalImage. Right-click the desired field: In this case, right-click the label Name:. Select Copy Name, to copy the internal name of this field.\nCopy the field name Now, you can add this field as a new field to your window by pasting the name. All field settings are taken over from the internal field from the module LocalImage.\nIsoCSOs.script\nWindow { TabView { TabViewItem Main { Vertical { Field LocalImage.name {} Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 } Viewer SoExaminerViewer.self { type = SoExaminerViewer pw = 400 ph = 400 } } } } TabViewItem Settings { } } } Add name field Add Buttons to Your Panel As a next step, we like to add a Browse... button, like in the module LocalImage, and also a button to create the CSOs.\nTo create the Browse... button:\nCreate a button containing the command fileDialog. Right-click the command to create the respective function in the Python file. Edit the function in the Python file to enable the file dialog (similar function as in LocalImage.py). To create the Iso Generator Button:\nWe like to copy the field of the Update button from the internal module IsoCSOGenerator, but not its layout so:\nCreate a new Field in the interface, called IsoGenerator, which contains the internal field Update from the module IsoCSOGenerator. Create a new Button in your Window that uses the field IsoGenerator. After these steps, you can use the Iso Generator button to create CSOs.\nIsoCSOs.script\nInterface { Inputs {} Outputs {} Parameters { Field IsoGenerator { internalName = CSOIsoGenerator.apply } } } Commands { source = $(LOCAL)/IsoCSOs.py } Window { TabView { TabViewItem Main { Vertical { Horizontal { Field LocalImage.name {} Button { title = \u0026#34;Browse...\u0026#34; command = fileDialog } Button IsoGenerator { title = \u0026#34;Iso Generator\u0026#34; } } Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 } Viewer SoExaminerViewer.self { type = SoExaminerViewer pw = 400 ph = 400 } } } } TabViewItem Settings { } } } IsoCSOs.py\nfrom mevis import * def fileDialog(): exp = ctx.expandFilename(ctx.field(\u0026#34;LocalImage.name\u0026#34;).stringValue()) filename = MLABFileDialog.getOpenFileName(exp, \u0026#34;\u0026#34;, \u0026#34;Open file\u0026#34;) if filename: ctx.field(\u0026#34;LocalImage.name\u0026#34;).value = ctx.unexpandFilename(filename) Automatically generate CSOs based on Iso value Colorizing CSOs We like to colorize the CSO we hover over with our mouse in the 2D viewer. Additionally, when clicking a CSO with the left mouse button , this CSO shall be colorized in the 3D viewer. This functionality can be implemented via Python scripting (even though MeVisLab has a build-in function to do that). We can do this in the following way:\nEnable the View Scripting Assistant, which translates actions into Python code.\nScripting Assistant Enable a functionality that allows us to notice the ID of the CSO we are currently hovering over with our mouse. For this, open the internal network of our macro module. We will use the module SoView2DCSOExtensibleEditor. Open its panel and select the tab Advanced. You can check a box to enable Update CSO id under mouse. If you now hover over a CSO, you can see its ID in the panel. We can save the internal network to save this functionality, but we can also solve our problem via scripting. The Scripting Assistant translated our action into code that we can use.\nEnabling CSO id identification We like to activate this functionality when opening the panel of our macro module IsoCSOs. Thus, we add a starting command to the control Window. We can call this command, for example, enableFunctionalities.\nIn the .script file:\nIsoCSOs.script\nWindow { windowActivatedCommand = enableFunctionalities TabView { TabViewItem Main { ... } } } In the Python file, we define the function enableFunctionalities. We see our action as Python code in the Scripting Assistant. Just copy the code into our Python function.\nIsoCSOs.py\ndef enableFunctionalities(): ctx.field(\u0026#34;SoView2DCSOExtensibleEditor.updateCSOIdUnderMouseCursor\u0026#34;).value = True Implement a field listener. This field listener will detect when you hover over a CSO and the CSO ID changes. Triggered by a CSO ID change, a colorization function will be executed that will colorize the selected CSO. In the .script file:\nIsoCSOs.script\nCommands { source = $(LOCAL)/IsoCSOs.py FieldListener SoView2DCSOExtensibleEditor.csoIdUnderMouseCursor { command = colorizeCSO } } In the Python file:\nIsoCSOs.py\n# global variables listCSOs = [] idxCSO = -1 def colorizeCSO(): if ctx.field(\u0026#34;CSOManager.numCSOs\u0026#34;) == 0: pass else: global listCSOs global idxCSO if listCSOs == []: listCSOs = ctx.field(\u0026#34;CSOManager.outCSOList\u0026#34;).object() # COLORIZATION OF CSO # Changing back color of previously selected CSO to default value if idxCSO \u0026gt;= 0: oldCSO = listCSOs.getCSOAt(idxCSO) oldCSO.setPathPointColor((1.0, 1.0, 0.0)) # Color change of CSO oldCSO.setPathPointWidth(1) # Line width change # Changing color and width of selected CSO idxCSO = ctx.field(\u0026#34;SoView2DCSOExtensibleEditor.csoIdUnderMouseCursor\u0026#34;).value - 1 # -1 because CSOs are indexed starting at 1 if idxCSO \u0026gt;= 0: currentCSO = listCSOs.getCSOAt(idxCSO) currentCSO.setPathPointColor((1.0, 0.0, 1.0)) currentCSO.setPathPointWidth(5) Reload your module ( F5 ) and open the panel. After generating CSOs, the CSO under your mouse is marked. Clicking this CSO enables the marking in the 3D viewer. If you like, you can add some settings to your Settings page. For example:\nIsoCSOs.script\nTabViewItem Settings { Field CSOIsoGenerator.isoValue {} Field SoCSOVisualizationSettings.ghostingDepthInVoxel {} } Colored selection Summary The control Tabview creates tabs in panels. The control Viewer allows to add viewers to your panel. The control Button creates a button executing a Python function when pressed. The tag WindowActivationCommand of the control Window triggers Python functions executed when opening the panel. Field listeners can be used to activate Python functions triggered by a change of defined parameter fields. Use the view Scripting Assistant to translate actions into Python code. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Macro","Macro modules","Global Macro","Python","Scripting"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/viewerexample/","title":"Example 3: Creating a Simple Application","summary":"Example 3: Creating a Simple Application Introduction In the previous examples, you already learned how to create macro modules, user interfaces, and how to interact with your UI via Python scripting.\nIn this example, you will learn how to create a simple prototype application in MeVisLab including a user interface with 2D and 3D viewer. You will learn how to implement field listeners and react on events.\nSteps to Do Create Your Network Start with an empty network and add the module ImageLoad to your workspace. Then, add the modules View2D and View3D to your workspace and connect them as seen below.\n","content":"Example 3: Creating a Simple Application Introduction In the previous examples, you already learned how to create macro modules, user interfaces, and how to interact with your UI via Python scripting.\nIn this example, you will learn how to create a simple prototype application in MeVisLab including a user interface with 2D and 3D viewer. You will learn how to implement field listeners and react on events.\nSteps to Do Create Your Network Start with an empty network and add the module ImageLoad to your workspace. Then, add the modules View2D and View3D to your workspace and connect them as seen below.\nLoading and viewing images Load an Image Now, double-click on the ImageLoad module and open any image. You can use the included file ./MeVisLab/Resources/DemoData/MRI_Head.dcm.\nOpening your viewers should now show the images in 2D and 3D.\nShow images in 2D and 3D Save Your Network Now, save your network as .mlab file and remember the location.\nCreate a Macro Module Open the Project Wizard via [ File \u0026rarr; Run Project Wizard ] and run the Wizard for a macro module. Name your module MyViewerApplication, enter your details, and click Next \u0026gt;.\nModule Properties On the next screen, make sure to add a Python file and use the existing network you previously saved. Click Next \u0026gt;.\nMacro module Properties You can leave all fields empty for now and just click Create.\nModule Field Interface MeVisLab reloads its internal database and you can open a new tab. Search for your newly created module, in our case it was MyViewerApplication.\nMyViewerApplication In the case you double-click your module now, you will see the Automatic Panel only showing the name of your module, because we did not add any own Window until now.\nDevelop Your User Interface Before adding your own UI, open the internal network of your macro module via right-click and [ Related Files \u0026rarr; MyViewerApplication.mlab ]. Open the panel of your ImageLoad module and set filename to an empty string (clear). This is necessary for later.\nNow, right-click on your MyViewerApplication and select [ Related Files \u0026rarr; MyViewerApplication.script ]\nMATE opens showing your script file. You already learned how to create simple UI elements in Example 2.4. Now, we will create a little more complex UI including your View2D and View3D.\nFirst we need a new Field in your Parameters section. Name the field filepath and set internalName to ImageLoad.filename.\nMyViewerApplication.script\nInterface { Inputs {} Outputs {} Parameters { Field filepath { internalName = ImageLoad.filename } } } We now reuse the filepath field from the ImageLoad module for our interface. Add a Window and a Vertical to the bottom of your .script file. Add the just created parameter field filepath inside your Vertical as seen below.\nMyViewerApplication.script\nInterface { Inputs {} Outputs {} Parameters { Field filepath { internalName = ImageLoad.filename } } } Commands { source = $(LOCAL)/MyViewerApplication.py } Window { Vertical { Field filepath {} } } If you now double-click on your module, you can see your just created filepath field.\nFilepath field in UI Next, we will add your 2D and 3D viewers and a Button to your Window. Change your .script file as seen below:\nMyViewerApplication.script\nWindow { Vertical { Horizontal { Field filepath {} Button { title = \u0026#34;Reset\u0026#34; } } Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 expandX = yes expandY = yes } Viewer View3D.self { pw = 400 ph = 400 expandX = yes expandY = yes } } } } We have a vertical layout having two items placed horizontally next to each other. The new Button gets the title Reset but does nothing yet, because we did not add a Python function to a command.\nAdditionally, we added the View2D and the View3D to our Window and defined the height, width, and the expandX/Y property to yes. This leads our viewers to resize together with our Window.\nExtra Infos:\u0026nbsp; Additional information about the View2D and View3D options can be found in the MeVisLab MDL Reference You can now play around with your module in MeVisLab SDK. Open the Window and select a file. You can see the two viewers showing the 2D and 3D images. You can interact with your viewers the same way as in your MeVisLab network. All functionalities are taken from the modules and transferred to your user interface.\n2D and 3D viewers in our application Develop a Python Function for Your Button Next, we want to reset the filepath to an empty string on clicking our Reset button. Add the reset command to your Button. MyViewerApplication.script\n... Button { title = \u0026#34;Reset\u0026#34; command = reset } ... Right-click on reset and select [ Create Python function \u0026#39;reset\u0026#39; ]. MATE opens the Python file of your module and automatically adds the function definition. Set the filename of the ImageLoad module to an empty string.\nMyViewerApplication.py\nfrom mevis import * def reset(): ctx.field(\u0026#34;filepath\u0026#34;).value = \u0026#34;\u0026#34; Clicking on Reset in your module now clears the filename field and the viewers do not show any images anymore.\nField Listeners A field listener watches a given field in your network and reacts on any changes of the field value. You can define Python functions to execute in the case a change has been detected.\nIn order to define such a listener, you need to add it to the Commands section in your .script file.\nExample: MyViewerApplication.script\nCommands { source = $(LOCAL)/MyViewerApplication.py FieldListener View2D.startSlice { command = printCurrentSliceNumber } } In the above example, we react on changes of the field startSlice of the module View2D. Whenever the field value (currently displayed slice) changes, the Python function printCurrentSliceNumber is executed.\nIn your Python file MyViewerApplication.py, you can now add the following:\nMyViewerApplication.py\ndef printCurrentSliceNumber(field): MLAB.log(field.value) Scrolling through slices in the View2D module now logs a message containing the slice number currently visible to the MeVisLab Debug Output.\nSummary You can add any viewers to your application UI by reusing them in MDL. Parameter fields using the internalName of an existing field in your network allows reusing this UI element in your own UI. Changes in your UI are applied to the field in the module. Field Listeners allow reacting on changes of a field value in Python. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Macro","Macro modules","Global Macro","Python","Scripting"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/pythonpip/","title":"Example 4: Installing Additional Python Packages Using the PythonPip Module","summary":"Example 4: Installing Additional Python Packages Using the PythonPip Module Introduction MeVisLab already comes with a lot of integrated third-party software tools ready to use. Nevertheless, it might be necessary to install additional Python packages for your specific needs. This example will walk you through the process of adding packages through usage of/using the PythonPip module.\nThe PythonPip module allows to work with the Python package manager pip. It can be used to install Python packages into the site-packages of the MeVisLab Python installation.\n","content":"Example 4: Installing Additional Python Packages Using the PythonPip Module Introduction MeVisLab already comes with a lot of integrated third-party software tools ready to use. Nevertheless, it might be necessary to install additional Python packages for your specific needs. This example will walk you through the process of adding packages through usage of/using the PythonPip module.\nThe PythonPip module allows to work with the Python package manager pip. It can be used to install Python packages into the site-packages of the MeVisLab Python installation.\nIt technically provides the full Python package ecosystem, though you will have to keep some things in mind to avoid your newly added packages to interfere with the existing ones that MeVisLab operates on:\nPackages can contain C-Extensions (since we use the same MSVC compiler resp. same GCC settings as Python 3 itself), but you can only install packages that do not interfere with packages or DLLs that are already part of MeVisLab. This means that installing packages with C-Extensions might work in many circumstances, but is not guaranteed to work All installed packages with C-Extensions are release only, so you can only import them in a release MeVisLab (under Windows) Attention:\u0026nbsp; On Windows: Existing packages (e.g., NumPy) can only be upgraded if they haven\u0026rsquo;t already been loaded by MeVisLab\u0026rsquo;s Python. So please make sure to start with a fresh MeVisLab Packages that you should not upgrade or install (because they have been adapted for MeVisLab):\nvtk cv2 (OpenCV) PySide2 / PyQt (we have our own PythonQt binding and our own Qt DLLs) matplotlib These are some of the most important packages that have been adapted for MeVisLab. If you seem to have a problem upgrading another one that is not listed here, make sure to ask in the MeVisLab forum or directly contact our developers via EMail.\nWorking with the PythonPip Module on your MeVisLab Workspace The module PythonPip can be found via module search. It provides a user interface showing the currently installed Python packages including version and MeVisLab package it has been installed to.\nPythonPip interface Select the package to install the Python package into, write the name of the package and click install.\nIn the case you want to install a specific version, you can also use ==1.2.0\nAttention:\u0026nbsp; We strongly recommend to install the packages into a MeVisLab user package. This has many advantages:\nUser packages can be updated without administrator privileges User packages remain available after uninstalling and/or updating MeVisLab It is possible to install multiple versions of the Python packages into different user packages. Possible conflicts will be shown in the PythonPip module panel. The only disadvantage: Python commands will not be recognized outside of MeVisLab by default.\nThird-party information and .mli files are updated automatically.\nUsing the Commandline Another option is using the commandline tool provided by MeVisLab. On Windows, you need to change to directory Packages\\MeVis\\ThirdParty\\Python first.\ncommandline\nMeVisPython -m pip ... Attention:\u0026nbsp; The commandline option does not provide the possibility to install into a specified user package. Third-party information and .mli files are not adapted automatically with the commandline tool. In Example 1: Installing PyTorch using the PythonPip module we are installing PyTorch to use it in MeVisLab scripting.\nSummary The PythonPip module allows to install additional Python packages to adapt MeVisLab to a certain extent. ","tags":["Advanced","Tutorial","Python","PythonPip","pip"],"section":"tutorials"},{"date":"1736380800","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/pythondebugger/","title":"Example 5: Debugging Python in MATE","summary":"Example 5: Debugging Python Files in MATE \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction MeVisLab provides the powerful integrated text editor MATE. By default, MATE is used to create/edit files like Python scripts. In this tutorial, we want to show you how to debug Python scripts in MeVisLab.\nPrepare Your Network We are using a very simple network of predefined modules, but you can also debug your self-written Python scripts. Add a LocalImage module to your workspace and connect it to a DicomTagBrowser module. The DicomTagBrowser module shows a table containing the DICOM tags of your currently opened file.\n","content":"Example 5: Debugging Python Files in MATE \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction MeVisLab provides the powerful integrated text editor MATE. By default, MATE is used to create/edit files like Python scripts. In this tutorial, we want to show you how to debug Python scripts in MeVisLab.\nPrepare Your Network We are using a very simple network of predefined modules, but you can also debug your self-written Python scripts. Add a LocalImage module to your workspace and connect it to a DicomTagBrowser module. The DicomTagBrowser module shows a table containing the DICOM tags of your currently opened file.\nExample Network Open Python Script in MATE To debug our module, we need to open the Python file. Right-click the module DicomTagBrowser and select [ Related Files (3) \u0026rarr; DicomTagBrowser.py ]. The file is opened in MATE.\nAttention:\u0026nbsp; MATE only opens Python files if the default configuration in MeVisLab/Preferences is not changed for Supportive Programs. MATE Information:\u0026nbsp; Not only can you debug your own files, but you can also debug Python scripts of predefined MeVisLab modules. The user interface of MATE provides some relevant views for debugging.\nOutline View The Outline view shows a list of all functions defined in your currently opened script.\nProject Workspace View The Project Workspace view shows the content of the directories for all of your opened files. In this case, we only opened one file and only see the content of the directory for the DicomTagBrowser module.\nDebug Output View The Debug Output view shows the messages you also see in MeVisLab. Additional views are available as soon as we start debugging our file.\nDebug a Python Script First we need to enable debugging. In the MATE main menu, select [ Debug \u0026rarr; Enable Debugging ]. You can see some new panels appearing in MATE.\nDebugging Panel The Debugging panel allows you to step through your code.\nDebugging Panel Stack Frames Panel The Stack Frames panel shows your current stack trace while debugging.\nStack Frames Variables/Watches/Evaluate Expression Panel Another panel Variables/Watches/Evaluate Expression appears, where you can see all current local and global variables. Add your own variables to watch their current value and evaluate your own expressions.\nVariables/Watches/Evaluate Expression Scroll to line 180 and left click on the line number.\n179 def copyCurrentTagName(): \u0026gt; 180 item = ctx.control(\u0026#34;dicomTree\u0026#34;).currentItem() 181 if item: 182 MLAB.copyToPasteboard(item.text(1)) You can see a red dot marking a break point for debugging. Whenever this line of code is executed, execution will stop here and you can evaluate your variables. This line will be reached whenever you right-click on the list in the DicomTagBrowser module and select [ Copy Tag Name ].\nGo back to MeVisLab and right click on any DICOM tag in the DicomTagBrowser module. Select [ Copy Tag Name ].\nCopy Tag Name MATE opens automatically and you can see an additional yellow arrow indicating the line about to be executed next.\nMATE Debugger You can now use the controls of the Debugging panels to step through your code or just continue execution of your code. Whenever your execution is stopped, you can use the Stack Frames and the Variables/Watches/Evaluate Expression panel to see the current value of all or just watched variables.\nWe want to see the name of the DICOM tag we selected in the DicomTagBrowser module. You can access the values the following way:\nitem.text(0) # shows the tag ID (first column) item.text(1) # shows the tag Name item.text(2) # shows the tag VR item.text(3) # shows the tag Value Select Watches panel and enter item.text(1). Again copy any tag name in MeVisLab DicomTagBrowser module. You will see that MATE shows an error. The reason is that the execution stops before executing the current line of code. Your Python code in line 180 defines the variable item, and therefore it is not yet defined at this moment.\nUse the Debugging panel (fifth button Step to next line) or press F10 . The debugger jumps to the next line (181) and the variable item is defined. You can see the value of the Tag Name you just copied. You can add any variables you are interested in the same way.\nWatches panel The Variables panel now shows all currently available local and global variables including their value(s). The Stack Trace panel shows that the copyCurrentTagName function has been called after the DicomTagBrowser.MenuItem.command from the .script file of the DicomTagBrowser module.\nVariables/Watches panel Conditions for Breakpoints You can also define conditions for your breakpoints. Remove breakpoint in line 180 and set a new one in line 181. In the case you only want to stop the execution of your script if a specific condition is met, right click on your breakpoint and select [ Set Condition for Breakpoint ]. A dialog opens where you can define your condition. Enter item.text(1) == \u0026lsquo;SOPClassUID\u0026rsquo; as condition.\nConditions for Breakpoints Now, the code execution is only stopped if you copy the tag name SOPClassUID. In the case another line is copied, the execution does not stop and just continues.\nEvaluate Expression The Evaluate Expression tab allows you to modify variables during execution. In our example you can set the result item.text(1) to something like item.setText(1, \u0026ldquo;Hello\u0026rdquo;). If you now step to the next line via F10 , your watched value shows \u0026ldquo;Hello\u0026rdquo; instead of \u0026ldquo;SOPClassUID\u0026rdquo;.\nDebug9 Debug9a Summary MATE allows debugging of any Python files including files predefined in MeVisLab. Values of variables can be watched. It is possible to define conditions for breakpoints, so that the execution is only stopped if the condition is met. It is possible to change values of variables while program execution is stopped via Evaluate Expression panel. ","tags":["Advanced","Tutorial","Python","Debugging","MATE"],"section":"tutorials"},{"date":"1745280000","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/soviewportregion/","title":"Example 6: Creating Multi View Layouts Using SoViewportRegion","summary":"Example 6: Creating Multi View Layouts Using SoViewportRegion Introduction In this guide, we will show how to use the SoViewportRegion module to create custom layouts within the SoRenderArea module. This allows you to display multiple views or slices in a single window.\nWe will demonstrate how to:\nDivide the render area into multiple regions. Assign different content to each region. Use alternative methods, such as SoView2D, when applicable. Prepare Your Network Displaying Three Images in One Panel Add an ImageLoad module to your workspace and select a 3D image like ./MeVisLab/Resources/DemoData/MRI_Head.tif from the MeVisLab demo data directory. Connect an OrthoReformat3 module and add three View2D modules.\n","content":"Example 6: Creating Multi View Layouts Using SoViewportRegion Introduction In this guide, we will show how to use the SoViewportRegion module to create custom layouts within the SoRenderArea module. This allows you to display multiple views or slices in a single window.\nWe will demonstrate how to:\nDivide the render area into multiple regions. Assign different content to each region. Use alternative methods, such as SoView2D, when applicable. Prepare Your Network Displaying Three Images in One Panel Add an ImageLoad module to your workspace and select a 3D image like ./MeVisLab/Resources/DemoData/MRI_Head.tif from the MeVisLab demo data directory. Connect an OrthoReformat3 module and add three View2D modules.\nImage Display Setup Opening the three View2D module panels now shows the image data in three orthogonal views. The module OrthoReformat3 transforms the input image (by rotating and/or flipping) into the three main views commonly used.\n3 Views in 3 Viewers The module SoViewportRegion divides the render window into multiple areas, allowing different views or slices to be shown in the same window. It\u0026rsquo;s useful in medical applications, like displaying MRI or CT images from different angles (axial, sagittal, coronal) at once, making data analysis easier and faster.\nAdd three SoViewportRegion modules and connect each one to a View2D module. To display the hidden outputs of the View2D module, press SPACE and connect the output to the input of SoViewportRegion as shown below.\nConnect SoViewportRegion with View2D Add a SoRenderArea for your final result to the network and connect all three SoViewportRegion modules to it.\nThe result is that all of your viewers are initially displayed on top of each other in the bottom right corner.\nAll three viewers on top of each other This happens, because all three SoViewportRegion modules have the same settings for position and height or width.\nSoViewportRegion The SoViewportRegion module allows to define the x- and y-position and the width and height of the image in the SoRenderArea module.\nValues can be in pixels or as fractions from 0 to 1:\n0 means the start of the render area (depending on the reference) 0.5 means the center of the render area 1 means the end of the render area (depending on the reference) We want to create a layout with the following setting:\nAxial view on the left side Coronal view on the top right side Sagittal view on the bottom right side Target Layout Now, open the left SoViewportRegion module and change settings:\nX-Position and Width Left Border to 0 Right Border to 0.5 Domain Fraction of width Reference Left window border Y-Position and Height Lower Border to 1 Upper Border to 0 Domain Fraction of height Reference Upper window border Axial View Continue with the middle SoViewportRegion module and change settings:\nX-Position and Width Left Border to 0 Right Border to 0.5 Domain Fraction of width Reference Right window border Y-Position and Height Lower Border to 0.5 Upper Border to 0 Domain Fraction of smallest dimension Reference Upper window border Coronal View The right SoViewportRegion module should look as follows:\nX-Position and Width Left Border to 0.5 Right Border to 0 Domain Fraction of width Reference Right window border Y-Position and Height Lower Border to 1 Upper Border to 0.5 Domain Fraction of smallest dimension Reference Upper window border Sagittal View Displaying Four Images in One Panel In the next example, the SoRenderArea will display four views at the same time: axial, coronal, sagittal, and a 3D view.\n3D View Layout These views will be arranged in a single panel that is split into two sides with each side showing two images. To add the 3D view, insert a View3D module and connect it to the ImageLoad module. Then, connect the View3D to SoCameraInteraction, connect that to another SoViewportRegion, and finally to SoRenderArea.\n3D View Network Now, open the left SoViewportRegion module and change settings:\nX-Position and Width Left Border to 0 Right Border to 0.5 Domain Fraction of width Reference Left window border Y-Position and Height Lower Border to 0.5 Upper Border to 0 Domain Fraction of height Reference Upper window border Open the right SoViewportRegion connected to the SoCameraInteraction module and change settings:\nX-Position and Width Left Border to 0 Right Border to 0.5 Domain Fraction of width Reference Left window border Y-Position and Height Lower Border to 1 Upper Border to 0.5 Domain Fraction of height Reference Upper window border This setup will let you interact with the 3D view and display all four views together as shown in the figure below.\n3D View You will see that the orientation cube of the 3D viewer appears in the bottom right corner of the SoRenderArea. To resolve this, you can check Render delayed paths in the SoViewportRegion module of the 3D viewer.\nFinal Network Alternatively Using SoView2D In the case you want the same dataset to be visualized in multiple viewers, the module SoView2D already provides this functionality.\nInitial SoView2D Whenever you are using the SoView2D module to visualize a 2D dataset, you need to add a View2DExtensions module and, for example, a SoRenderArea module. Without the View2DExtensions module, interactions like scrolling through slices or changing the window and level settings will not be possible.\nBy default, you will see your images in a single viewer the same way as if you use the View2D module. The number of columns is defined as 1 by default. If you now change the Number of Slices to something like 3, you will see three viewers shown in a single column. As we can only connect one dataset, this network cannot display multiple series at the same time.\nMultiple slices in SoView2D Changing the number of columns to 3 and the Number of Slices to 9 results in a 3x3 layout.\nMultiple slices and columns in SoView2D By default, the module shows consecutive slices with a slice step size 1. If you change this, you can define the size of the steps between the viewers. If the resulting slice does not exist in the dataset, the viewer remains empty.\nNot existing slices You are also able to define a slab size. This means the depth of the slab. You can either use the blend mode Maximum or Blend.\nExercise You can play around with the different SoViewportRegion modules to create your own layouts by setting the values a little different.\nExercise Summary Own layouts can be created by using multiple SoViewportRegion modules. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","SoViewportRegion","Layout","Multi-View"],"section":"tutorials"},{"date":"1748908800","url":"https://mevislab.github.io/examples/pull/133/tutorials/basicmechanisms/macromodules/itemmodelview/","title":"Example 7: Creating Your Own ItemModel by Using the ItemModelView","summary":"Example 7: Creating Your Own ItemModel by Using the ItemModelView Introduction In this example, we will show how to use the ItemModelView MDL control to represent an abstract hierarchical item model with generically named attributes. You will see how to select the displayed attributes in the resulting table and how to interact with this table.\nWe create a macro module that receives an input image and then shows some selected DICOM attributes of this patient in our own ItemModelView.\n","content":"Example 7: Creating Your Own ItemModel by Using the ItemModelView Introduction In this example, we will show how to use the ItemModelView MDL control to represent an abstract hierarchical item model with generically named attributes. You will see how to select the displayed attributes in the resulting table and how to interact with this table.\nWe create a macro module that receives an input image and then shows some selected DICOM attributes of this patient in our own ItemModelView.\nPrepare Your Network Create a Macro Module Info:\u0026nbsp; Make sure to create a package first. Packages are the way MeVisLab organizes different development projects. You can find an example in chapter Package creation. Use the Project Wizard via menu entry [ File \u0026rarr; Run Project Wizard ... ] to create a new macro module named MyItemModelView.\nStart with an empty network and add a Python file.\nWe can leave the Fields empty for now. We can add them in the .script file.\nClick Create .\nItemModel_1 ItemModel_2 ItemModel_3 If you cannot find your module via Module Search, reload module cache by clicking the menu item [ Extras \u0026rarr; Reload Module Database (Clear Cache) ]\nDefine the Necessary Fields Add your new module MyItemModelView to your workspace. It does not provide a user interface and you do not have any Fields available.\nEmpty Module Open the .script file of your module via right-click and [ Related Files (4) \u0026rarr; MyItemModelView.script ].\nWe first define the input for the image. MyItemModelView.script\nInterface { Inputs { Field inImage { type = Image } } Outputs {} Parameters {} } Then, we define all attributes of the input image we want to use in our ItemModel.\nPatient PatientName PatientBirthDate Study StudyDescription StudyDate Modality Series SeriesDescription SeriesDate Image SOPInstanceUID All of them have to be defined as a Field in the Parameters section of the script.\nMyItemModelView.script\nInterface { Inputs { Field inImage { type = Image } } Outputs {} Parameters { Field id { type = Int } Field patientName { type = String } Field patientBirthdate { type = String } Field studyDescription { type = String } Field studyDate { type = String } Field modality { type = String } Field seriesDescription { type = String } Field seriesDate { type = String } Field sopInstanceUID { type = String } } } If you now open your panel, you should see the Input inImage and the just created Fields. The Field id is necessary to identify unique objects in your ItemModel later. In order to make this example easier to understand, we defined all types of the Fields as String. You can also use different types, if you like.\nModule Input and Fields Add the ItemModelView to Your Panel We can now add the ItemModelView to our panel and define the columns of the view, that we want to see. Add a Window section to your script file and define it as seen below.\nMyItemModelView.script\nWindow { Category { Vertical { ItemModelView myItemModel { name = itemModelView idAttribute = id Column id {} Column patientName {} Column patientBirthdate {} Column studyDescription {} Column studyDate {} Column modality {} Column seriesDescription {} Column seriesDate {} Column sopInstanceUID {} } } } } Every Field that we defined in the Parameters section is now used as a column in our view. The Field id has been defined to be the idAttribute. If you now open your panel, MeVisLab will complain that you did not define the Field myItemModel. You have to add a Field with this name to your Parameters section or as an Output Field. We will add an Output Field, so that our model can also be used by other modules, if necessary. The type is MLBase.\nMyItemModelView.script\nOutputs { Field myItemModel { type = MLBase } } Your module now also shows an output MLBase object and the columns you defined for the ItemModelView.\nModule Output and Columns Fill Your Table with Data We want to get the necessary information from the defined input image inImage. We want the module to update the content whenever the input image changes. Therefore, we need a Field Listener calling a Python function whenever the input image changes. Add it to your Commands section.\nMyItemModelView.script\nCommands { source = $(LOCAL)/MyItemModelView.py FieldListener inImage { command = imageChanged } } Whenever the input image changes, the Python function imageChanged is executed. Right-click on the imageChanged and select [ Create Python Function \u0026#39;imageChanged\u0026#39; ]. MATE automatically opens the Python file and creates the function.\nBefore implementing the Python function, we have to add necessary imports and global parameters.\nMyItemModelView.py\nfrom mevis import MLAB gAttributes = [\u0026#34;patientName\u0026#34;, \u0026#34;patientBirthdate\u0026#34;, \u0026#34;studyDescription\u0026#34;, \u0026#34;studyDate\u0026#34;, \u0026#34;modality\u0026#34;, \u0026#34;seriesDescription\u0026#34;, \u0026#34;seriesDate\u0026#34;, \u0026#34;sopInstanceUID\u0026#34;] gModel = None gNextId = 0 def getNextId(): global gNextId gNextId += 1 return gNextId We need to import mevis.MLAB and we define the attributes of our resulting view.\nThe unique id is an increasing Integer and we can now initialize our model.\nImplement the Model In Python, we have to define some basic classes and functions for our final model. Define a class MyItem which represents a single item. Each item may have children of the same type to provide a hierarchical structure.\nMyItemModelView.py\nclass MyItem: def __init__(self, parent=None): self.children = [] self.parent = parent self.data = {} Now, we implement a very simple and basic model named MyItemModel. Initially, we create a new MLBase object using the existing StandardItemModel and define the structure of our items as already done using the attributes.\nSome additional functions are necessary to get the root item and the selected index of the model. We also need functions to add and insert items and to clear all items.\nMyItemModelView.py\nclass MyItemModel: def __init__(self): self.model = MLAB.createMLBaseObject(\u0026#34;StandardItemModel\u0026#34;, [[\u0026#34;id\u0026#34;] + gAttributes]) self.root = MyItem() self.map = {} def makeCurrent(self): ctx.field(\u0026#34;myItemModel\u0026#34;).setObject(self.model) def getRootItem(self): return self.root def getSelectedIndex(self): ids = [int(x) for x in ctx.field(\u0026#34;selection\u0026#34;).value.split()] if ids: return self.model.findFirst(\u0026#34;id\u0026#34;, ids[0]) else: return None def addBefore(self): index = self.getSelectedIndex() if index: parent = self.model.getParent(index) pos = self.model.getChildPosition(index) else: parent = None pos = 0 self.insertItem(parent, pos, True) def addChild(self): parent = self.getSelectedIndex() pos = self.model.getChildCount(parent) self.insertItem(parent, pos) def insertItem(self, parent, pos, updateSelection=False, data=None): self.model.insertItems(parent, pos, data) self.map[data[\u0026#34;id\u0026#34;]] = data if updateSelection: ctx.field(\u0026#34;selection\u0026#34;).value = str(data[\u0026#34;id\u0026#34;]) def updateValues(self): index = self.getSelectedIndex() if index: for attr in gAttributes: value = ctx.field(attr).value if value != self.model.getData(index, attr): self.model.setData(index, attr, value) def clearAll(self): self.model.clear() You can see that the above Python code uses a field selection that contains the ID of the selected item in our table. We have to add this Field to our .script file, too.\nMyItemModelView.script\nInterface { ... Parameters { ... Field selection { type = String } ... } } Window { Category { Vertical { ItemModelView myItemModel { ... selectionField = selection ... } } } } Fill the Model With Your Data Now, we can implement the function imageChanged.\nMyItemModelView.py\ndef imageChanged(field: \u0026#34;mevislab.MLABField \u0026#34;): global gModel, gNextId gModel = MyItemModel() gNextId = 0 gModel.makeCurrent() if field.isValid(): patientName, patientBirthdate, studyDescription, studyDate, modality, seriesDescription, seriesDate, numberOfSlices = _getImageData() gModel.insertItem( None, 0, True, { \u0026#34;id\u0026#34;: getNextId(), \u0026#34;patientName\u0026#34;: patientName, \u0026#34;patientBirthdate\u0026#34;: patientBirthdate, \u0026#34;studyDescription\u0026#34;: studyDescription, \u0026#34;studyDate\u0026#34;: studyDate, \u0026#34;modality\u0026#34;: modality, \u0026#34;seriesDescription\u0026#34;: seriesDescription, \u0026#34;seriesDate\u0026#34;: seriesDate, \u0026#34;sopInstanceUID\u0026#34;: \u0026#34;\u0026#34;, }, ) tmpSelectedIndex = gModel.getSelectedIndex() for sliceNumber in range(0, numberOfSlices): instanceUid = field.getFrameSpecificDicomTag(\u0026#34;SOPInstanceUID\u0026#34;, sliceNumber) gModel.insertItem( tmpSelectedIndex, 0, True, { \u0026#34;id\u0026#34;: getNextId(), \u0026#34;patientName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;patientBirthdate\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;studyDescription\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;studyDate\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;modality\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;seriesDescription\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;seriesDate\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sopInstanceUID\u0026#34;: instanceUid.value(), }, ) Whenever the image changes, we create a new and empty model (gModel) and reset the next ID (gNextId) to 0. If the image is valid, we get the image data.\nMyItemModelView.py\ndef _getImageData(): imageField = ctx.field(\u0026#34;inImage\u0026#34;) patientName = imageField.getDicomTagValueByName(\u0026#34;PatientName\u0026#34;) patientBirthdate = imageField.getDicomTagValueByName(\u0026#34;PatientBirthDate\u0026#34;) studyDescription = imageField.getDicomTagValueByName(\u0026#34;StudyDescription\u0026#34;) studyDate = imageField.getDicomTagValueByName(\u0026#34;StudyDate\u0026#34;) modality = imageField.getDicomTagValueByName(\u0026#34;Modality\u0026#34;) seriesDescription = imageField.getDicomTagValueByName(\u0026#34;SeriesDescription\u0026#34;) seriesDate = imageField.getDicomTagValueByName(\u0026#34;SeriesDate\u0026#34;) numberOfSlices = imageField.sizeZ() return patientName, patientBirthdate, studyDescription, studyDate, modality, seriesDescription, seriesDate, numberOfSlices The image data is then used to create the root item of our model. We use the selected index of our first entry to walk through all available slices of the image and add the SOP Instance UID of each slice as a child object to our root item.\nIf you now open the panel of your module, you can already see the results.\nModule Panel The first line shows the information of the patient, the study and the series and each child item represents a single slice of the image.\nInteract With Your Model We can now add options to interact with the ItemModelView. Open the .script file of your module and go to the Commands section. We add a FieldListener to our selection field. Whenever the user selects a different item in our view, the Python function itemClicked in the FieldListener is executed.\nMyItemModelView.script\nCommands { ... FieldListener selection { command = itemClicked } ... } Before adding the new Python function, we need a function in our model that returns the values of items from our model. Implement the function getItemByID in our model the following way:\nMyItemModelView.py\ndef getItemByID(self, id): return self.map[id] It uses id to find the selected item and returns all values of this item.\nNow, add the Python function of our FieldListener to your Python script:\nMyItemModelView.py\ndef itemClicked(field: \u0026#34;mevislab.MLABField\u0026#34;): columnIndex = 8 column = gAttributes[columnIndex - 1] itemID = int(ctx.field(\u0026#34;selection\u0026#34;).value) data = gModel.getItemByID(itemID)[column] print(f\u0026#34;Click: {data}\u0026#34;) The itemClicked function uses id from the selected item to get the value of column 8 (in this case, it is the SOP Instance UID of the image) and prints this value.\nClicked Item The problem is that the Field selection also changes whenever a new item is added to the model. Your debug output is already flooded with SOP Instance UIDs without interaction.\nDebug Output Add another global parameter to your Python script to prevent the FieldListener from executing during the imageChanged event.\nMyItemModelView.py\n... gInitializing = False ... def imageChanged(field: \u0026#34;mevislab.MLABField \u0026#34;): global gModel, gNextId, gInitializing gInitializing = True ... gInitializing = False def itemClicked(field: \u0026#34;mevislab.MLABField\u0026#34;): if not gInitializing: ... While the imageChanged function is executed, the parameter is set to False and the itemClicked function does not print anything.\nSummary ItemModelViews allow you to define your own abstract hierarchical item model with generically named attributes. This model can be provided as Output and added to the Panel of your module. Interactions with the model can be implemented by using a FieldListener. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","ItemModel","ItemModelView"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/openinventor/","title":"Chapter II: Open Inventor","summary":"Open Inventor Modules Introduction In total, there are three types of modules:\nblue ML modules brown macro modules green Open Inventor modules The names of Open Inventor modules start with the prefix So\\* (for Scene Objects). Open Inventor modules process and render 3D scene objects and enable image interactions. Scene objects are transmitted using the semicircle-shaped input and output connectors. With the help of these modules, Open Inventor scenes can be implemented.\n","content":"Open Inventor Modules Introduction In total, there are three types of modules:\nblue ML modules brown macro modules green Open Inventor modules The names of Open Inventor modules start with the prefix So\\* (for Scene Objects). Open Inventor modules process and render 3D scene objects and enable image interactions. Scene objects are transmitted using the semicircle-shaped input and output connectors. With the help of these modules, Open Inventor scenes can be implemented.\nAn exemplary Open Inventor scene will be implemented in the following paragraph.\nOpen Inventor Scenes and Execution of Scene Graphs Inventor scenes are organized in structures called scene graphs. A scene graph is made up of nodes, which represent 3D objects to be drawn, properties of the 3D objects, nodes that combine other nodes and are used for hierarchical grouping, and others (cameras, lights, etc.). These nodes are accordingly called shape nodes, property nodes, group nodes, and so on. Each node contains one or more pieces of information stored in fields. For example, the SoSphere node contains only its radius, stored in its radius field. Open Inventor modules function as Open Inventor nodes, so they may have input connectors to add Open Inventor child nodes (modules) and output connectors to link themselves to Open Inventor parent nodes (modules).\nExecution order in Open Inventor scenes:\u0026nbsp; The model below depicts the order in which the modules are traversed. The red arrow indicates the traversal order: from top to bottom and from left to right. The modules are numbered accordingly from 1 to 8. Knowing about the traversal order can be crucial to achieve a certain ouput. Traversing through a network of Open Inventor modules SoGroup and SoSeparator The SoGroup and SoSeparator modules can be used as containers for child nodes. They both allow multiple inputs and combine the results in one single output as seen above. Nevertheless, there is a big difference in handling the traversal state of the scene graph.\nSoGroup vs. SoSeparator In the network above, we render four SoCone objects. The left side uses the SoSeparator modules, the right side uses the SoGroup ones. There is a SoMaterial module defining one of the left cone objects to be yellow. As you can see, the SoMaterial module is only applied to that cone, the other left cone remains in its default gray color, because the SoSeparator module isolates the separator\u0026rsquo;s children from the rest of the scene graph.\nOn the right side, we are using SoGroup ( SoGroup module reference ). The material of the cone is set to be of red color. As the SoGroup module does not alter the traversal state in any way, the second cone in this group is also colored in red.\nCheck:\u0026nbsp; Be aware of some Open Inventor modules altering the traversal order. If your scene turns out to differ from your expected result, check whether incorporated SoSeparator modules are the cause. Details on these can be found in the SoSeparator module reference \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. More information about Open Inventor and Scene Graphs can be found here , in the Open Inventor Overview or the Open Inventor Reference.\n","tags":["Beginner","Tutorial","Open Inventor","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/openinventor/openinventorobjects/","title":"Example 1: Open Inventor Objects","summary":"Example 1: Open Inventor Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to construct an Open Inventor scene in which we display three 3D objects of different color and shape.\nSteps to Do Generating Open Inventor Objects First, add the modules SoExaminerViewer and SoCone to the workspace and connect both modules as shown. The module SoCone creates a cone shaped object, which can be displayed in the viewer SoExaminerViewer.\n","content":"Example 1: Open Inventor Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to construct an Open Inventor scene in which we display three 3D objects of different color and shape.\nSteps to Do Generating Open Inventor Objects First, add the modules SoExaminerViewer and SoCone to the workspace and connect both modules as shown. The module SoCone creates a cone shaped object, which can be displayed in the viewer SoExaminerViewer.\nSoExaminerViewer We like to change the color of the cone. In order to do so, add the module SoMaterial to the workspace and connect the module as shown below. When creating an Open Inventor scene (by creating networks of Open Inventor modules), the sequence of module connections, in this case the sequence of the inputs to the module SoExaminerViewer, determines the functionality of the network.\nOpen Inventor modules are executed like scene graphs. This means modules are executed from top to bottom and from left to right. Here, it is important to connect the module SoMaterial to an input on the left side of the connection between SoCone and SoExaminerViewer. With this, we first select features like a color and these features are then assigned to all objects, which were executed afterward. Now, open the panel of the module SoMaterial and select any Diffuse Color you like. Here, we choose green.\nColors and Material in Open Inventor We like to add a second object to the scene.\nIn order to do that, add the module SoSphere to the workspace. Connect this module to SoExaminerViewer. When connecting SoSphere to an input on the right side of the connection between the viewer and the module SoMaterial, the sphere is also colored in green. One problem now is, that currently both objects are displayed at the same position.\nAdding a SoSphere They display both objects at different positions, add the modules SoSeparator and SoTransform to the scene and connect both modules shown on the following picture. Open the panel of SoTransform and implement a translation in the x-direction to shift the object. Now you can examine two things:\nThe sphere loses its green color The cone is shifted to the side Transformation The module SoTransform is responsible for shifting objects, in this case the cone, to the side. The module SoSeparator ensures that only the cone is shifted and also only the cone is colored in green. It separates this features from the rest of the scene.\nWe like to add a third object, a cube, and shift it to the other side of the sphere. Add the modules SoCube and SoTransform to the workspace and connect both modules as shown below. To shift the cube to the other side of the sphere, open the panel of SoTransform and adjust the Translation in the x-direction. The sphere is not affected by the translation, as the connection from SoTransform1 to SoExaminerViewer is established on the right side of the connection between SoSphere and SoExaminerViewer.\nAdding a SoCube Again, we use the module SoMaterial to select a color for the cone and the sphere.\nMultiple Materials For an easier handling, we group an object together with its features by using the module SoGroup. This does not separate features, which is the reason for the cube to be colorized. All modules that are derived from SoGroup offer a basically infinite number of input connectors (a new connector is added for every new connection).\nSoGroup If we do not want to colorize the cube, we have to exchange the module SoGroup for another SoSeparator module.\nSoSeparator The implementation of all objects can be grouped together.\nGrouping In addition to the objects, a background can be added to the scene using the module SoBackground.\nSoBackground Summary Scene objects are represented by nodes. Size and position is defined by transformation nodes. A rendering node represents the root of the scene graph. Nodes are rendered in the order of traversal. Nodes on the same level are traversed from left to right. All modules that are derived from SoGroup offer a basically infinite number of input connectors (a new connector is added for every new connection). \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Open Inventor","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/openinventor/mouseinteractions/","title":"Example 2: Mouse Interactions in Open Inventor","summary":"Example 2: Mouse Interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we implement some image or object interactions. We will create a 3D scene, in which we display a cube and change its size using the mouse. We also get to know another viewer, the module SoExaminerViewer. This viewer is important. It enables the rendering of Open Inventor scenes and allows interactions with the Open Inventor scenes.\n","content":"Example 2: Mouse Interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we implement some image or object interactions. We will create a 3D scene, in which we display a cube and change its size using the mouse. We also get to know another viewer, the module SoExaminerViewer. This viewer is important. It enables the rendering of Open Inventor scenes and allows interactions with the Open Inventor scenes.\nSteps to Do Develop Your Network For implementing the example, build the following network. We already know the module SoCube, which builds a 3D scene object forming a cube. In addition to that, add the module SoMouseGrabber. Connect the modules as shown below.\nExtra Infos:\u0026nbsp; Additional information about the SoMouseGrabber can be found here: SoMouseGrabber SoMouseGrabber Configure Mouse Interactions Now, open the panels of the module SoMouseGrabber and the module SoExaminerViewer, which displays a cube. In the viewer, press the right button of your mouse and move the mouse around. This action can be seen in the panel of the module SoMouseGrabber.\nAttention:\u0026nbsp; Make sure to configure SoMouseGrabber fields as seen below. SoMouseGrabber You can see:\nButton 3, the right mouse button , is tagged as being pressed Changes of the mouse coordinates are displayed in the box Output Mouse Interactions Resize Cube via Mouse Interactions We like to use the detected mouse movements to change the size of our cube. In order to that, open the panel of SoCube. Build parameter connections from the mouse coordinates to the width and depth of the cube.\nChange Cube Size With Mouse Events If you now press the right mouse button in the viewer and move the mouse around, the size of the cube changes.\nExercises Change location of the cube via Mouse Interactions by using the Module SoTransform. Add more objects to the scene and interact with them. Summary The module SoExaminerViewer enables the rendering of Open Inventor scenes and allows interactions with the Open Inventor scenes. Mouse interactions can be applied to the objects in the scene. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Open Inventor","3D","Mouse Interactions"],"section":"tutorials"},{"date":"1679443200","url":"https://mevislab.github.io/examples/pull/133/tutorials/openinventor/camerainteraction/","title":"Example 3: Camera Interactions in Open Inventor","summary":"Example 3: Camera Interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are learning the basic principles of camera interactions in Open Inventor. We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nThe SoRenderArea Module The module SoRenderArea is a simple renderer for Open Inventor scenes. It offers functionality to record movies and to create snapshots, but does not include an own camera or light.\n","content":"Example 3: Camera Interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are learning the basic principles of camera interactions in Open Inventor. We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nThe SoRenderArea Module The module SoRenderArea is a simple renderer for Open Inventor scenes. It offers functionality to record movies and to create snapshots, but does not include an own camera or light.\nAdd a SoBackground, a SoMaterial and a SoOrientationModel module to your workspace and connect them to a SoGroup. Add a SoRenderArea to the SoGroup and open the viewer.\nSoRenderArea without camera and lights You cannot interact with your scene and the rendered content is very dark. Open the SoOrientationModel and change Model to Skeleton to see that a little bit better. You can also change the material by using the panel of the SoMaterial module.\nAdd a SoCameraInteraction module and connect it between the SoGroup and the SoRenderArea.\nSoRenderArea with SoCameraInteraction The SoCameraInteraction does not only allow you to change the camera position in your scene but also adds light. The module automatically adds a headlight that you can switch off with a field of the module.\nHeadlight_TRUE Headlight_FALSE The SoCameraInteraction can also be extended by a SoPerspectiveCamera or a SoOrthographicCamera. Add a SoSwitch to your SoGroup and connect a SoPerspectiveCamera and a SoOrthographicCamera.\nSoPerspectiveCamera and SoOrthographicCamera You can now switch between both cameras, but you cannot interact with them in the viewer. Select the SoCameraInteraction and toggle detectCamera. Now the default camera of the SoCameraInteraction is replaced by the camera selected in the SoSwitch.\nWhenever you change the camera in the switch, you need to detect the new camera in the SoCameraInteraction.\nSoPerspectiveCamera SoOrthographicCamera A SoPerspectiveCamera camera defines a perspective projection from a viewpoint.\nThe viewing volume for a perspective camera is a truncated pyramid. By default, the camera is located at (0, 0, 1) and looks along the negative z-axis; the Position and Orientation fields can be used to change these values. The Height Angle field defines the total vertical angle of the viewing volume; this and the Aspect Ratio field determine the horizontal angle.\nA SoOrthographicCamera camera defines a parallel projection from a viewpoint.\nThis camera does not diminish objects with distance as an SoPerspectiveCamera does. The viewing volume for an orthographic camera is a cuboid (a box).\nBy default, the camera is located at (0, 0, 1) and looks along the negative z-axis; the Position and Orientation fields can be used to change these values. The Height field defines the total height of the viewing volume; this and the Aspect Ratio field determine its width.\nAdd a SoCameraWidget and connect it to your SoGroup.\nSoCameraWidget This module shows a simple widget on an Inventor viewer that can be used to rotate, pan, or zoom the scene. You can configure the Main Interaction in the panel of the SoCameraWidget.\nYou can also add more than one widget to show multiple widgets in the same scene, see example network of the SoCameraWidget module.\nThe SoExaminerViewer Module The SoExaminerViewer makes some things much easier, because a camera and a light are already integrated.\nAdd a SoExaminerViewer to your workspace and connect it to the SoBackground, the SoMaterial and the SoOrientationModel modules.\nSoExaminerViewer The difference to the SoRenderArea can be seen immediately. You can interact with your scene and a light is available initially.\nThe module also allows you to switch between perspective and orthographic camera by changing the field cameraType.\nSoExaminerViewer_Perspective SoExaminerViewer_Orthographic The module also provides UI elements to interact.\nSummary MeVisLab provides multiple options for adding a camera to a scene. The SoExaminerViewer already has an integrated camera and light, the SoRenderArea requires additional modules. You can use perspective and orthographic cameras. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Open Inventor","3D","Camera","Perspective Camera","Orthographic Camera"],"section":"tutorials"},{"date":"1714726353","url":"https://mevislab.github.io/examples/pull/133/tutorials/openinventor/posteffectsinopeninventor/","title":"Example 4: Post Effects in Open Inventor","summary":"Example 4: Post Effects in Open Inventor Introduction Up to this point, we practiced constructing Open Inventor scenes and placed three-dimensional Open Inventor objects of different colors and shapes within them.\nIn this tutorial, we will go over the steps to add shadows to our 3D objects, make them glow, and vary their opacity to make them transparent. We will also incorporate WEMs from multi-frame DICOMs and render them as scene objects to see how different post effects can be used on them.\n","content":"Example 4: Post Effects in Open Inventor Introduction Up to this point, we practiced constructing Open Inventor scenes and placed three-dimensional Open Inventor objects of different colors and shapes within them.\nIn this tutorial, we will go over the steps to add shadows to our 3D objects, make them glow, and vary their opacity to make them transparent. We will also incorporate WEMs from multi-frame DICOMs and render them as scene objects to see how different post effects can be used on them.\nSteps to Follow From DICOM to Scene Object To incorporate DICOMs into your Open Inventor Scene, they have to be rendered as Open Inventor objects, which can be done by converting them into WEMs first. Begin by adding the modules LocalImage, WEMIsoSurface, and SoWEMRenderer to your workspace. Open the panel of the LocalImage module, browse your files, and choose a DICOM with multiple frames as input data. Connect the LocalImage module\u0026rsquo;s output connector to WEMIsoSurface module\u0026rsquo;s input connector to create a WEM of the study\u0026rsquo;s surface. Then, connect the WEMIsoSurface module\u0026rsquo;s output connector to the SoWEMRenderer module\u0026rsquo;s input connector to render a scene object that can be displayed by adding a SoExaminerViewer module to the workspace and connecting the SoWEMRenderer module\u0026rsquo;s output connector to its input connector.\nCheck:\u0026nbsp; We don\u0026rsquo;t recommend using single-frame DICOMs for this example as a certain depth is required to interact with the scene objects as intended. Also make sure that the pixel data of the DICOM file you choose contains all slices of the study, as it might be difficult to arrange scene objects of individual slices to resemble the originally captured study. How to create a scene object out of a multi-frame DICOM Info:\u0026nbsp; Consider adding a View2D and an Info module to your LocalImage module\u0026rsquo;s output connector to be able to compare the rendered object with the original image and adapt the isovalues to minimize noise. PostEffectShader To apply shading to our DICOM scene object, add a SoShaderPipeline and a SoShaderPipelineCellShading module to our network and connect their output connectors to a SoToggle module\u0026rsquo;s input connector. Then, connect the SoToggle module\u0026rsquo;s output connector to the SoExaminerViewer, but on the left side of the connection to the SoWEMRenderer module. This way, shading can be toggled and is applied to all scene objects connected to the right of the SoToggle module\u0026rsquo;s connection.\nShading toggled off Shading toggled on Tidying Your Workspace and Preparing the Next Steps Now, add a SoPostEffectBackground module to your workspace and connect its output connector to the SoExaminerViewer module\u0026rsquo;s input connector. Group the modules SoToggle, SoShaderPipeline, and SoShaderPipelineCellShading together and name the group \u0026ldquo;Toggle Shading\u0026rdquo;. Then, group the modules SoWEMRenderer, WEMIsoSurface, and LocalImage together and name the group \u0026ldquo;DICOM Object\u0026rdquo;.\nInfo:\u0026nbsp; Structuring the workspace by grouping modules based on their functionality helps to stay focused and keeps everything tidy. Use a SoPostEffectMainGeometry module to connect both of the groups you just created to the SoExaminerViewer module. Lastly, add a SoPostEffectRenderer module to your workspace and connect its output connector to the SoExaminerViewer module\u0026rsquo;s input connector.\nGrouped modules You can now change your Open Inventor scene\u0026rsquo;s background color.\nPostEffectEdges Add the module SoPostEffectEdges to your workspace and connect its output connector with the SoExaminerViewer module\u0026rsquo;s input connector.\nThen, open its panel and choose a color. You can try different modes, sampling distances and thresholds:\nColored edges Varying settings of colored edges Varying settings of colored edges PostEffectGeometry To include geometrical objects in your Open Inventor scene, add two SoSeparator modules to the workspace and connect them to the input connector of SoPostEffectMainGeometry. Then, add a SoMaterial, SoTransform, and SoSphere or SoCube module to each SoSeparator and adjust their size (using the panel of the SoSphere or SoCube module) and placement within the scene (using the panel of the SoTransform module) as you like.\nCheck:\u0026nbsp; You\u0026rsquo;ll observe that the transparency setting in the SoMaterial module does not apply to the geometrical objects. Add a SoPostEffectTransparentGeometry module to your workspace, connect its output connector to the SoExaminerViewer module\u0026rsquo;s input connector and its input connectors to the SoSeparator module\u0026rsquo;s output connector to create transparent geometrical objects in your scene. Workspace PostEffectGlow To put a soft glow on the geometrical scene objects, the module SoPostEffectGlow can be added to the workspace.\nApplied SoPostEffectGlow Summary Multi-frame DICOM images can be rendered to be scene objects by converting them into WEMs first. Open Inventor scenes can be augmented by adding PostEffects to scene objects. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Open Inventor","Post Effects"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/","title":"Chapter III: Visualization","summary":"Visualization in MeVisLab Introduction Images and data objects can be rendered in 2D and 3D and interacted with in several ways using a set of tools available through MeVisLab. In this chapter in particular, we will focus on simple image interaction with two- and three-dimensional visualizations.\nInfo:\u0026nbsp; Not only pixel- and voxel-based data, but also scene objects and 3D scenes can be visualized. See our tutorial on OpenInventorModules for further information. View2D and View3D An easy way to display data and images in 2D and 3D is by using them modules View2D and View3D. What can be done with these viewers?\n","content":"Visualization in MeVisLab Introduction Images and data objects can be rendered in 2D and 3D and interacted with in several ways using a set of tools available through MeVisLab. In this chapter in particular, we will focus on simple image interaction with two- and three-dimensional visualizations.\nInfo:\u0026nbsp; Not only pixel- and voxel-based data, but also scene objects and 3D scenes can be visualized. See our tutorial on OpenInventorModules for further information. View2D and View3D An easy way to display data and images in 2D and 3D is by using them modules View2D and View3D. What can be done with these viewers?\nView2D and View3D View2D Scroll through the slices using the mouse wheel and/or middle mouse button .\nChange the contrast of the image by clicking the right mouse button and moving the mouse.\nZoom in and out by pressing CTRL and middle mouse button .\nToggle between multiple timepoints (if available) via \u0026larr; ArrowLeft and \u0026rarr; ArrowRight .\nMore features can be found on the help page.\nInfo:\u0026nbsp; The View2DExtensions module provides additional ways to interact with an image. View2DExtensions View3D Zoom in and out using the mouse wheel .\nDrag the 3D objects using the middle mouse button .\nChange the contrast of the image by clicking the right mouse button and moving the mouse.\nRotate the object by pressing the left mouse button and moving the object around. The present orientation is displayed by a cube in the bottom right corner.\nMore features, like recording movies, can be found on the help page.\nToggle between multiple timepoints (if available) via \u0026larr; ArrowLeft and \u0026rarr; ArrowRight .\nInfo:\u0026nbsp; More information on Image Processing in MeVisLab can be found here ","tags":["Beginner","Tutorial","Visualization","2D","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample1/","title":"Example 1: Synchronous View of Two Images","summary":"Example 1: Synchronous View of Two Images Introduction In this example we like to use the module SynchroView2D to be able to inspect two different images simultaneously.\nThe module SynchroView2D provides two 2D viewers that are synchronized.\nAs in the tutorial Chapter 1 - Basic Mechanics of MeVisLab, the processed and the unprocessed image can be displayed simultaneously. Scrolling through one image automatically changes the slices of both viewers, so slices with the same slice number are shown in both images.\n","content":"Example 1: Synchronous View of Two Images Introduction In this example we like to use the module SynchroView2D to be able to inspect two different images simultaneously.\nThe module SynchroView2D provides two 2D viewers that are synchronized.\nAs in the tutorial Chapter 1 - Basic Mechanics of MeVisLab, the processed and the unprocessed image can be displayed simultaneously. Scrolling through one image automatically changes the slices of both viewers, so slices with the same slice number are shown in both images.\nThe difference is that we are now using an already existing module named SynchroView2D.\nExtra Infos:\u0026nbsp; The SynchroView2D module is explained here Steps to Do Develop Your Network Start the example by adding the module LocalImage to your workspace to load the example image Tumor1_Head_t1.small.tif. Next, add and connect the following modules as shown.\nSynchroView2D Viewer Summary Multiple images can be synchronized by the SynchroView2D module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample2/","title":"Example 2: Creating a Magnifier","summary":"Example 2: Creating a Magnifier \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Medical images are typically displayed in three different viewing directions (see image): coronal, axial, and sagittal.\nUsing the viewer OrthoView2D, you are able to decide which viewing direction you like to use. In addition to that, you have the opportunity to display all three orthogonal viewing directions simultaneously. Here, we like to display an image of the head in all three viewing directions and mark positions in the image.\n","content":"Example 2: Creating a Magnifier \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Medical images are typically displayed in three different viewing directions (see image): coronal, axial, and sagittal.\nUsing the viewer OrthoView2D, you are able to decide which viewing direction you like to use. In addition to that, you have the opportunity to display all three orthogonal viewing directions simultaneously. Here, we like to display an image of the head in all three viewing directions and mark positions in the image.\nBody Planes Steps to Do Develop Your Network In this example, use the module LocalImage to load the example image MRI_Head.tif. Now, connect the module OrthoView2D to the loaded image. The image is displayed in three orthogonal viewing directions. The yellow marker displays the same voxel in all three images. You can scroll through the slices in all three viewing directions.\nExtra Infos:\u0026nbsp; In the case your image is black, change the Window and Center values by moving the mouse with right mouse button\npressed.\nOrthoView2D SoView2DPosition Next, we add the module SoView2DPosition (an Open Inventor module).\nThe module enables the selection of an image position via mouse click . The last clicked location in the viewer is marked in white. If you now scroll through the slices, both the last clicked location and the current image location are shown.\nSoView2DPosition SoView2DRectangle Instead of points, we like to mark areas. In order to do that, replace the module SoView2DPosition with the module SoView2DRectangle. The module allows to add a rectangle to the image. Left-click on the image and draw a rectangle. In the OthoView2D, the rectangle is displayed in every viewing direction.\nSoView2DRectangle Using a Rectangle to Build a Magnifier We like to use the module SoView2DRectangle to create a magnifier. In order to do that, add the following modules to your workspace and connect them as shown below. We need to connect the module SoView2DRectangle to a hidden input connector of the module SynchroView2D. To be able to do this, click on your workspace and afterward press SPACE . You can see that SynchroView2D possesses Open Inventor input connectors. You can connect your module SoView2DRectangle to one of these connectors.\nHidden Inputs of SynchroView2D Connect Hidden Inputs of SynchroView2D In addition to that, add two instances of the module DecomposeVector3 to your network. In MeVisLab, different data types exist, for example, vectors, or single variables, which contain the data type float or integer. This module can be used to convert field values of type vector (in this case, a vector consisting of three entries) into three single coordinates. You will see in the next step why this module can be useful.\nDecomposeVector3 We like to use the module SubImage to select a section of a slice, which is then displayed in the viewer. The idea is to display a magnified section of one slice next to the whole slice in the module SynchroView2D. In order to do that, we need to tell the module SubImage which section to display in the viewer. The section is selected by using the module SoView2DRectangle. As a last step, we need to transmit the coordinates of the chosen rectangle to the module SubImage. To do that, we will build some parameter connections.\nSubImage Now, open the panels of the modules SoView2DRectangle, DecomposeVector3, and DecomposeVector31.\nWe rename the DecomposeVector3 modules (press F2 to do that) here for a better overview.\nIn the panel of the module Rectangle in the box Position, you can see the position of the rectangle given in two 3D vectors.\nWe like to use the modules DecomposeVector3 to extract the single x, y, and z values of the vector. For that, create a parameter connection from the field Start Wold Pos to the vector of the module we named StartWorldPos_Rectangle and create a connection from the field End World Pos to the vector of module EndWorldPos_Rectangle. The decomposed coordinates can be now used for further parameter connections.\nParameter Connections Open the panel of the module SubImage. Select the Mode World Start \u0026amp; End (Image Axis Aligned). Enable the function Auto apply.\nExtra Infos:\u0026nbsp; Make sure to also check Auto-correct for negative subimage extents, so that you can draw rectangles from left to right and from right to left. World Coordinates Now, create parameter connections from the fields X, Y, Z of the module StartWorldPos_Rectangle to the field Start X, Start Y, Start Z in the panel of the module SubImage. Similarly, connect the parameter fields X, Y, Z of the module EndWorldPos_Rectangle to the field End X, End Y, End Z in the panel of the module SubImage.\nAnother Parameter Connection With this, you finished your magnifier. Open the viewer and draw a rectangle on one slice to see the result.\nFinal Magnifier with SubImage Exercises Invert the image inside your magnified SubImage without changing the original image. You can use Arithmetic* modules for inverting.\nSummary The module OrthoView2D provides coronal, axial, and sagittal views of an image. The SubImage module allows to define a region of an input image to be treated as a separate image. Single x, y, and z coordinates can be transferred to a 3-dimensional vector and vice versa by using ComposeVector3 and DecomposeVector3. Some modules provide hidden inputs and outputs that can be shown via SPACE . \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D","Magnifier"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample3/","title":"Example 3: Image Overlays","summary":"Example 3: How to Blend Images Over Each Other \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will show you how to blend a 2D image over another one. With the help of the module SoView2DOverlay we will create an overlay, which allows us to highlight all bones in the scan.\nSteps to Do Develop Your Network Start this example by adding the shown modules, connecting the modules to form a network and loading the example image Bone.tiff.\n","content":"Example 3: How to Blend Images Over Each Other \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will show you how to blend a 2D image over another one. With the help of the module SoView2DOverlay we will create an overlay, which allows us to highlight all bones in the scan.\nSteps to Do Develop Your Network Start this example by adding the shown modules, connecting the modules to form a network and loading the example image Bone.tiff.\nOpen the panel of the module Threshold and configure the module as shown below.\nExtra Infos:\u0026nbsp; The Threshold module is explained here The module Threshold compares the value of each voxel of the image with a customizable threshold. In this case: If the value of the chosen voxel is lower than the threshold, the voxel value is replaced by the minimum value of the image. If the value of the chosen voxel is higher than the threshold, the voxel value is replaced by the maximum value of the image. With this, we can construct a binary image that divides the image into bone (white) and no bone (black).\nSelect output of the Threshold module to see the binary image in Output Inspector.\nImage Threshold Overlays The module SoView2DOverlay blends a 2D image over another one in a 2D viewer. In this case, all voxels with a value above the Threshold are colored and therefore highlighted. The colored voxels are then blended over the original image. Using the panel of SoView2DOverlay, you can select the color of the overlay.\nSoView2DOverlay Extra Infos:\u0026nbsp; The SoView2DOverlay module is explained here Exercises Play around with different Threshold values and SoView2DOverlay colors. Visualize your generated threshold mask in 3D by using the View3D module. Summary The module Threshold applies a relative or an absolute threshold to a voxel image. The module SoView2DOverlay blends an 2D image over another one in a 2D viewer. You can also use a 3D SoRenderArea for the same visualizations. An example can be seen in the next Example 4. Warning:\u0026nbsp; The SoView2DOverlay module is not intended to work with OrthoView2D; in this case, use a GVROrthoOverlay. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D","Overlays","Masks"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample4/","title":"Example 4: Display 2D Images in Open Inventor SoRenderArea","summary":"Example 4: Display Images Converted to Open Inventor Scene Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous example you learned how to use the module SoView2DOverlay together with a View2D. MeVisLab provides a whole family of SoView2D* modules (SoView2DOverlay, SoView2DRectangle, SoView2DGrid, \u0026hellip;). All these modules create or interact with scene objects and are based on the module SoView2D, which can convert a voxel image into a scene object. In this example, you will get to know some members of the SoView2D family.\n","content":"Example 4: Display Images Converted to Open Inventor Scene Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous example you learned how to use the module SoView2DOverlay together with a View2D. MeVisLab provides a whole family of SoView2D* modules (SoView2DOverlay, SoView2DRectangle, SoView2DGrid, \u0026hellip;). All these modules create or interact with scene objects and are based on the module SoView2D, which can convert a voxel image into a scene object. In this example, you will get to know some members of the SoView2D family.\nExtra Infos:\u0026nbsp; More information about the SoView2D family can be found here\nand in the SoView2D Reference\n.\nSteps to Do Develop Your Network We will start the example by creating an overlay again. Add the following modules and connect them as shown. Select a Threshold and a Comparison Operator for the module Threshold as in the previous example. The module SoView2D converts the image into a scene object. The image as well as the overlay is rendered and displayed by the module SoRenderArea.\nSoRenderArea Add Extension You may have noticed that you are not able to scroll through the slices. This functionality is not implemented in the viewer SoRenderArea. To add a set of functionalities and viewer extensions, which are commonly used in conjunction with a 2D viewer, add the module View2DExtensions to the workspace and connect it as shown below. Now, additional information of the image can be displayed in the viewer and you can navigate and scroll through the slices.\nView2DExtensions Add Screenshot Gallery to Views Area With the help of the module SoRenderArea you can record screenshots and movies. Before we do that, open [ View \u0026rarr; Views \u0026rarr; Screenshot Gallery ], to add the Screenshot Gallery to your views area.\nScreenshot Gallery Create Screenshots and Movies If you now select your favorite slice of the bone in the viewer SoRenderArea and press F11 , a screenshot is taken and displayed in the Screenshot Gallery. For recording a movie, press F9 to start the movie and F10 to stop recording. You can find the movie in the Screenshot Gallery.\nRecord Movies and Snapshots Exercises Create movies of a 3D scene. Summary Modules of the SoView2D family create or interact with scene objects and are based on the module SoView2D, which can convert a voxel image into a scene object. The SoRenderArea module provides functionalities for screenshots and movie generation. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D","3D","Open Inventor","Snapshots","Movies"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample5/","title":"Example 5: Volume Rendering and Interactions","summary":"Example 5: Volume Rendering and Interactions \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to convert a scan of a head into a 3D scene object. The scene object allows to add some textures, interactions, and animations.\nSteps to Do Develop Your Network Implement the following network and open the image $(DemoDataPath)/BrainMultiModal/ProbandT1.tif.\nSoGVRVolumeRenderer The module SoGVRVolumeRenderer allows volume rendering of 3D and 4D images.\n","content":"Example 5: Volume Rendering and Interactions \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to convert a scan of a head into a 3D scene object. The scene object allows to add some textures, interactions, and animations.\nSteps to Do Develop Your Network Implement the following network and open the image $(DemoDataPath)/BrainMultiModal/ProbandT1.tif.\nSoGVRVolumeRenderer The module SoGVRVolumeRenderer allows volume rendering of 3D and 4D images.\nExtra Infos:\u0026nbsp; Additional information about Volume Rendering can be found here: Giga Voxel Renderer Change LUT We like to add a surface color to the head. In order to do that, we add the module SoLUTEditor, which adds an RGBA lookup table (LUT) to the scene. Connecting this module to SoExaminerViewer to the left of the connection between SoGVRRenderer and SoExaminerViewer (remember the order in which Open Inventor modules are executed) allows you to set the surface color of the head.\nSoLUTEditor To change the color, open the panel of SoLUTEditor. In this editor we can change color and transparency interactively (for more information, see the help page ). Here, we have a range from black to white and from complete transparency to full opacity.\nSoLUTEditor change colors We now like to add color. New color points can be added by clicking on the color bar at the bottom side of the graph and existing points can be moved by dragging. You can change the color of each point under Color.\nSoLUTEditor add colors Interactions As a next step, we add some dynamics to the 3D scene: We like to rotate the head. In order to do this, add the modules SoRotationXYZ and SoElapsedTime to the workspace and connect the modules as shown.\nSoRotationXYZ Open the panels of both modules and select the axis the image should rotate around. In this case, the z-axis was selected. Now, build a parameter connection from the parameter Time out of the module SoElapsedTime to the parameter Angle of the module SoRotationXYZ. The angle changes with time and the head starts turning.\nTime and Angle Exercises Change the rotation speed. change the rotation angle. Pause the rotation on pressing SPACE . Summary The module SoGVRVolumeRenderer renders paged images like DICOM files in a GVR. Lookup tables (LUT) allow you to modify the color of your renderings. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","3D","Volume Rendering","GVR","LUT"],"section":"tutorials"},{"date":"1677110400","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample6/","title":"Example 6: MeVis Path Tracer","summary":"Example 6: MeVis Path Tracer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;We have a Short video showing the possibilities of the MeVis Path Tracer on YouTube. Introduction The MeVis Path Tracer offers a Monte Carlo Path Tracing framework running on CUDA GPUs. It offers photorealistic rendering of volumes and meshes, physically based lightning with area lights and soft shadows and fully integrates into MeVisLab Open Inventor (camera, depth buffer, clipping planes, etc.).\nExtra Infos:\u0026nbsp; CUDA is a parallel computing platform and programming model created by NVIDIA. For further information, see NVIDIA website. PathTracer1 ","content":"Example 6: MeVis Path Tracer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;We have a Short video showing the possibilities of the MeVis Path Tracer on YouTube. Introduction The MeVis Path Tracer offers a Monte Carlo Path Tracing framework running on CUDA GPUs. It offers photorealistic rendering of volumes and meshes, physically based lightning with area lights and soft shadows and fully integrates into MeVisLab Open Inventor (camera, depth buffer, clipping planes, etc.).\nExtra Infos:\u0026nbsp; CUDA is a parallel computing platform and programming model created by NVIDIA. For further information, see NVIDIA website. PathTracer1 PathTracer2 PathTracer3 PathTracer4 PathTracer5 The SoPathTracer module implements the main renderer (like the SoGVRVolumeRenderer). It collects all SoPathTracer* extensions (on its left side) in the scene and renders them. Picking is also supported, but it supports currently only the first hit position instead of a full hit profile. It supports an arbitrary number of objects with different orientation and bounding boxes.\nPath Tracing Path Tracing allows interactive, photorealistic 3D environments with dynamic light and shadow, reflections, and refractions.\nTraditional volume rendering is a technique used to visualize 3D volumetric data by rendering 2D images of the data from different viewpoints. It typically uses a transfer function that maps the scalar values of the volume to colors and opacities, which are then used to create a 2D image of the volume. This technique can produce visually pleasing images of volumetric data, but it can struggle with complex lighting and shadow effects, and it may not accurately capture the scattering and absorption of light within the volume.\nMonte Carlo path tracing is a technique used to simulate the behavior of light in a scene by tracing rays of light as they bounce around the scene and interact with objects. It uses statistical methods to simulate the behavior of light, making it more accurate than traditional volume rendering for simulating the scattering and absorption of light within the volume. However, path tracing can be computationally expensive, as it requires many iterations to produce a high-quality image.\nRay tracing is a technique for modelling light transport. It follows all light rays throughout the entire scene. Depending on the scene, this takes a lot of time to fully compute the resulting pixels. In contrast to ray tracing, path tracing only traces the most likely path of the light by using the Monte Carlo method. Computation is much faster but the results are comparable.\nExtra Infos:\u0026nbsp; For more information about Path Tracing, see the NVIDIA website. Modules The SoPathTracer is the main renderer of the framework and should always appear on the right in your scene. It collects the current Open Inventor camera and clipping planes and uses them when rendering.\nThere are various extensions that can be used.\nVolumes SoPathTracerVolume loads and renders a volume, multiple volumes with arbitrary world coordinates are supported.\nVolumes support: Diffuse/emissive/material LUT Shading options Subvolume mixing Additional transformation matrix Material selection SoPathTracerMaskVolume can be used to mask voxels in SoPathTracerVolume volumes.\nAllows to load a 8-bit mask volume The mask volume can be used by any volume or instance It allows to: Change the alpha and color of inside/outside voxels Change the tag value (see SoPathTracerVolume) Very useful in combination with SoVolumeCutting SoPathTracerTagVolume can be used to tag voxels in SoPathTracerVolume volumes.\nAllows to load a 8-bit tag volume The tags are used to select a per-object LUT and/or material A 2D LUT can be provided using LUTConcat or SoLUTEditor2D Per-tag materials can be provided by adding multiple materials to the inMaterial scene Useful to render segmented objects SoPathTracerVolumeInstance can be used to render a SoPathTracerVolume with differnt transformation, subvolume, LUT, material, \u0026hellip;\nAllows to instantiate an existing volume Supports all options of a normal volume, but only references the volume itself This allows to: Use a different LUT Change shading options Change subvolume Change transformation matrix Different material selection SoPathTracerSlice renders a slice at the given plane, showing the volume data of the given volume.\nAllows to render a cut slice through a volume Allows to set an arbitrary plane and works on volumes and instances Has its own LUT and can be opaque or transparent SoPathTracerIsoSurface renders an iso surface (with first hit refinement) on the given base volume.\nAllows to render an isosurface of a volume Works on volumes and instances Supports opaque and transparent surfaces Isosurface is rendered on-the-fly Hit refinement is used to provide high-quality surfaces Arbitrary material can be specified Geometry SoPathTracerMesh scans the input scene for triangle meshes and ray traces them. Allows to render arbitrary triangle meshes Scans the input scene for triangle meshes and converts them to a bounding volume hierarchy (BVH) Supports different materials by adding SoPathTracerMaterials Objects can be turned on/off via material nodes (without BVH rebuilding) Supports opaque and transparent meshes SoPathTracerLines scans the input scene for line sets and ray traces them as cylinders with round caps. Allows to render thick lines (capsules) Scans the input scene for SoLineSet/SoIndexedLineSet and converts them to a BVH Different materials are supported by adding SoPathTracerMaterial nodes Supports opaque and transparent lines Useful to render fibers or stream lines SoPathTracerSpheres renders a marker list as ray traced spheres. Allows to render markers as spheres Converts marker list to spheres and creates a BVH Currently only supports single material Lights SoPathTracerAreaLight provides a realistic area light with attenuation, area, and distance. Provides an area light Multiple area lights are supported Lights can be placed: Around the scene bounding box using polar coordinates At absolute camera or world position (as head or local light) Light intensity is automatically adapted to the scene size Otherwise, it would be hard to select an intensity that works on different scales SoPathTracerBackgroundLight provides a background light, using image based lighting from a sphere or cube map. Provides environmental lighting It supports: Specifying environment light colors Image based lighting using a cube map or sphere map Only one background light can be added to a scene Material SoPathTracerMaterial allows to specify which material should be used for a given object. Provides material settings for other nodes Offers to select the used material and its parameters Is connected to the inMaterial of other nodes Multiple materials may be placed into the input scene of SoPathTracerMesh and SoPathTracerLines Allows to override volume shader settings as well The following examples shall help you to learn how to use the modules.\n","tags":["Advanced","Tutorial","Visualization","3D","Volume Rendering","Path Tracer"],"section":"tutorials"},{"date":"1677110400","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/pathtracer/pathtracerexample1/","title":"Example 6.1: Volume Rendering vs. Path Tracer","summary":"Example 6.1: Volume Rendering vs. Path Tracer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you develop a network to show some differences between volume rendering and the MeVisLab Path Tracer. You will visualize the same scene using both 3D rendering techniques and some of the modules for path tracing.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace. You will see a message if your hardware does not support CUDA:\n","content":"Example 6.1: Volume Rendering vs. Path Tracer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you develop a network to show some differences between volume rendering and the MeVisLab Path Tracer. You will visualize the same scene using both 3D rendering techniques and some of the modules for path tracing.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace. You will see a message if your hardware does not support CUDA:\nMeVisLab detected an Intel onboard graphics adapter. If you experience rendering problems, try setting the environment variables SOVIEW2D_NO_SHADERS and GVR_NO_GLSL. Handling cudaGetDeviceCount returned 35 (CUDA driver version is insufficient for CUDA runtime version)\nSteps to Do As a first step for comparison, you are creating a 3D scene with two spheres using the already known volume rendering.\nVolume Rendering Create 3D Objects Add three WEMInitialize modules for one Cube and two Icosphere to your workspace and connect each of them to a SoWEMRenderer. Set instanceName of the WEMInitialize modules to Cube, Sphere1, and Sphere2. Set instanceName of the SoWEMRenderer modules to and RenderCube, RenderSphere1, and RenderSphere2.\nFor RenderSphere1, define a Diffuse Color yellow and set Face Alpha to 0.5. The RenderCube remains as is and the RenderSphere2 is defined as Diffuse Color red and Face Alpha 0.5.\nGroup your modules and name the group Initialization. Your network should now look like this:\nExample Initialization Use the Output Inspector for your SoWEMRenderer outputs and inspect the 3D rendering. You should have a yellow and a red sphere, and a grey cube.\nSphere1 Sphere2 Cube Rendering Add 2 SoGroup modules and a SoBackground to your network. Connect the modules as seen below.\nExample Group If you now inspect the output of the SoGroup, you will see an orange sphere.\nMissing Translation You did not translate the locations of the three objects; they are all located at the same place in world coordinates. Open the WEMInitialize panels of your 3D objects and define the following translations and scalings:\nWEMInitializeSphere1 WEMInitializeSphere2 WEMInitializeCube The result of the SoGroup now shows two spheres on a rectangular cube.\nObjects Translated and Scaled For the viewer, you now add a SoCameraInteraction, a SoDepthPeelRenderer, and a SoRenderArea module to your network and connect them.\nNetwork with Viewer You now have a 3D volume rendering of our three objects.\nIn order to distinguish between the two viewers, you now add a label to the SoRenderArea describing that this is the Volume Rendering. Add a SoMenuItem, a SoBorderMenu, and a SoSeparator to your SoRenderArea.\nSoMenuItem Define the Label of the SoMenuItem as Volume Rendering and set Border Alignment to Top Right and Menu Direction to Horizontal for the SoBorderMenu.\nLabel in SoRenderArea Finally, you should group all modules belonging to your volume rendering.\nVolume Rendering Network Path Tracing For the Path Tracer, you can just reuse our 3D objects from volume rendering. This helps us to compare the rendering results.\nRendering Path Tracer modules fully integrate into MeVisLab Open Inventor; therefore, the general principles and the necessary modules are not completely different. Add a SoGroup module to your workspace and connect it to your 3D objects from SoWEMRenderer. A SoBackground as in volume rendering network is not necessary but you add a SoPathTracerMaterial and connect it to the SoGroup. You can leave all settings as default for now.\nPath Tracer Material Add a SoPathTracerAreaLight, a SoPathTracerMesh, and a SoPathTracer to a SoSeparator and connect the SoPathTracerMesh to your SoGroup. This adds your 3D objects to a Path Tracer Scene.\nPath Tracer Selecting the SoSeparator output already shows a preview of the same scene rendered via Path Tracing.\nPath Tracer Preview Add a SoCameraInteraction and a SoRenderArea to your network and connect them as seen below.\nSoCameraInteraction You can now use both SoRenderArea modules to visualize the differences side by side. You should also add the SoMenuItem, a SoBorderMenu, and a SoSeparator to your SoRenderArea in order to have a label for Path Tracing inside the viewer.\nDefine the Label of the SoMenuItem as Path Tracing and set Border Alignment to Top Right and Menu Direction to Horizontal for the SoBorderMenu.\nLabel in SoRenderArea Finally, group your Path Tracer modules to another group named Path Tracing.\nNew Group for Path Tracing Side by Side Share the Same Camera Finally, you want to have the same camera perspective in both viewers, so that you can see the differences. Add a SoPerspectiveCamera module to your workspace and connect it to the volume rendering and the Path Tracer network. The Path Tracer network additionally needs a SoGroup, see below for connection details. You have to toggle detectCamera in both of your SoCameraInteraction modules in order to synchronize the view for both SoRenderArea viewers.\nCamera Synchronization Additional Info:\u0026nbsp; Path Tracing requires a lot of iterations before reaching the best possible result. You can see the maximum number of iterations defined and the current iteration in the SoPathTracer panel. The more iterations, the better the result but the more time it takes to finalize your image. PathTracer_1_Iteration PathTracer_100_Iterations PathTracer_1000_Iterations Results Path Tracing provides a much more realistic way to visualize the behavior of light in a scene. It simulates the scattering and absorption of light within the volume.\nExercises Play around with different SoPathTracerMaterial settings and define different materials. Change the maximum number of iterations in SoPathTracer module. Change the configurations in SoPathTracerAreaLight module. Summary Path Tracer modules can be used the same way as Open Inventor modules. A SoPerspectiveCamera can be used for multiple viewers to synchronize camera position. Path Tracing produces beautiful, photorealistic renderings, but can be computationally expensive. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Volume Rendering","Path Tracer"],"section":"tutorials"},{"date":"1704153600","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/pathtracer/pathtracerexample2/","title":"Example 6.2: Visualization Using Path Tracer","summary":"Example 6.2: Visualization Using SoPathTracer Introduction In this tutorial, we will explain the basics of using the SoPathTracer module in MeVisLab. You will learn how to create a scene, assign materials, add light sources, and configure the PathTracer to generate enhanced renderings.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace. You will see a message if your hardware does not support CUDA:\n","content":"Example 6.2: Visualization Using SoPathTracer Introduction In this tutorial, we will explain the basics of using the SoPathTracer module in MeVisLab. You will learn how to create a scene, assign materials, add light sources, and configure the PathTracer to generate enhanced renderings.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace. You will see a message if your hardware does not support CUDA:\nMeVisLab detected an Intel onboard graphics adapter. If you experience rendering problems, try setting the environment variables SOVIEW2D_NO_SHADERS and GVR_NO_GLSL. Handling cudaGetDeviceCount returned 35 (CUDA driver version is insufficient for CUDA runtime version)\nSteps to Do Develop Your Network Download and open the images by using a LocalImage module. Connect it to a View2D to visually inspect its contents.\nMR Image of Knee in 2D Replace the View2D module by a SoExaminerViewer. Add the modules SoPathTracerVolume and SoPathTracer to your workspace and connect them as seen below.\nThe SoPathTracerVolume enables the loading and transforming the data into renderable volumes for Path Tracing. The SoPathTracer is the main rendering module of the MeVis Path Tracer framework. It provides a much more realistic way to visualize the behavior of light in a scene. It simulates the scattering and absorption of light within the volume.\nAdditional Info:\u0026nbsp; It\u0026rsquo;s essential to consistently position the SoPathTracer module on the right side of the scene. This strategic placement ensures that the module can render all objects located in the scene before it accurately. SoPathTracerVolume \u0026amp; SoPathTracer If you check your SoExaminerViewer, you will see a black box. We need to define a LUT for the gray values first.\nSoExaminerViewer Now, connect the SoLUTEditor module to your SoPathTracerVolume as illustrated down below and you will be able to see the knee.\nSoLUTEditor Add a MinMaxScan module to the LocalImage module and open the panel. The module shows the actual minimal and maximal gray values of the volume.\nOpen the panel of the SoLUTEditor module and define Range between 0 and 2047 as calculated by the MinMaxScan.\nMinMaxScan Next, add lights to your scene. Connect a SoPathTracerAreaLight and a SoPathTracerBackgroundLight module to your SoExaminerViewer to improve scene lighting.\nThe SoPathTracerAreaLight module provides a physically-based area light that illuminates the scene of a SoPathTracer. The lights can be rectangular or discs and have an area, color, and intensity. They can be positioned with spherical coordinates around the bounding box of the renderer, or they can be position in world or camera space.\nThe SoPathTracerBackgroundLight module provides a background light for the SoPathTracer. It supports setting a top, middle, and bottom color or alternatively, it support image-based lighting (IBL) using a sphere or cube map. Only one background light can be active for a given SoPathTracer.\nLights Open the panel of the SoPathTracerBackgroundLight module and choose your three colors.\nSoPathTracerBackgroundLight Manually Define LUT Either define your desired colors for your LUT in the Editor tab manually as shown down below.\nLUT Load Example LUT from File As an alternative, you can replace the SoLUTEditor with a LUTLoad and load this XML file to use a predefined LUT.\nLUTLoad Now, let\u0026rsquo;s enhance your rendering further by using the SoPathTracerMaterial module. This module provides essential material properties for geometry and volumes within the SoPathTracer scene.\nAdd a SoPathTracerMaterial module to your SoPathTracerVolume. Open its panel and navigate to the tab Surface Brdf. Change the Diffuse color for altering the visual appearance of surfaces. The Diffuse color determines how light interacts with the surface, influencing its overall color and brightness. Set Specular to 0.5, Shininess to 1.0, and Specular Intensity to 0.5.\nSoPathTracerMaterial Additional Info:\u0026nbsp; The resulting rendering in SoExaminerViewer might look different depending on your defined LUT. Visualize Bones If you want to visualize multiple volumes at the same time, you need to add another SoPathTracerVolume and LocalImage for loading the mask to the SoExaminerViewer.\nAs the LUT of the second volume also differs, add another SoLUTEditor module or LUTLoad module to the new SoPathTracerVolume.\nFor later usage, you can also already add a SoPathTracerMaterial module to the SoPathTracerVolume module.\nLoad the Bones mask by using the new LocalImage module and preview it in a View2D.\nBones mask Start by disabling the visibility of your first volume by toggeling SoPathTracerVolume Enabled field to off. This helps to improve the rendering of the bones itself and makes it easier to define colors for your LUT.\nLoad Example LUT from File Once again, you can decide to define the LUT yourself in SoLUTEditor module, or load a prepared XML File in a LUTLoad module as provided here.\nManually Define LUT If you want to define your own LUT, connect a MinMaxScan module to your LocalImage1 and define the Range for the SoLUTEditor as already done before.\nMinMaxScan of Bones mask Open the panel of SoLUTEditor1 for the bones and go to tab Range and set New Range Min to 0 and New Range Max to 127. Define the following colors in the tab Editor.\nSoLUTEditor1 You can increase the Shininess of the bones and change the Diffuse color in the Surface Brdf tab within the SoPathTracerMaterial1. Also set Specular to 0.5, Shininess to 0.904, and Specular Intensity to 0.466.\nSoPathTracerMaterial1 Visualize Vessels Repeat the process for the vessels. Add another LocalImage, SoPathTracerVolume, SoLUTEditor (or LUTLoad), and View2D module as seen below. Load this Vessels mask and check it using View2D.\nVessels mask Load Example LUT from File Load a prepared XML File in a LUTLoad module as provided here\nManually Define LUT Connect the MinMaxScan to your LocalImage2.\nAccess the SoLUTEditor2 panel in the tab Range and set the New Range Min to 0 and the New Range Max to 255. Additionally, modify the illustrated color settings within the Editor tab.\nMinMaxScan of Vessels mask SoLUTEditor2 Now you should set your first volume visible again by toggling SoPathTracerVolume Enabled field to on.\nFinal Result Additional Info:\u0026nbsp; The resulting rendering in SoExaminerViewer might look different depending on your defined LUTs. Final Result with Enhanced Visualization Summary: You can generate photorealistic renderings using SoPathTracer and associated modules. Render volumes efficiently in SoPathTracer scenes with SoPathTracerVolume, enabling diverse rendering options, LUT adjustments, lights and material enhancements. Enhance your scene\u0026rsquo;s look by adjusting materials and colors interactively using SoPathTracerMaterial and SoLUTEditor. Use lighting modules such as SoPathTracerAreaLight and SoPathTracerBackgroundLight to optimize the illumination of your rendered scenes. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Path Tracer"],"section":"tutorials"},{"date":"1700524800","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample7/","title":"Example 7: Add 3D Viewer to OrthoView2D","summary":"Example 7: Add 3D Viewer to OrthoView2D \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will use the OrthoView2D module and add a 3D viewer to the layout Cube.\nSteps to Do Develop Your Network Add the modules LocalImage and OrthoView2D to your workspace and connect them.\nNetwork The OrthoView2D module allows you to select multiple layouts. Select layout Cube Equal. The layout shows your image in three orthogonal viewing directions. The top left segment remains empty.\n","content":"Example 7: Add 3D Viewer to OrthoView2D \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will use the OrthoView2D module and add a 3D viewer to the layout Cube.\nSteps to Do Develop Your Network Add the modules LocalImage and OrthoView2D to your workspace and connect them.\nNetwork The OrthoView2D module allows you to select multiple layouts. Select layout Cube Equal. The layout shows your image in three orthogonal viewing directions. The top left segment remains empty.\nOrthoView2D Layouts We now want to use a 3D rendering in the top left segment whenever the layout Cube Equal is chosen. Add a View3D and a SoViewportRegion module to your workspace. Connect the LocalImage with your View3D. The image is rendered in 3D. Hit SPACE on your keyboard to make the hidden output of the View3D module visible. Connect it with your SoViewportRegion and connect the SoViewportRegion with the inInvPreLUT input of the OrthoView2D.\nNetwork with SoViewportRegion Open the OrthoView2D and inspect your layout.\nOrthoView2D with 3D You can see your View3D being visible in the bottom right segment of the layout behind the coronal view of the image. Open the panel of the SoViewportRegion module. In section X-Position and Width, set Left Border to 0 and Right Border to 0.5. In section Y-Position and Height, set Lower Border to 0 and Upper Border to 0.5. Also check Render delayed paths.\nDefine viewport region The View3D image is now rendered to the top left segment of the OrthoView2D, because the module SoViewportRegion renders a subgraph into a specified viewport region (VPR). The problem is: We cannot rotate and pan the 3D object, because there is no camera interaction available after adding the SoViewportRegion. The camera interaction is consumed by the View3D module before it can be used by the viewport.\nAdd a SoCameraInteraction module between the View3D and the SoViewportRegion. You can now interact with your 3D scene but the rotation is not executed on the center of the object. Trigger ViewAll on your SoCameraInteraction module.\nSoCameraInteraction You have now successfully added the View3D to the OrthoView2D, but there is still a problem remaining: If you change the layout to something different than LAYOUT_CUBE_EQUAL, the 3D content remains visible.\nWe can use a StringUtils module to resolve that. Set Operation to Compare and draw a parameter connection from the field OrthoView2D.layout to the field StringUtils.string1. The currently selected layout is displayed as String A. Enter LAYOUT_CUBE_EQUAL as String B. Now, draw a parameter connection from the field StringUtils.boolResult to the field SoViewportRegion.on.\nStringUtils If the selected layout in OrthoView2D now matches the string LAYOUT_CUBE_EQUAL (the field boolResult of the StringUtils module is TRUE), the SoViewportRegion is turned on. In any other case, the 3D segment is not visible.\nFinal Network Summary The module SoViewportRegion renders a subgraph into a specified viewport region (VPR). \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","3D","OrthoView2D"],"section":"tutorials"},{"date":"1701993600","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample8/","title":"Example 8: Vessel Segmentation Using SoVascularSystem","summary":"Example 8: Vessel Segmentation using SoVascularSystem \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using an input mask to create a vessel centerline using the DtfSkeletonization module and visualize the vascular structures in 3D using the SoVascularSystem module. The second part uses the distance between centerline and surface of the vessel structures to color thin vessels red and thick vessels green.\nSteps to Do Develop Your Network Load the example tree mask by using the LocalImage module. Connect the output to a DtfSkeletonization module as seen below. The initial output of the DtfSkeletonization module is empty. Press the Update button to calculate the skeleton and the erosion distances.\n","content":"Example 8: Vessel Segmentation using SoVascularSystem \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using an input mask to create a vessel centerline using the DtfSkeletonization module and visualize the vascular structures in 3D using the SoVascularSystem module. The second part uses the distance between centerline and surface of the vessel structures to color thin vessels red and thick vessels green.\nSteps to Do Develop Your Network Load the example tree mask by using the LocalImage module. Connect the output to a DtfSkeletonization module as seen below. The initial output of the DtfSkeletonization module is empty. Press the Update button to calculate the skeleton and the erosion distances.\nNetwork Below you can see the output of the original image taken from the LocalImage module (left) compared to the output after calculating the skeleton via DtfSkeletonization module (right).\nOutput comparison The output DtfSkeletonization.outBase1 shows nothing. Here you can find the three-dimensional graph of the vascular structures. To generate it, open the panel of the DtfSkeletonization module, set Update Mode to Auto Update, and select Update skeleton graph. Now the output additionally provides a 3D graph. Additionally, enable the Compile Graph Voxels to provide all object voxels at the output.\nDtfSkeletonization You can use the Output Inspector to see the 3D graph.\nGraph output of DtfSkeletonization If you want to visualize your graph, you should connect a GraphToVolume module to the DtfSkeletonization module. The result is a 2D or 3D volume of your graph that you can connect to any 2D or 3D viewer. Add a View2D and a View3D module to the GraphToVolume module and update the volume.\nGraphToVolume For coloring the vessels depending on their distances to the centerline, we need a SoLUTEditor module. Change your network to use a SoExaminerViewer module, a SoLUTEditor module and a SoBackground module instead of a View3D module.\nUse the SoLUTEditor for the View2D, too.\nNetwork Open the output of the GraphToVolume module and inspect the images in Output Inspector. You will see that the voxel value of the black background is defined as -1 while the vessel tree is defined as 0.\nOutput Inspector Open the Panel of the SoLUTEditor and select tab Range. Define New Range Min as -1 and New Range Max as 0.\nSoLUTEditor Range Change to Editor tab and define the following LUT:\nSoLUTEditor Editor The viewers now show your vessel graph.\nView2D and SoExaminerViewer Store Edge IDs in Skeletons With RunPythonScript Each edge of the calculated skeleton gets a unique ID defined by the DtfSkeletonization module. We now want to use this ID to define a different color for each edge of the skeleton. You can use the Label property of each skeleton to store the ID of the edge.\nAdd a RunPythonScript module to your network, open the panel of the module and enter the following Python code:\nctx.field(\u0026#34;DtfSkeletonization.update\u0026#34;).touch() graph = ctx.field(\u0026#34;DtfSkeletonization.outBase1\u0026#34;).object() if graph is not None: for edge in graph.getEdges(): print(edge.getId()) ctx.field(\u0026#34;GraphToVolume.update\u0026#34;).touch() First, we always want a fresh skeleton. We touch the update trigger of the module DtfSkeletonization. Then, we get the graph from the DtfSkeletonization.outBase1 output. If a valid graph is available, we walk through all edges of the graph and print the ID of each edge. In the end, we update the GraphToVolume module to get the calculated values of the Python script in the viewers. Click Execute.\nThe Debug Output of the MeVisLab IDE shows a numbered list of edge IDs from 1 to 153.\nRunPythonScript We now want the edge ID to be used for coloring each of the skeletons differently. Open the Panel of the SoLUTEditor and select tab Range. Define New Range Min as 0 and New Range Max as 153. Define different colors for your LUT.\nSoLUTEditor The SoGVRVolumeRenderer module also needs a different setting. Open its panel in the Main tab, select Illuminated as the Render Mode. Adjust the Quality setting to 0.10. On tab Advanced, set Filter Volume Data to Nearest. Change to the Illumination tab and define below parameters:\nSoGVRVolumeRendererMain SoGVRVolumeRendererIllumination Change your Python script as follows: ctx.field(\u0026#34;DtfSkeletonization.update\u0026#34;).touch() graph = ctx.field(\u0026#34;DtfSkeletonization.outBase1\u0026#34;).object() if graph is not None: label = \u0026#34;Label\u0026#34; for edge in graph.getEdges(): for skeleton in edge.getSkeletons(): if label not in skeleton.properties: skeleton.createPropertyDouble(label, edge.getId()) skeleton.setProperty(label, edge.getId()) ctx.field(\u0026#34;GraphToVolume.update\u0026#34;).touch() In the case the graph is valid, we now define a static text for the label. Instead of printing the edge ID, we also walk through each skeleton of the edge and define the property for the label using the ID of the edge as value.\nYour viewers now show a different color for each skeleton, based on our LUT.\nView2D and SoExaminerViewer Render the Vascular System Using SoVascularSystem The SoVascularSystem module is optimized for rendering vascular structures. In comparison to the SoGVRVolumeRenderer module, it allows to render the surface, the skeleton or points of the structure in an open inventor scene graph. Interactions with edges of the graph are also already implemented.\nAdd a SoVascularSystem module to your workspace. Connect it to your DtfSkeletonization module and to the SoLUTEditor as seen below. Add another SoExaminerViewer for comparing the two visualization. The same SoBackground can be added to your new scene.\nUncheck Use skeleton colors and Use integer LUT on Appearance tab of the SoVascularSystem module panel.\nEditedNetwork Extra Infos:\u0026nbsp; More information about the SoVascularSystem module can be found in the help page\nof the module.\nDraw parameter connections from one SoExaminerViewer to the other. Use the fields seen below to synchronize your camera interaction.\nCamera positions Connect the backwards direction of the two SoExaminerViewer by using multiple SyncFloat modules and two SyncVector modules for position and orientation fields.\nExtra Infos:\u0026nbsp; To establish connections between fields with the type Float, you can use the SyncFloat module. For fields containing vector, the appropriate connection can be achieved using the SyncVector module. SyncFloat \u0026amp; SyncVector Camera interactions are now synchronized between both SoExaminerViewer modules.\nNow, you can notice the difference between the two modules. We use SoVascularSystem for a smoother visualization of the vascular structures by using the graph as reference. The SoGVRVolumeRenderer renders the volume from the GraphToVolume module, including the visible stairs from voxel representations in the volume.\nSoVascularSystem \u0026amp; SoGVRVolumeRenderer The SoVascularSystem module has additional visualization examples unlike SoGVRVolumeRenderer. Open the panel of the SoVascularSystem module and select Random Points for Display Mode in the Main tab to see the difference.\nRandom Points Change it to Skeleton to only show the centerlines/skeletons of the vessels.\nSkeleton Warning:\u0026nbsp; For volume calculations, use the original image mask instead of the result from GraphToVolume. Enhance Vessel Visualization Based on Distance Information Now that you\u0026rsquo;ve successfully obtained the vessel skeleton graph using DtfSkeletonization, let\u0026rsquo;s take the next step to enhance the vessel visualization based on the radius information of the vessels. We will modify the existing code to use the minimum distance between centerline and surface of the vessels for defining the color.\nThe values for the provided vascular tree vary between 0mm and 10mm. Therefore, define the range of the SoLUTEditor to New Range Min as 1 and New Range Max as 10. On Editor tab, define the following LUT:\nSoLUTEditor In the RunPythonScript module, change the existing code to the following: ctx.field(\u0026#34;DtfSkeletonization.update\u0026#34;).touch() graph = ctx.field(\u0026#34;DtfSkeletonization.outBase1\u0026#34;).object() if graph is not None: label = \u0026#34;Label\u0026#34; print(\u0026#39;Num edges\u0026#39;, len(graph.getEdges())) for edge in graph.getEdges(): end_node = edge.getEndNode() for skeleton in edge.getSkeletons(): if label not in skeleton.properties: skeleton.createPropertyDouble(label, skeleton.getProperty(\u0026#34;MinDistance\u0026#34;)) skeleton.setProperty(label, skeleton.getProperty(\u0026#34;MinDistance\u0026#34;)) ctx.field(\u0026#34;GraphToVolume.update\u0026#34;).touch() ctx.field(\u0026#34;SoVascularSystem.apply\u0026#34;).touch() Warning:\u0026nbsp; Be aware that the MinDistance and MaxDistance values are algorithm-specific and don\u0026rsquo;t precisely represent vessel diameters. The result of DTFSkeletonization is a vascular graph with an idealized, circular profile while in reality, the vessels have more complicated profiles. It is an idealized graph where all vessels have a circular cross section. This cross section only has one radius, described by MinDistance and MaxDistance. Those are not the two radii of an elliptical cross section, but the results of two different algorithms to measure the one, idealized radius at Skeletons. Instead of using the ID of each edge for the label property, we are now using the MinDistance property of the skeleton. The result is a color-coded 3D visualization depending on the radius of the vessels. Small vessels are red, large vessels are green.\nRadius based Visualization Additional Info:\u0026nbsp; If you have a NIfTI file, convert it into an ML image. Load your tree mask NIfTI file using the itkImageFileReader module. Connect the output to a BoundingBox module, which removes black pixels and creates a volume without unmasked parts. In the end, add a MLImageFormatSave module to save it as .mlimage file. They are much smaller than a NIfTI file.\nNIFTI file conversion Mouse Clicks on Vessel Graph Open the Interaction tab of the SoVascularSystem module. In SoExaminerViewer module, change to Pick Mode and click into your vessel structure. The panel of the SoVascularSystem module shows all information about the hit of your click in the vessel tree.\nGetting the click point in a vascular tree Summary Vessel centerlines can be created using a DtfSkeletonization module. Vascular structures can be visualized using a SoVascularSystem module, which provides several vessel-specific display modes. The SoVascularSystem module provides information about mouse clicks into a vascular tree. The labels of a skeleton can be used to store additional information for visualization. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Vessel Segmentation"],"section":"tutorials"},{"date":"1704672000","url":"https://mevislab.github.io/examples/pull/133/tutorials/visualization/visualizationexample9/","title":"Example 9: Creating Dynamic 3D Animations Using AnimationRecorder","summary":"Example 9: Creating Dynamic 3D Animations using AnimationRecorder \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using the AnimationRecorder module to generate dynamic and visually appealing animations of our 3D scenes. We will be recording a video of the results of our previous project, particularly the detailed visualizations of the muscles, bones, and blood vessels created using PathTracer.\nSteps to Do Open the network and files of Example 6.2, add a SoSeparator module and an AnimationRecorder module to your workspace and connect them as shown below.\n","content":"Example 9: Creating Dynamic 3D Animations using AnimationRecorder \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using the AnimationRecorder module to generate dynamic and visually appealing animations of our 3D scenes. We will be recording a video of the results of our previous project, particularly the detailed visualizations of the muscles, bones, and blood vessels created using PathTracer.\nSteps to Do Open the network and files of Example 6.2, add a SoSeparator module and an AnimationRecorder module to your workspace and connect them as shown below.\nThe SoSeparator module collects all components of our scene and provides one output to be used for the AnimationRecorder.\nThe AnimationRecorder module allows to create animations and record them as video streams. It provides an editor to create keyframes for animating field values.\nAnimationRecorder Define the following LUTs in SoLUTEditor of the knee or load this XML file with LUTLoad1 to use a predefined LUT.\nSoLUTEditor Open the AnimationRecorder module and click on New to initiate a new animation, selecting a filename for the recorded keyframes (.mlmov).\nAt the bottom of the AnimationRecorder panel, you\u0026rsquo;ll find the keyframe editor, which is initially enabled. It contains the camera track with a keyframe at position 0. The keyframe editor at the bottom serves as a control hub for playback and recording.\nExtra Infos:\u0026nbsp; Close the SoExaminerViewer while using the AnimationRecorder to prevent duplicate renderings and to save resources. AnimationRecorder Keyframes in the AnimationRecorder mark specific field values at defined timepoints. You can add keyframes on the timeline by double-clicking at the chosen timepoint or right-clicking and selecting Insert Key Frame. Between these keyframes, values of the field are interpolated (linear or spline) or not. Selecting a keyframe, a dialog Edit Camera Key Frame will open.\nWhen adding a keyframe at a specific timepoint, you can change the camera dynamically in the viewer. This involves actions such as rotating to left or right, zooming in and out, and changing the camera\u0026rsquo;s location. Within the Edit Camera Key Frame dialog save each keyframe by clicking on the Store Current Camera State button. Preview the video to observe the camera\u0026rsquo;s movement.\nThe video settings in the AnimationRecorder provide essential parameters for configuring the resulting animation. You can control the Framerate, determining the number of frames per second in the video stream. It\u0026rsquo;s important to note that altering the framerate may lead to the removal of keyframes, impacting the animation\u0026rsquo;s smoothness.\nAdditionally, the Duration of the animation, specified as videoLength, defines how long the animation lasts in seconds. The Video Size determines the resolution of the resulting video.\nRepeat this process for each timepoint where adjustments to the camera position are needed, thus creating a sequence of keyframes.\nBefore proceeding further, use the playback options situated at the base of the keyframe editor. This allows for a quick preview of the initial camera sequence, ensuring the adjustments align seamlessly for a polished transition between keyframes.\nExtra Infos:\u0026nbsp; Decrease the number of iterations in the SoPathTracer module for a quicker preview if you like. Make sure to increase again before recording the final video. AnimationRecorder Modulating Knee Visibility with LUTRescale in Animation We want to show and hide the single segmentations during camera movements. Add two LUTRescale modules to your workspace and connect them as illustrated down below. The rationale behind using LUTRescale is to control the transparency and by that the visibility of elements in the scene at different timepoints.\nLUTRescale Animate Bones and Vessels Now, let\u0026rsquo;s shift our focus to highlighting bones and vessels within the animation. Right-click on the LUTRescale module, navigate to Show Window, and select Automatic Panel. This will bring up the control window for the LUTRescale module. Search for the field named targetMax. You can either drag and drop it directly from the Automatic Panel, or alternatively, locate the Max field in the Output Index Range box within the module panel and then drag and drop it onto the fields section in the AnimationRecorder module, specifically under the Perspective Camera field.\nBy linking the targetMax field of the LUTRescale module to the AnimationRecorder, you establish a connection that allows you to define different values of the field for specific timepoints. The values between these timepoints can be interpolated as described above.\nLUTRescale \u0026amp; AnimationRecorder To initiate the animation sequence, start by adding a keyframe at position 0 for the targetMax field. Set the Target Max value in the Edit Key Frame – [LUTRescale.targetMax] window to 1, and click on the Store Current Field Value button to save it.\nNext, proceed to add keyframes at the same timepoints as the desired keyframes of the Perspective Camera field\u0026rsquo;s first sequence. For each selected keyframe, progressively set values for the Target Max field, gradually increasing to 10. This ensures specific synchronization between the visibility adjustments controlled by the LUTRescale module and the camera movements in the animation, creating a seamless transition. This gradual shift visually reveals the bones and vessels while concealing the knee structures and muscles.\nTo seamlessly incorporate the new keyframe at the same timepoints as the Perspective Camera field, you have two efficient options. Simply click on the keyframe of the first sequence, and the line will automatically appear in the middle of the keyframe. A double-click will effortlessly insert a keyframe at precisely the same position. If you prefer more accurate adjustments, you can also set your frame manually using the Edit Key Frame - [LUTRescale.targetMax] window. This flexibility allows for precise control over the animation timeline, ensuring keyframes align precisely with your intended moments.\nLUTRescale \u0026amp; AnimationRecorder Showcasing Only Bones To control the visibility of the vessels, right-click on the LUTRescale1 module connected to the vessels. Open the Show Window and select Automatic Panel. Drag and drop the targetMax field into the AnimationRecorder module\u0026rsquo;s fields section.\nLUTRescale1 \u0026amp; AnimationRecorder Add keyframes for both the Perspective Camera field and the targetMax in LUTRescale1 at the same timepoints. Access the Edit Camera Key Frame window for the added keyframe in the Perspective Camera field and save the current camera state. To exclusively highlight only bones, adjust the Target Max values from 1 to 10000 in Edit Key Frame - [LUTRescale1.targetMax].\nLUTRescale1 \u0026amp; AnimationRecorder To feature everything again at the end, copy the initial keyframe of each field and paste it at the end of the timeline. This ensures a comprehensive display of all elements in the closing frames of your animation.\nFinal Animation Sequence Key Frames Finally, use the playback and recording buttons at the bottom of the keyframe editor to preview and record your animation.\nSummary Animations are created by strategically placing keyframes at different timepoints in the timeline using the AnimationRecorder module. It is possible to add any field of your network to your animation via drag-and-drop. The visibility of elements can be controlled using the LUTRescale module. Video settings in the AnimationRecorder can be adjusted to specify resolution, framerate, and duration of the resulting animation. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Animation Recorder","Movies"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/image_processing/","title":"Chapter IV: Image Processing","summary":"Image Processing in MeVisLab Digital image processing is the use of a digital computer to process digital images through an algorithm (see Wikipedia).\nMeVisLab provides multiple modules for image processing tasks, such as:\nFilters Masks Transformations Arithmetics Statistics For details about Image Processing in MeVisLab, see the MeVisLab Documentation In this chapter, you will find some examples for different types of image processing in MeVisLab.\n","content":"Image Processing in MeVisLab Digital image processing is the use of a digital computer to process digital images through an algorithm (see Wikipedia).\nMeVisLab provides multiple modules for image processing tasks, such as:\nFilters Masks Transformations Arithmetics Statistics For details about Image Processing in MeVisLab, see the MeVisLab Documentation In this chapter, you will find some examples for different types of image processing in MeVisLab.\n","tags":["Beginner","Tutorial","Image Processing"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/image_processing/image_processing1/","title":"Example 1: Applying Scalar Functions to Two Images","summary":"Example 1: Arithmetic Operations on Two Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We are using the Arithmetic2 module to apply basic scalar functions on two images. The module provides two inputs for images and one output image for the result.\nSteps to Do Develop Your Network Add two LocalImage modules to your workspace for the input images. Select $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm and $(DemoDataPath)/BrainMultiModal/ProbandT2.dcm from MeVisLab demo data and add a SynchroView2D to your network.\n","content":"Example 1: Arithmetic Operations on Two Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We are using the Arithmetic2 module to apply basic scalar functions on two images. The module provides two inputs for images and one output image for the result.\nSteps to Do Develop Your Network Add two LocalImage modules to your workspace for the input images. Select $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm and $(DemoDataPath)/BrainMultiModal/ProbandT2.dcm from MeVisLab demo data and add a SynchroView2D to your network.\nIn the end, add the Arithmetic2 module and connect them as seen below.\nExample Network Your SynchroView2D shows two images. On the left hand side, you can see the original image from your left LocalImage module. The right image shows the result of the arithmetic operation performed by the Arithmetic2 module on the two input images.\nSynchroView2D The SynchroView2D module automatically synchronizes the visible slice of both input images; you can see the same slice with and without applied filter.\nArithmetic Operations Double-click the Arithmetic2 module to select different functions to be applied.\nArithmetic2 The selected function is applied automatically.\nSummary Arithmetic operations on two images can be applied on images by using Arithmetic* modules. The SynchroView2D module allows to scroll through slices synchronized on two images. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Image Processing","Arithmetic"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/image_processing/image_processing2/","title":"Example 2: Masking Images","summary":"Example 2: Masking Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The background of medical images is black for most cases. In the case an image is inverted or window/level values are adapted, these black voxels outside clinical relevant voxels might become very bright or even white.\nBeing in a dark room using a large screen, the user might be blended by these large white regions.\n","content":"Example 2: Masking Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The background of medical images is black for most cases. In the case an image is inverted or window/level values are adapted, these black voxels outside clinical relevant voxels might become very bright or even white.\nBeing in a dark room using a large screen, the user might be blended by these large white regions.\nImage masking is a very good way to select a defined region where image processing shall be applied. A mask allows to define a region (the masked region) to allow image modifications whereas voxels outside the mask remain unchanged.\nSteps to Do Develop Your Network Add a LocalImage and a SynchroView2D module to your network and connect the modules as seen below.\nExample Network Open the Automatic Panel of the SynchroView2D module via context menu and selecting [ Show Window \u0026rarr; Automatic Panel ]. Set the field synchLUTs to TRUE.\nSynchronize LUTs in SynchroView2D Double-click the SynchroView2D and change window/level values via right mouse button . You can see that the background of your images gets very bright and changes based on the LUT are applied to all voxels of your input image - even on the background. Hovering your mouse over the image(s) shows the current gray value under your cursor in Hounsfield Unit (HU).\nWithout masking the image Hovering the mouse over black background voxels shows a value between 0 and about 60. This means we want to create a mask that only allows modifications on voxels having a gray value larger than 60.\nAdd a Mask and a Threshold module to your workspace and connect them as seen below.\nExample Network Changing the window/level values in your viewer still also changes background voxels. The Threshold module still leaves the voxels as is because the threshold value is configured as larger than 0. Open the panels of the modules Threshold and Mask via double-click and set the values as seen below.\nThreshold Mask Now, all voxels having a value lower or equal 60 are set to 0, all others are set to 1. The resulting image from the Threshold module is a binary image that can now be used as a mask by the Mask module.\nOutput of the Threshold module The Mask module is configured to use the Masked Original image. Changing the window/level values in your images now, you can see that the background voxels are not affected anymore (at least as long as you do not reach a very large value).\nAfter masking the image Summary The module Threshold applies a relative or an absolute threshold to a voxel image. It can be defined what should be written to those voxels that pass or fail the adjustable comparison. The module Mask masks the image of input one with the mask at input two. A mask can be used to filter voxels inside images. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Image Processing","Mask"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/image_processing/image_processing3/","title":"Example 3: Region Growing","summary":"Example 3: Region Growing \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction A very simple approach to segment parts of an image is the region growing method. A general explanation can be found here.\nIn this example, you will segment the brain of an image and show the segmentation results as an overlay on the original image.\nSteps to Do Develop Your Network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm. Add a View2D module and connect both as seen below.\n","content":"Example 3: Region Growing \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction A very simple approach to segment parts of an image is the region growing method. A general explanation can be found here.\nIn this example, you will segment the brain of an image and show the segmentation results as an overlay on the original image.\nSteps to Do Develop Your Network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm. Add a View2D module and connect both as seen below.\nExample Network Add the RegionGrowing Module Add the RegionGrowing module and connect the input with the LocalImage module. You will see a message results invalid. The reason is that a region growing always needs a starting point for getting similar voxels. The output of the module does not show a result in Output Inspector.\nResults Invalid Add a SoView2DMarkerEditor to your network and connect it with your RegionGrowing and with the View2D. Clicking into your viewer now creates markers that can be used for the region growing.\nSoView2DMarkerEditor The region growing starts on manually clicking Update or automatically if Update Mode is set to Auto-Update. We recommend to set update mode to automatic update. Additionally, you should set the Neighborhood Relation to 3D-6-Neighborhood (x,y,z), because then your segmentation will also be performed in the z-direction.\nSet Threshold Computation to Automatic and define Interval Size as 1.600 % for relative, automatic threshold generation.\nExtra Infos:\u0026nbsp; For more information, see MeVisLab Module Reference Auto-Update for RegionGrowing Clicking into your image in the View2D now already generates a mask containing your segmentation. As you did not connect the output of the RegionGrowing, you need to select the output of the module and use the Output Inspector to visualize your results.\nOutput Inspector Preview In order to visualize your segmentation mask as an overlay in the View2D, you need to add the SoView2DOverlay module. Connect it as seen below.\nSoView2DOverlay Your segmentation is now shown in the View2D. You can change the color and transparency of the overlay via SoView2DOverlay.\nClose Gaps Scrolling through the slices, you will see that your segmentation is not closed. There are lots of gaps where the gray value of your image differs more than your threshold. You can simply add a CloseGap module to resolve this issue. Configure Filter Mode as Binary Dilatation, Border Handling as Pad Src Fill and set KernelZ to 3.\nThe difference before and after closing the gaps can be seen in the Output Inspector.\nOutput_Before Output_After You can play around with the different settings of the RegionGrowing and CloseGap modules to get a better result.\nVisualize 2D and 3D You can now also add a View3D to show your segmentation in 3D. Your final result should look similar to this.\nFinal Result Summary The module RegionGrowing allows a very simple segmentation of similar gray values. Gaps in a segmentation mask can be closed by using the CloseGap module. Segmentation results can be visualized in 2D and 3D. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Image Processing","Segmentation","Region Growing"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/image_processing/image_processing4/","title":"Example 4: Subtracting 3D Surface Objects","summary":"Example 4: Subtracting 3D Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we load an image and render it as WEMIsoSurface. Then, we create a three-dimensional SoSphere and subtract the sphere from the initial WEM.\nSteps to Do Develop Your Network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm. Add a WEMIsoSurface, a SoWEMRenderer, a SoBackground, and a SoExaminerViewer module and connect them as seen below. Make sure to configure the WEMIsoSurface to use a Iso Min. Value of 420 and a Voxel Sampling 1.\n","content":"Example 4: Subtracting 3D Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we load an image and render it as WEMIsoSurface. Then, we create a three-dimensional SoSphere and subtract the sphere from the initial WEM.\nSteps to Do Develop Your Network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm. Add a WEMIsoSurface, a SoWEMRenderer, a SoBackground, and a SoExaminerViewer module and connect them as seen below. Make sure to configure the WEMIsoSurface to use a Iso Min. Value of 420 and a Voxel Sampling 1.\nExample Network The SoExaminerViewer now shows the head as a three-dimensional rendering.\nSoExaminerViewer Add a 3D Sphere to Your Scene We now want to add a three-dimensional sphere to our scene. Add a SoMaterial and a SoSphere to your network, connect them to a SoSeparator and then to the SoExaminerViewer. Set your material to use a Diffuse Color red and adapt the size of the sphere to Radius 50.\nExample Network The SoExaminerViewer now shows the head and the red sphere inside.\nSoExaminerViewer Set Location of Your Sphere In order to define the best possible location of the sphere, we additionally add a SoTranslation module and connect it to the SoSeparator between the material and the sphere. Define a translation of x=0, y=20 and z=80.\nExample Network Subtract the Sphere From the Head We now want to subtract the sphere from the head to get a hole. Add another SoWEMRenderer, a WEMLevelSetBoolean, and a SoWEMConvertInventor to the network and connect them to a SoSwitch as seen below. The SoSwitch also needs to be connected to the SoWEMRenderer of the head. Set your WEMLevelSetBoolean to use the Mode Difference.\nExample Network What happens in your network now?\nThe SoSphere is converted to a WEM. The WEMs from the head and from the sphere are subtracted by using a WEMLevelSetBoolean. The result of the subtraction is used for a SoWEMRenderer Both SoWEMRenderer (the head on the left side and the subtraction on the right side) are inputs for a SoSwitch. The SoSwitch toggles through its inputs and you can show the original WEM of the head or the subtraction. SoExaminerViewer_1 SoExaminerViewer_2 You can now toggle the hole to be shown or not, depending on your setting for the SoSwitch.\nSummary The module WEMLevelSetBoolean allows to subtract or add three-dimensional WEM objects. The SoSwitch can toggle multiple Open Inventor scenes as input. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Image Processing","3D","Subtraction"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/image_processing/image_processing5/","title":"Example 5: Clip Planes","summary":"Example 5: Clip Planes \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the SoGVRDrawOnPlane module to define the currently visible slice from a 2D view as a clip plane in 3D.\nSteps to Do Develop Your Network First, we need to develop the network to scroll through the slices. Add a LocalImage module to your workspace and select the file ProbandT1 from MeVisLab demo data.\n","content":"Example 5: Clip Planes \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the SoGVRDrawOnPlane module to define the currently visible slice from a 2D view as a clip plane in 3D.\nSteps to Do Develop Your Network First, we need to develop the network to scroll through the slices. Add a LocalImage module to your workspace and select the file ProbandT1 from MeVisLab demo data.\nAdd the modules OrthoReformat3, Switch, SoView2D, View2DExtensions, and SoRenderArea and connect them as seen below.\nExample Network In previous tutorials, we already learned that it is possible to show 2D slices in a SoRenderArea. For scrolling through the slices, a View3DExtensions module is necessary. In this network, we also have a OrthoReformat3 module. It allows us to transform the input image (by rotating and/or flipping) into the three main views commonly used:\nAxial Coronal Sagittal The Switch module takes multiple input images and you can toggle between them to show one of the orthogonal transformations to be used as output.\nThe SoRenderArea now shows the 2D images in a view defined by the Switch.\nView0 View1 View2 Current 2D Slice in 3D We now want to visualize the slice visible in the 2D images as a 3D plane. Add a SoGVRDrawOnPlane and a SoExaminerViewer to your workspace and connect them. We should also add a SoBackground and a SoLUTEditor. The viewer remains empty because no source image is selected to display. Add a SoGVRVolumeRenderer and connect it to your viewer and the LocalImage.\nExample Network A three-dimensional plane of the image is shown. Adapt the LUT as seen below.\nSoLUTEditor We now have a single slice of the image in 3D, but the slice is static and cannot be changed. In order to use the currently visible slice from the 2D viewer, we need to create a parameter connection from the SoView2D position Slice as plane to the SoGVRDrawOnPlane plane vector.\nSoView2D Position SoGVRDrawOnPlane Plane Now, the plane representation of the visible slice is synchronized to the plane of the 3D view. Scrolling through your 2D slices changes the plane in 3D.\nVisible slice in 3D Current 2D Slice as Clip Plane in 3D This slice shall now be used as a clip plane in 3D. In order to achieve this, you need another SoExaminerViewer and a SoClipPlane. Add them to your workspace and connect them as seen below. You can also use the same SoLUTEditor and SoBackground for the 3D view. Also use the same SoGVRVolumeRenderer; the 3D volume does not change.\nExample Network Now, your 3D scene shows a three-dimensional volume cut by a plane in the middle. Once again, the clipping is not the same slice as your 2D view shows.\nClip plane in 3D Again, create a parameter connection from the SoView2D position Slice as plane, but this time to the SoClipPlane.\nSoClipPlane Plane If you now open all three viewers and scroll through the slices in 2D, the 3D viewers are both synchronized with the current slice. You can even toggle the view in the Switch and the plane is adapted automatically.\nFinal 3 views Summary The module OrthoReformat3 transforms input images to the three viewing directions: coronal, axial, and sagittal. A Switch can be used to toggle through multiple input images. The SoGVRDrawOnPlane module renders a single slice as a three-dimensional plane. Three-dimensional clip planes on volumes can be created by using a SoClipPlane module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Image Processing","3D","Clip Planes"],"section":"tutorials"},{"date":"1746403200","url":"https://mevislab.github.io/examples/pull/133/tutorials/image_processing/image_processing6/","title":"Example 6: DICOM RT Visualization in MeVisLab – RTSTRUCT and RTDOSE Workflow","summary":"Example 6: DICOM RT Visualization in MeVisLab – RTSTRUCT and RTDOSE Workflow Introduction This tutorial explains how to load and visualize DICOM RT (Radiotherapy) data in MeVisLab. You will learn how to:\nLoad CT and related RTSTRUCT data. Visualize RTSTRUCTs as colored CSOs. Show labels next to each RTSTRUCT contour. Visualize RTDOSE as a semitransparent colored overlay. DICOM RT files are essential in radiotherapy treatment planning.\nThey include:\nRT Structure Set, containing information related to patient anatomy, for example, structures, markers, and isocenters. These entities are typically identified on devices such as CT scanners, physical or virtual simulation workstations, or treatment planning systems. RT Plan, containing geometric and dosimetric data specifying a course of external beam and/or brachytherapy treatment, for example, beam angles, collimator openings, beam modifiers, and brachytherapy channel and source specifications. The RT Plan entity may be created by a simulation workstation, and subsequently enriched by a treatment planning system before being passed on to a record and verify system or treatment device. An instance of the RT Plan object usually references an RT Structure Set instance to define a coordinate system and set of patient structures. RT Dose, containing dose data generated by a treatment planning system in one or more of several formats: three-dimensional dose data, isodose curves, DVHs, or dose points. Additional objects not used in this tutorial are:\n","content":"Example 6: DICOM RT Visualization in MeVisLab – RTSTRUCT and RTDOSE Workflow Introduction This tutorial explains how to load and visualize DICOM RT (Radiotherapy) data in MeVisLab. You will learn how to:\nLoad CT and related RTSTRUCT data. Visualize RTSTRUCTs as colored CSOs. Show labels next to each RTSTRUCT contour. Visualize RTDOSE as a semitransparent colored overlay. DICOM RT files are essential in radiotherapy treatment planning.\nThey include:\nRT Structure Set, containing information related to patient anatomy, for example, structures, markers, and isocenters. These entities are typically identified on devices such as CT scanners, physical or virtual simulation workstations, or treatment planning systems. RT Plan, containing geometric and dosimetric data specifying a course of external beam and/or brachytherapy treatment, for example, beam angles, collimator openings, beam modifiers, and brachytherapy channel and source specifications. The RT Plan entity may be created by a simulation workstation, and subsequently enriched by a treatment planning system before being passed on to a record and verify system or treatment device. An instance of the RT Plan object usually references an RT Structure Set instance to define a coordinate system and set of patient structures. RT Dose, containing dose data generated by a treatment planning system in one or more of several formats: three-dimensional dose data, isodose curves, DVHs, or dose points. Additional objects not used in this tutorial are:\nRT Image, specifying radiotherapy images that have been obtained on a conical imaging geometry, such as those found on conventional simulators and portal imaging devices. It can also be used for calculated images using the same geometry, such as digitally reconstructed radiographs (DRRs). RT Beams Treatment Record, RT Brachy Treatment Record, and RT Treatment Summary Record, containing data obtained from actual radiotherapy treatments. These objects are the historical record of the treatment, and are linked with the other „planning” objects to form a complete picture of the treatment. Precondition If you do not have DICOM RT data, you can download an example dataset at: https://medicalaffairs.varian.com/headandneckbilat-imrtsx2\nAttention:\u0026nbsp; This data is FOR EDUCATIONAL AND SCIENTIFIC EXCHANGE ONLY – NOT FOR SALES OR PROMOTIONAL USE. Extract the .zip file into a new folder named DICOM_FILES.\nPrepare Your Network Add the module DicomImport to your workspace.\nThen, click Browse and select the new folder named DICOM_FILES where you copied the content of the ZIP file earlier. Click Import . You can see the result after import below:\nDICOM RT Data in DicomImport module The dataset contains an anonymized patient with four series:\nRTPLAN \u0026lt;no image data\u0026gt; RTSTRUCT \u0026lt;no image data\u0026gt; CT 512×512×272×1 RTDOSE 199×115×147×1 In order to see the images, add a View2D module and connect it to the DicomImport module.\nThe RTPLAN and RTSTRUCT files do not contain pixel data. Therefore, the DicomImport module informs that there is no image data available. The CT series contains the original CT data and the RTDOSE series contains a mask providing three-dimensional dose data.\nRTPLAN RTSTRUCT CT512 RTDOSE Select the CT 512×512×272×1 series.\nWe now want to view the CT images and the RTSTRUCT data together. The module DicomImport only allows to select one single object. In order to select more than one object, we use a DicomImportExtraOutput module. Select the CT series in the DicomImport module and the RTSTRUCT in the DicomImportExtraOutput module.\nYou have to select the correct index for the RTSTRUCT. In our example it is index 2.\nRTSTRUCT in DicomImportExtraOutput Visualize RTSTRUCTs as Colored CSOs Add an ExtractRTStruct module to the DicomImportExtraOutput to convert RTSTRUCT data into MeVisLab contours (CSOs). CSOs allow to visualize the contours on the CT scan and to interact with them in MeVisLab.\nA preview of the resulting CSOs can be seen in the Output Inspector.\nExtractRTStruct in Output Inspector Add a SoView2DCSOExtensibleEditor module to enable visualization and interaction with the CSOs in the 2D viewer.\nSoView2DCSOExtensibleEditor We want to display the names for the contours available in the RTSTRUCT file to identify the segmented structure. Use the CSOLabelRenderer module to show labels (e.g., \u0026lsquo;Bladder\u0026rsquo;, \u0026lsquo;Prostate\u0026rsquo;) next to each contour.\nCSOLabelRenderer By default, the ID of the contours is rendered. Open the panel of the CSOLabelRenderer and change the labelString parameter as seen below.\nCSOLabelRenderer labelString labelString = cso.getGroupAt(0).getLabel() Then, press apply . The name of the structure is defined in the group of each CSO. We now show the label of the group next to the contour. Add a CSOLabelPlacementGlobal module to define a better readable location of these labels.\nThe module CSOLabelPlacementGlobal implements an automatic label placement strategy that considers all CSOs on a slice.\nEdited CSOLabelRenderer Panel 3D Visualization of Contours Using SoExaminerViewer The contours can also be shown in 3D.\nAdd a SoCSO3DRenderer and a SoExaminerViewer module and connect them to the ExtractRTStruct module. The SoCSO3DRenderer will render the contours (CSOs) into the SoExaminerViewer.\nCSOs in 3D Visualizing RTDOSE as a Colored Overlay We now want to show the RTDOSE data as provided for the patient as a semitransparent, colored overlay.\nAdd another DicomImportExtraOutput module to get the RTDOSE object. Again, select the correct index. In this case, we select index 4.\nAdd a MinMaxScan module to scan the input image and calculate the minimum and maximum values of the image. Connect it with the DicomImportExtraOutput module.\nMinMaxScan Add a Histogram and a SoLUTEditor module to calculate the image\u0026rsquo;s intensity distribution and define a colored lookup table for the overlay.\nChange update mode of the Histogram module to Auto Update.\nOpen the panel of the SoLUTEditor module and go to tab Range. Click Update Range From Histogram to apply the histogram values for the Range of the lookup table.\nLookup table and Histogram On tab Editor, define a lookup table as seen below.\nLookup table The lookup table shall be used for showing the RT Dose data as a semitransparent overlay on the CT image. Add a SoView2DOverlay and a SoGroup module to your network. Replace the input of the View2D module from the SoView2DCSOExtensibleEditor with the SoGroup.\nRT Dose data using SoView2DOverlay If you want to visualize the RT Struct contours together with the RT Dose overlay, connect the SoView2DCSOExtensibleEditor module and the SoGroup module.\nRT Dose and RT Struct Summary DICOM RT data can be loaded and processed in MeVisLab. RT Structure Sets can be converted to MeVisLab contours and visualized using ExtractRTStruct and CSOLabelRenderer modules. Anatomical information can be shown using the module CSOLabelRenderer. RT Dose files can be shown as a semitransparent colored overlay using SoView2DOverlay. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Image Processing","DICOM","RTSTRUCT","RTDOSE","RTPLAN"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/","title":"Chapter V: Data Objects","summary":"Data Objects in MeVisLab MeVisLab provides predefined data objects, for example:\nContour Segmentation Objects (CSOs) three-dimensional objects encapsulating formerly defined contours within images. Surface Objects (Winged Edge Meshes or WEMs) represent the surface of geometrical figures and allow the user to manipulate them. Markers are used to mark specific locations or aspects of an image and allow to process those later on. Curves can print the results of a function as two-dimensional mathematical graphs into a diagram. Usage, advantages, and disadvantages of each above-mentioned data object type will be covered in the following specified chapters, where you will be building example networks for some of the most common use cases.\n","content":"Data Objects in MeVisLab MeVisLab provides predefined data objects, for example:\nContour Segmentation Objects (CSOs) three-dimensional objects encapsulating formerly defined contours within images. Surface Objects (Winged Edge Meshes or WEMs) represent the surface of geometrical figures and allow the user to manipulate them. Markers are used to mark specific locations or aspects of an image and allow to process those later on. Curves can print the results of a function as two-dimensional mathematical graphs into a diagram. Usage, advantages, and disadvantages of each above-mentioned data object type will be covered in the following specified chapters, where you will be building example networks for some of the most common use cases.\n","tags":["Beginner","Tutorial","Data Objects","2D","Contours","3D","Surfaces"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contourobjects/","title":"Contour Objects (CSO)","summary":"Contour Segmentation Objects (CSOs) in MeVisLab Introduction Structure of CSOs MeVisLab provides modules to create contours in images. 3D objects that encapsulate these contours are called Contour Segmentation Objects (CSOs).\nIn the next image, you can see a rectangular shaped CSO. The pink circles you can see are called Seed Points.\nSeed Points define the shape of the CSO. In the case of a rectangle, you need four Seed Points forming the corners to define the whole rectangle.\n","content":"Contour Segmentation Objects (CSOs) in MeVisLab Introduction Structure of CSOs MeVisLab provides modules to create contours in images. 3D objects that encapsulate these contours are called Contour Segmentation Objects (CSOs).\nIn the next image, you can see a rectangular shaped CSO. The pink circles you can see are called Seed Points.\nSeed Points define the shape of the CSO. In the case of a rectangle, you need four Seed Points forming the corners to define the whole rectangle.\nThe points forming the blue lines are called Path Points.\nThe Path Points form the connection between the Seed Points whereby contour objects (CSOs) are generated. CSOs are often closed, but do not need to be.\nIn general, the Seed Points are created interactively using an editor module and the Path Points are generated automatically by interpolation or other algorithms.\nContour Segmented Object (CSO) CSO Editors As mentioned, when creating CSOs, you can do this interactively by using an editor.\nThe following images show editors available in MeVisLab for drawing CSOs:\nSoCSOPointEditor SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor SoCSODistancePolylineEditor SoCSOEllipseEditor SoCSORectangleEditor SoCSOSplineEditor SoCSOPolygonEditor SoCSOIsoEditor SoCSOLiveWireEditor Extra Infos:\u0026nbsp; The SoCSOIsoEditor and SoCSOLiveWireEditor are special, because they are using an algorithm to detect edges themselves.\nThe SoCSOIsoEditor generates isocontours interactively. The SoCSOLiveWireEditor renders and semi-interactively generates CSOs based on the LiveWire algorithm. CSO Lists and CSO Groups All created CSOs are stored in CSO lists that can be saved and loaded on demand. The lists cannot only store the coordinates of the CSOs, but also additional information in the form of name-value pairs (using specialized modules or Python scripting).\nBasic CSO Network Each SoCSO*Editor requires a SoView2DCSOExtensibleEditor that manages attached CSO editors and renderers and offers an optional default renderer for all types of CSOs. In addition to that, the list of CSOs needs to be stored in a CSOManager.\nThe appearance of the CSO can be defined by using a SoCSOVisualizationSettings module.\nCSOs can also be grouped together. The following image shows two different CSO groups. Groups can be used to organize CSOs, in this case to distinguish the CSOs of the right and the left lung. Here you can find more information about CSO Groups.\nCSO Groups Extra Infos:\u0026nbsp; For more information, see CSO Overview ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contours/contourexample1/","title":"Contour Example 1: Creation of Contours","summary":"Contour Example 1: Creation of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We like to start with the creation of CSOs. To create CSOs, you need a SoCSO*-Editor. There are several different editors that can be used to create CSOs (see here). Some of them are introduced in this example.\nSteps to Do Develop Your Network For this example, we need the following modules. Add the modules to your workspace, connect them as shown below, and load the example image $(DemoDataPath)/BrainMultiModal/ProbandT1.tif.\n","content":"Contour Example 1: Creation of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We like to start with the creation of CSOs. To create CSOs, you need a SoCSO*-Editor. There are several different editors that can be used to create CSOs (see here). Some of them are introduced in this example.\nSteps to Do Develop Your Network For this example, we need the following modules. Add the modules to your workspace, connect them as shown below, and load the example image $(DemoDataPath)/BrainMultiModal/ProbandT1.tif.\nData Objects Contours Example 1 Edit Rectangular CSO Now, open the module View2D. Use your left mouse button , to draw a rectangle as your first CSO.\nRectangle Contour The involved modules have the following tasks:\nSoCSORectangleEditor: Enables the creation of the CSO and defines the shape of the CSOs\nSoView2DCSOExtensibleEditor: Manages attached CSO editors and the appearance of CSOs\nCSOManager: Creates a list of all drawn CSOs and offers the possibility to group CSOs\nIf you now open the panel of the CSOManager, you will find one CSO, the one we created before. If you like, you can name the CSO.\nCSO Manager Change Properties of CSO Now, add the module SoCSOVisualizationSettings to your workspace and connect it as shown below.\nCSO Manager Open the module to change the visualization settings of your CSOs. In this case, we change the line style (to dashed lines) and the color (to be red). Tick the Auto apply box at the bottom or press Apply.\nVisualization Settings CSOs of Different Shapes Exchange the module SoCSORectangleEditor with another editor, for example, the SoSCOPolygonEditor or SoCSOSplineEditor. Other editors allow to draw CSOs of other shapes. For polygon-shaped CSOs or CSOs consisting of splines, left-click on the image viewer to add new points to form the CSO. Double-click to finish the CSO.\nSoSCOPolygonEditor SoCSOSplineEditor Draw Filled CSOs If you want to fill the shapes, you can simply add a SoCSOFillingRenderer module to your SoView2DCSOExtensibleEditor. SoCSOFillingRenderer Exercises Create CSOs with green color and ellipsoid shapes.\nSummary CSOs can be created using a SoCSO*-Editor. CSOs of different shapes can be created. A list of CSOs can be stored in the CSOManager. Properties of CSOs can be changed using SoCSOVisualizationSettings. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contours/contourexample2/","title":"Contour Example 2: Contour Interpolation","summary":"Contour Example 2: Creating Contours using Live Wire and Interpolation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to create CSOs using the Live Wire Algorithm, which allows semiautomatic CSO creation. The algorithm uses edge detection to support the user creating CSOs.\nWe also like to interpolate CSOs over slices. That means additional CSOs are generated between manual segmentations based on a linear interpolation.\n","content":"Contour Example 2: Creating Contours using Live Wire and Interpolation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to create CSOs using the Live Wire Algorithm, which allows semiautomatic CSO creation. The algorithm uses edge detection to support the user creating CSOs.\nWe also like to interpolate CSOs over slices. That means additional CSOs are generated between manual segmentations based on a linear interpolation.\nAs a last step, we will group together CSOs of the same anatomical unit.\nSteps to Do Develop Your Network and Create CSOs In order to do that, create the shown network. You can use the network from the previous example and exchange the SoCSO-Editor. In addition to that, load the example image $(DemoDataPath)/Thorax1_CT.small.tif . Now, create some CSOs on different, not consecutive slices. Afterward, hover over the CSOManager and press the emerging plus symbol. This displays the amount of existing CSOs.\nData Objects Contours Example 2 Create CSO Interpolations We like to generate interpolated contours for existing CSOs. In order to do that, add the module CSOSliceInterpolator to your workspace and connect it as shown.\nSlice Interpolation Open the panel of module CSOSliceInterpolator and change the Group Handling and the Mode as shown. If you now press Update, interpolating CSOs are created.\nSlice Interpolation Settings You can see the interpolated CSOs are added to the CSOManager. If you now scroll through your slices, you can find the interpolated CSOs.\nYou can also take a look on all existing CSOs by inspecting the output of the CSOManager using the Output Inspector. Custom CSOs are displayed in white and interpolated CSOs are marked in yellow.\nInterpolated CSOs Group CSOs We like to segment both lobes of the lung. To distinguish the CSOs of both lungs, we like to group CSOs together, according to the lung they belong to. First, we like to group together all CSOs belonging to the lung we already segmented. In order to do this, open the CSOManager. Create a new Group and label that Group. We chose the label Left Lung. Now, mark the created Group and all CSOs you want to include into that group and press Combine. If you click on the Group, all CSOs belonging to this Group are marked with a star.\nAttention:\u0026nbsp; Keep in mind, that the right lung might be displayed on the left side of the image and vice versa, depending on your view. Creating CSO Groups Creating CSO Groups As a next step, segment the right lung by creating new CSOs. Creation of further CSOs Create a new Group for all CSOs of the right lung. We labeled this Group Right Lung. Again, mark the group and the CSOs you like to combine and press Combine. Grouping remaining CSOs To visually distinguish the CSOs of both groups, change the color of each group under [ Group \u0026rarr; Visuals ]. We changed the color of the Left Lung to be green and of the Right Lung to be orange for path and seed points. In addition, we increased the Width of the path points. Interpolated CSOs As a last step, we need to disconnect the module SoCSOVisualizationSettings, as this module overwrites the visualization settings we enabled for each group in the CSOManager. Interpolated CSOs Summary SoCSOLiveWireEditor can be used to create CSOs semiautomatically. CSO interpolations can be created using CSOSliceInterpolator. CSOs can be grouped together using the CSOManager. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","Interpolation"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contours/contourexample3/","title":"Contour Example 3: 2D and 3D Visualization of Contours","summary":"Contour Example 3: Overlay Creation and 3D Visualization of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we\u0026rsquo;d like to use the created CSOs to display an overlay. This allows us to mark one of two lungs. In addition to that, we will display the whole segmented lobe of the lung in a 3D image.\nSteps to Do Develop Your Network Use the network from the contour example 2 and add the modules VoxelizeCSO, SoView2DOverlay and View2D to your workspace. Connect the module as shown. The module VoxelizeCSO allows to convert CSOs into voxel images.\n","content":"Contour Example 3: Overlay Creation and 3D Visualization of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we\u0026rsquo;d like to use the created CSOs to display an overlay. This allows us to mark one of two lungs. In addition to that, we will display the whole segmented lobe of the lung in a 3D image.\nSteps to Do Develop Your Network Use the network from the contour example 2 and add the modules VoxelizeCSO, SoView2DOverlay and View2D to your workspace. Connect the module as shown. The module VoxelizeCSO allows to convert CSOs into voxel images.\nData Objects Contours Example 3 Convert CSOs into Voxel Images Update the module VoxelizeCSOs to create voxel masks based on your CSOs. The result can be seen in View2D1.\nOverlay Next, we like to inspect the marked lobe of the lung. This means we like to inspect the object that is built out of CSOs. In order to do that, add the View3D module. The 3D version of the lung can be seen in the viewer.\nAdditional 3D Viewer Extracted Object Summary The module VoxelizeCSO converts CSOs to voxel images. Create an overlay out of voxel images using SoView2DOverlay. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contours/contourexample4/","title":"Contour Example 4: Annotation of Images","summary":"Contour Example 4: Annotation of Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to calculate the volume of our object, in this case, the part of the lung we have segmented.\nSteps to Do Develop Your Network and Calculate the Lung Volume Add the modules CalculateVolume and SoView2DAnnotation to your workspace and connect both modules as shown. Update the module CalculateVolume, which directly shows the volume of our object.\n","content":"Contour Example 4: Annotation of Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to calculate the volume of our object, in this case, the part of the lung we have segmented.\nSteps to Do Develop Your Network and Calculate the Lung Volume Add the modules CalculateVolume and SoView2DAnnotation to your workspace and connect both modules as shown. Update the module CalculateVolume, which directly shows the volume of our object.\nData Objects Contours Example 4 Display the Lung Volume in the Image We now like to display the volume in the image viewer. For this, open the panel of the modules CalculateVolume and SoView2DAnnotation. Open the tab Input in the panel of the module SoView2DAnnotation. Now, establish a parameter connection between Total Volume calculated in the module CalculateVolume and the input00 of the module SoView2DAnnotation. This connection projects the Total Volume to the input of SoView2DAnnotation.\nDisplay Volume Go back to the tab General to select the Annotation Mode User. A separate tab exists for each annotation mode.\nAnnotate Image We select the tab User that we like to work on. You can see four fields that display four areas of a viewer in which you can add information text to the image.\nAnnotate Image In this example we only like to add the volume, so delete all present input and replace that by the shown text. Now, you can see that the volume is displayed in the image viewer. If this is not the case, switch the annotations of the viewer by pressing the keyboard shortcut A .\nDisplay Volume in Image Summary CalculateVolume can calculate the volume of a voxel image. SoView2DAnnotation enables to manually change the annotation mode of a viewer. Annotations shown in a View2D can be customized by using a SoView2DAnnotation module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","Annotations"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contours/contourexample5/","title":"Contour Example 5: Contours and Ghosting","summary":"Contour Example 5: Visualizing Contours and Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to automatically create CSOs based on a predefined isovalue.\nSteps to Do Develop Your Network Add the following modules to your workspace and connect them as shown. Load the example image Bone.tiff.\nAutomatic Creation of CSOs Based on the Isovalue Now, open the panel of CSOIsoGenerator to set the Iso Value to 1200. If you press Update in the panel, you can see the creation of CSOs on each image slice when opening the module View2D. In addition to that, the number of CSOs is displayed in the CSOManager. The module CSOIsoGenerator generates isocontours for each slice at a fixed isovalue. This means that closed CSOs are formed based on the detection of the voxel value of 1200 on every slice.\n","content":"Contour Example 5: Visualizing Contours and Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to automatically create CSOs based on a predefined isovalue.\nSteps to Do Develop Your Network Add the following modules to your workspace and connect them as shown. Load the example image Bone.tiff.\nAutomatic Creation of CSOs Based on the Isovalue Now, open the panel of CSOIsoGenerator to set the Iso Value to 1200. If you press Update in the panel, you can see the creation of CSOs on each image slice when opening the module View2D. In addition to that, the number of CSOs is displayed in the CSOManager. The module CSOIsoGenerator generates isocontours for each slice at a fixed isovalue. This means that closed CSOs are formed based on the detection of the voxel value of 1200 on every slice.\nData Objects Contours Example 5 Ghosting Now, we like to make CSOs of previous and subsequent slices visible (ghosting). In order to do that, open the panel of SoCSOVisualizationSettings and open the tab Misc. Increase the parameter Ghosting depth in voxel, which shows you the number of slices above and below the current slice in which CSOs are also seen in the viewer. The result can be seen in the viewer.\nGhosting Display Created CSOs At last, we like to make all CSOs visible in a 3D viewer. To do that, add the modules SoCSO3DRenderer and SoExaminerViewer to your network and connect them as shown. In the viewer SoExaminerViewer, you can see all CSOs together. In this case all scanned bones can be seen.\nCSOs in 3D View Summary CSOIsoGenerator enables automatic CSO generation based on an isovalue. Ghosting allows to display CSOs of previous and following slices. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO"],"section":"tutorials"},{"date":"1710115200","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contours/contourexample6/","title":"Contour Example 6: Adding Labels to Contours","summary":"Contour Example 6: Adding Labels to Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are adding a label to a contour. The label provides information about measurements and about the contour itself. The label remains connected to the contour and can be moved via mouse interactions.\nSteps to Do Develop Your Network Add the modules LocalImage and View2D to your workspace and connect them as shown below. Load the file ProbandT1.dcm from MeVisLab demo data. In order to create contours (CSOs), we need a SoView2DCSOExtensibleEditor module. It manages attached CSO editors, renderers and offers an optional default renderer for all types of CSOs.\n","content":"Contour Example 6: Adding Labels to Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are adding a label to a contour. The label provides information about measurements and about the contour itself. The label remains connected to the contour and can be moved via mouse interactions.\nSteps to Do Develop Your Network Add the modules LocalImage and View2D to your workspace and connect them as shown below. Load the file ProbandT1.dcm from MeVisLab demo data. In order to create contours (CSOs), we need a SoView2DCSOExtensibleEditor module. It manages attached CSO editors, renderers and offers an optional default renderer for all types of CSOs.\nThe first CSO we want to create is a distance line. Add a SoCSODistanceLineEditor to the SoView2DCSOExtensibleEditor. It renders and interactively generates CSOs that consist of a single line segment. The line segment can be rendered as an arrow; it can be used to measure distances.\nWe are going to add some more editors later. In order to have the same look and feel for all types of CSOs, add a SoCSOVisualizationSettings module as seen below. The module is used to adjust visual parameters like color and line style for CSOs. Also add a CSOManager module to organize CSOs and CSOGroups within a network.\nInitial Network We are now able to create lines in the View2D. You can also modify the lines by dragging the seed points to a different location.\nSoCSODistanceLineEditor The created lines do neither provide any details about the length of your measurement, nor a unique ID to identify it in the case of multiple CSOs.\nAdd a CSOLabelRenderer module to your network and connect it to a SoGroup. Also connect your SoCSODistanceLineEditor to the SoGroup as seen below. The ID of each CSO appears next to your distance lines. Moving the ID also shows the name of the contour.\nCSOLabelRenderer We now want to customize the details to be shown for each distance line. Open the panel of the CSOLabelRenderer. You can see the two parameters labelString and labelName. The labelString is set to the ID of the CSO. The labelName is set to a static text and the label property of the CSO. The label can be defined in the module CSOManager. You can do this, but we are not defining a name for each contour in this example.\nEnter the following to the panel of the CSOLabelRenderer module: CSOLabelRenderer\nlabelString = f\u0026#34;Length: {cso.getLength()} mm\u0026#34; labelName = f\u0026#34;ID: {cso.getId()}\u0026#34; deviceOffsetX = 0 deviceOffsetY = 0 We are setting the labelName to a static text showing the type of the CSO and the unique ID of the contour. We also define the labelString to the static description of the measurement and the length parameter of the CSO.\nlabelString and labelName You can also round the length by using: CSOLabelRenderer\nlabelString = f\u0026#34;Length: {cso.getLength():.2f} mm\u0026#34; In order to see all possible parameters of a CSO, add a CSOInfo module to your network and connect it to the CSOManager. The geometric information of the selected CSO from CSOManager can be seen there.\nCSOInfo For labels shown on grayscale images, it makes sense to add a shadow. Open the panel of the SoCSOVisualizationSettings module and on tab Misc check the option Should render shadow. This increases the readability of your labels.\nEx6_NoShadow Ex6_Shadow If you want to define your static text as a parameter in multiple labels, you can open the panel of the CSOLabelRenderer module and define text as User Data. The values can then be used in Python via userData.\nUser Data You can also add multiple CSO editors to see the different options. Add the SoCSORectangleEditor module to your workspace and connect it to the SoGroup module. As we now have two different editors, we need to tell the CSOLabelRenderer which CSO is to be rendered. Open the panel of the SoCSODistanceLineEditor. You can see the field Extension Id set to distanceLine. Open the panel of the SoCSORectangleEditor. You can see the field Extension Id set to rectangle.\nExtension ID We currently defined the labelName and labelString for the distance line. If we want to define different labels for different types of CSOs, we have to change the CSOLabelRenderer Python script. Open the panel of the CSOLabelRenderer and change the Python code to the following:\nCSOLabelRenderer\nif cso.getSubType() == \u0026#34;distanceLine\u0026#34;: labelString = f\u0026#34;{userData0} {cso.getLength():.2f} mm\u0026#34; labelName = userData1 labelName += str(cso.getId()) elif cso.getSubType() == \u0026#34;rectangle\u0026#34;: labelString = f\u0026#34;{userData0} {cso.getLength():.2f} mm\\n\u0026#34; labelString += f\u0026#34;{userData2} {cso.getArea():.2f} mm^2\u0026#34; labelName = userData3 labelName += str(cso.getId()) deviceOffsetX = 0 deviceOffsetY = 0 SoCSORectangleEditor If you now draw new CSOs, you will notice that you still always create distance lines. Open the panel of the SoView2DCSOExtensibleEditor. You can see that the Creator Extension Id is set to __default. By default, the first found eligible editor is used to create a new CSO. In our case this is the SoCSODistanceLineEditor.\nSoCSORectangleEditor Change Creator Extension Id to rectangle.\nSoCSORectangleEditor \u0026amp; SoView2DCSOExtensibleEditor Newly created CSOs are now rectangles. The label values are shown as defined in the CSOLabelRenderer and show the length and the area of the rectangle.\nLabeled Rectangle in View2D Extra Infos:\u0026nbsp; The Length attribute in the context of rectangles represents the perimeter of the rectangle, calculated as 2a + 2b, where a and b are the lengths of the two sides of the rectangle. You will find a lot more information in the CSOInfo module for your rectangles. The exact meaning of the values for each type of CSO is explained in the table below.\nCSOInfo Parameters and Meanings for All CSO Types CSO Editor PCA X Ext. PCA Y Ext. PCA Z Ext. Length Area SoCSOPointEditor n.a. n.a. n.a. n.a. n.a. SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor Length (in mm) SoCSODistancePolylineEditor Length of all lines (in mm) SoCSOEllipseEditor Perimeter (in mm) Area (in mm2) SoCSORectangleEditor Length of all sides (in mm) Area (in mm2) SoCSOSplineEditor SoCSOPolygonEditor Length of all lines (in mm) SoCSOIsoEditor SoCSOLiveWireEditor Summary Custom labels can be added to contours using the CSOLabelRenderer module. Python scripting is used within the CSOLabelRenderer module to customize label content based on CSO types. Visual properties can be adjusted within the CSOLabelRenderer and the SoCSOVisualizationSettings modules to improve label visibility and appearance. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","Label"],"section":"tutorials"},{"date":"1722470400","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/contours/contourexample7/","title":"Contour Example 7: Using the CSOListContainer","summary":"Contour Example 7: Using the CSOListContainer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the module CSOListContainer instead of the CSOManager. The CSOManager is a heavyweight, UI driven module. You can use it to see all of your CSOs and CSOGroups in the module panel. The CSOListContainer is a lightweight module with focus on Python scripting. We recommend to use this module for final application development, because Python provides much more flexibility in handling CSO objects.\n","content":"Contour Example 7: Using the CSOListContainer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the module CSOListContainer instead of the CSOManager. The CSOManager is a heavyweight, UI driven module. You can use it to see all of your CSOs and CSOGroups in the module panel. The CSOListContainer is a lightweight module with focus on Python scripting. We recommend to use this module for final application development, because Python provides much more flexibility in handling CSO objects.\nCSOManager CSOListContainer We will create multiple CSOs by using the SoCSOEllipseEditor and dynamically add these CSOs to different groups via Python scripting depending on their size. CSOs larger than a configurable threshold will be drawn in red, small CSOs will be drawn in green. The colors will also be adapted if we manually resize the contours.\nSteps to Do Develop Your Network Add a LocalImage and a View2D module to your workspace and connect them as shown below. Load the file ProbandT1.dcm from MeVisLab demo data. In order to create contours (CSOs), we need a SoView2DCSOExtensibleEditor module. It manages attached CSO editors, renderers, and offers an optional default renderer for all types of CSOs.\nAdd a SoCSOEllipseEditor and a CSOListContainer to the SoView2DCSOExtensibleEditor\nInitial Network You are now able to draw CSOs.\nCreate a separate directory for this tutorial and save your network in this empty directory. This makes the final structure easier to read.\nCreate a Local Macro Module Select the module CSOListContainer and open menu [ File \u0026rarr; Create Local Macro ]. Enter some details about your new local macro module and click Finish. Leave the already defined output as is.\nCreate Local Macro The appearance of the CSOListContainer module changes, because it is a macro module named csoList now.\nNetwork with new local macro The behavior of your network does not change. You can still draw the same CSOs and they are still managed by the CSOListContainer module. The reason why we created a local macro with a single module inside is that we want to add Python scripting to the module. Python scripts can only be added to macro modules.\nOpen the context menu of your csoList module and select [ Related Files \u0026rarr; csoList.script ].\nThe MeVisLab text editor MATE opens, showing your .script file. You can see the output of your module as CSOListContainer.outCSOList. We want to define a threshold for the color of our CSOs. For this, add another field to the Parameters section of your script file named areaThreshold. Define the type as Float and value as 2000.0.\nIn order to call Python functions, we also need a Python file. Add a Commands section and define the source of the Python file as $(LOCAL)/csoList.py. Also add an initCommand as initCSOList. The initCommand defines the Python function that is called whenever the module is added to the workspace or reloaded.\ncsoList.script\nInterface { Inputs {} Outputs { Field baseOut0 { internalName = CSOListContainer.outCSOList } } Parameters { Field areaThreshold { type = Float value = 2000.0 } } } Commands { source = $(LOCAL)/csoList.py initCommand = initCSOList } Right-click on the initCSOList command and select [ Create Python Function initCSOList ]. The Python file and the function are generated automatically.\nBack in MeVisLab, the new field areaThreshold can be seen in Module Inspector when selecting your module. The next step is to write the Python function initCSOList.\nWrite Python Script Whenever the local macro module is added to the workspace or reloaded, new CSOLists shall be created and we need a possibility to update the lists whenever a new CSO has been created or existing contours changed.\nDefine a function setupCSOList.\ncsoList.py\ndef setupCSOList(): csoList = _getCSOList() csoList.removeAll() csoGroupSmall = csoList.addGroup(\u0026#34;small\u0026#34;) csoGroupLarge = csoList.addGroup(\u0026#34;large\u0026#34;) csoGroupSmall.setUsePathPointColor(True) csoGroupSmall.setPathPointColor((0, 1, 0)) csoGroupLarge.setUsePathPointColor(True) csoGroupLarge.setPathPointColor((1, 0, 0)) def _getCSOList(): return ctx.field(\u0026#34;CSOListContainer.outCSOList\u0026#34;).object() The function gets the current CSOList from the output field of the CSOListContainer. Initially, it should be empty. If not, we want to start with an empty list; therefore, we remove all existing CSOs.\nWe also create two new CSO lists: one list for small contours, one list for larger contours, depending on the defined areaThreshold from the modules parameter.\nAdditionally, we also want to define different colors for the CSOs in the lists. Small contours shall be drawn in green, large contours shall be drawn in red.\nIn order to listen for changes on the contours, we need to register for notifications. Create a new function registerForNotification.\ncsoList.py\ndef registerForNotification(): csoList = _getCSOList() csoList.registerForNotification(csoList.NOTIFICATION_CSO_FINISHED, ctx, \u0026#34;csoFinished\u0026#34;) def csoFinished(_arg): csoList = _getCSOList() for cso in csoList.getCSOs(): cso.removeFromAllGroups() csoArea = cso.getArea() csoGroup = csoList.getGroupByLabel(\u0026#34;large\u0026#34;) if csoArea \u0026lt;= _getAreaThreshold(): csoGroup = csoList.getGroupByLabel(\u0026#34;small\u0026#34;) csoGroup.addCSO(cso.getId()) def _getAreaThreshold(): return ctx.field(\u0026#34;areaThreshold\u0026#34;).value The function gets all currently existing CSOs from the CSOListContainer. Then, we register for notifications on this list. Whenever the notification NOTIFICATION_CSO_FINISHED is sent in the current context, we call the function csoFinished.\nThe csoFinished function again needs all existing contours. We walk through each CSO in the list and remove it from all groups. As we do not know which CSO has been changed from the notification, we evaluate the area of each CSO and add them to the correct list again.\nThe function getAreaThreshold returns the current value of our parameter field areaThreshold.\nNow, we can call our functions in the initCSOList function and test our module.\ncsoList.py\ndef initCSOList(): setupCSOList() registerForNotification() def setupCSOList(): csoList = _getCSOList() csoList.removeAll() csoGroupSmall = csoList.addGroup(\u0026#34;small\u0026#34;) csoGroupLarge = csoList.addGroup(\u0026#34;large\u0026#34;) csoGroupSmall.setUsePathPointColor(True) csoGroupSmall.setPathPointColor((0, 1, 0)) csoGroupLarge.setUsePathPointColor(True) csoGroupLarge.setPathPointColor((1, 0, 0)) def registerForNotification(): csoList = _getCSOList() csoList.registerForNotification(csoList.NOTIFICATION_CSO_FINISHED, ctx, \u0026#34;csoFinished\u0026#34;) def csoFinished(_arg): csoList = _getCSOList() for cso in csoList.getCSOs(): cso.removeFromAllGroups() csoArea = cso.getArea() csoGroup = csoList.getGroupByLabel(\u0026#34;large\u0026#34;) if csoArea \u0026lt;= _getAreaThreshold(): csoGroup = csoList.getGroupByLabel(\u0026#34;small\u0026#34;) csoGroup.addCSO(cso.getId()) def _getAreaThreshold(): return ctx.field(\u0026#34;areaThreshold\u0026#34;).value def _getCSOList(): return ctx.field(\u0026#34;CSOListContainer.outCSOList\u0026#34;).object() Final Network If you now draw contours, they are automatically colored depending on the size. You can also edit existing contours and the color is adapted.\nSummary The module CSOListContainer provides a lightweight Python interface to manage contours. It makes sense to encapsulate a single module into a macro module to provide additional functionalities via Python scripting. Notifications can be used to react on events. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Data Objects","2D","Contours","CSO","Notifications","CSOListContainer"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/surfaceobjects/","title":"Surface Objects (WEM)","summary":"Surface Objects (WEMs) Introduction In MeVisLab it is possible to create, visualize, process, and manipulate surface objects, also known as polygon meshes. Here, we call surface objects Winged Edge Mesh, in short WEM. In this chapter you will get an introduction into WEMs. In addition, you will find examples on how to work with WEMs. For more information on WEMs, take a look at the MeVisLab Toolbox Reference . If you like to know which WEM formats can be imported into MeVisLab, take a look at the assimp documentation here.\n","content":"Surface Objects (WEMs) Introduction In MeVisLab it is possible to create, visualize, process, and manipulate surface objects, also known as polygon meshes. Here, we call surface objects Winged Edge Mesh, in short WEM. In this chapter you will get an introduction into WEMs. In addition, you will find examples on how to work with WEMs. For more information on WEMs, take a look at the MeVisLab Toolbox Reference . If you like to know which WEM formats can be imported into MeVisLab, take a look at the assimp documentation here.\nWEM in MeVisLab Explained To explain WEMs in MeVisLab, we will build a network that shows the structure and the characteristics of WEMs. We will start the example by generating a WEM forming a cube. With this, we will explain structures of WEMs called Edges, Nodes, Surfaces, and Normals.\nInitialize a WEM Add the module WEMInitialize to your workspace, open its panel, and select a Cube. In general, a WEM is made up of surfaces. Here all surfaces are squares. In MeVisLab it is common to build WEMs out of triangles.\nWEM initializing Rendering of WEMs For rendering WEMs, you can use the module SoWEMRenderer in combination with the viewer SoExaminerViewer. Add both modules to your network and connect them as shown. A background is always a nice feature to have. WEM rendering Geometry of WEMs The geometry of WEMs is given by different structures. Using specialized WEM renderer modules, all structures can be visualized.\nEdges Add and connect the module SoWEMRendererEdges to your workspace to enable the rendering of WEM Edges. Here, we manipulated the line thickness to make the lines of the edges thicker. WEM Edges Nodes Nodes mark the corner points of each polygon. Therefore, nodes define the geometric properties of every WEM. To visualize the nodes, add and connect the module SoWEMRendererNodes as shown. By default, the nodes are visualized with an offset to the position they are located in. We reduced the offset to be zero, increased the point size, and changed the color. WEM Nodes Faces Between the nodes and alongside the edges, surfaces are created. The rendering of these surfaces can be enabled and disabled using the panel of SoWEMRenderer. WEM Faces Normals Normals display the orthogonal vector either to the faces (Face Normals) or to the nodes (Nodes Normals). With the help of the module SoWEMRendererNormals, these structures can be visualized.\nWEM normal editor WEMNodeNormals WEMFaceNormals WEMs in MeVisLab In MeVisLab, WEMs can consist of triangles, squares, or other polygons. Most common in MeVisLab are surfaces composed of triangles, as shown in the following example. With the help of the module WEMLoad, existing WEMs can be loaded into the network.\nWEMTriangles WEMNetwork WEMSurface Summary WEMs are polygon meshes, in most cases composed of triangles. WEM\u0026rsquo;s geometry is determined by nodes, edges, faces, and normals, which can be visualized using renderer modules. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/surfaces/surfaceexample1/","title":"Surface Example 1: Creation of WEMs","summary":"Surface Example 1: Create Winged Edge Mesh out of Voxel Images and CSOs \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you will learn how to create a Winged Edge Mesh (WEM). There are several approaches on creating WEMs, a few of them are shown in this example. Instead of creating WEMs, they can also be imported, see chapter Surface Objects (WEM).\nSteps to Do From Image to Surface: Generating WEMs out of Voxel Images At first, we will create a WEM out of a voxel image using the module WEMIsoSurface. Add and connect the shown modules. Load the image $(DemoDataPath)/Bone.tiff and set the Iso Min. Value in the panel of WEMIsoSurface to 1200. Tick the box Use image max. value. The module WEMIsoSurface creates surface objects out of all voxels with an isovalue equal or above 1200 (and smaller than the image max value). The module SoWEMRenderer can now be used to generate an Open Inventor scene, which can be displayed by the module SoExaminerViewer.\n","content":"Surface Example 1: Create Winged Edge Mesh out of Voxel Images and CSOs \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you will learn how to create a Winged Edge Mesh (WEM). There are several approaches on creating WEMs, a few of them are shown in this example. Instead of creating WEMs, they can also be imported, see chapter Surface Objects (WEM).\nSteps to Do From Image to Surface: Generating WEMs out of Voxel Images At first, we will create a WEM out of a voxel image using the module WEMIsoSurface. Add and connect the shown modules. Load the image $(DemoDataPath)/Bone.tiff and set the Iso Min. Value in the panel of WEMIsoSurface to 1200. Tick the box Use image max. value. The module WEMIsoSurface creates surface objects out of all voxels with an isovalue equal or above 1200 (and smaller than the image max value). The module SoWEMRenderer can now be used to generate an Open Inventor scene, which can be displayed by the module SoExaminerViewer.\nWEM From Surface to Image: Generating Voxel Images out of WEM It is not only possible to create WEMs out of voxel images. You can also transform WEMs into voxel images: Add and connect the modules VoxelizeWEM and View2D as shown and press the Update button of the module VoxelizeWEM.\nWEM From Contour to Surface: Generating WEMs out of CSOs Now, we like to create WEMs out of CSOs. To create CSOs, load the network from Contour Example 2 and create some CSOs.\nNext, add and connect the module CSOToSurface to convert CSOs into a surface object. To visualize the created WEM, add and connect the modules SoWEMRenderer and SoExaminerViewer.\nWEM It is also possible to display the WEM in 2D in addition to the original image. In order to do that, add and connect the modules SoRenderSurfaceIntersection and SoView2DScene. The module SoRenderSurfaceIntersection allows to display the voxel image and the created WEM in one viewer using the same coordinates. In its panel, you can choose the color used for visualizing the WEM. The module SoView2DScene renders an Open Inventor scene graph into 2D slices.\nWEM If you like to transform WEMs back into CSOs, have a look at the module WEMClipPlaneToCSO.\nSummary Voxel images can be transformed into WEMs using WEMIsoSurface. WEMs can be transformed into voxel images using VoxelizeWEM. CSOs can be transformed into WEMS using CSOToSurface. WEMs can be transformed into voxel images using WEMClipPlaneToCSO. Warning:\u0026nbsp; Whenever converting voxel data to pixel data, keep the so called Partial Volume Effect in mind, see wikipedia for details. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/surfaces/surfaceexample2/","title":"Surface Example 2: Processing and Modification of WEM","summary":"Surface Example 2: Processing and Modifying of WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to modify and process WEMs.\nSteps to Do Develop Your Network Modification of WEMs Use the module WEMLoad to load the file venus.off. Then, add and connect the shown modules. We like to display the WEM venus two times, one time this WEM is modified. You can use the module WEMModify to apply modifications. In its panel, change the scale and the size of the WEM. Now, you see two times the venus next to each other.\n","content":"Surface Example 2: Processing and Modifying of WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to modify and process WEMs.\nSteps to Do Develop Your Network Modification of WEMs Use the module WEMLoad to load the file venus.off. Then, add and connect the shown modules. We like to display the WEM venus two times, one time this WEM is modified. You can use the module WEMModify to apply modifications. In its panel, change the scale and the size of the WEM. Now, you see two times the venus next to each other.\nWEMModify Smoothing of WEMs It is possible to smooth the WEM using the module WEMSmooth. Add this module to your network as shown. You can see the difference of the smoothed and the unsmoothed WEM in your viewer. There are more modules that can modify WEMs, for example, WEMExtrude. You can find them via search or in [ Modules \u0026rarr; Visualization \u0026rarr; Surface Meshes (WEM) ].\nWEMSmooth Calculate Distance Between WEMs Now, we like to calculate the distance between our two WEMs. In order to do this, add and connect the module WEMSurfaceDistance as shown.\nCalculate surface distance Annotations in 3D As a last step, we like to draw the calculated distances as annotations into the image. This is a little bit tricky as we need the module SoView2DAnnotation to create annotations in a 3D viewer. Add and connect the following modules as shown. What is done here? We use the module SoView2D to display a 2D image in the SoExaminerViewer, in addition to the WEMs we already see in the viewer. We do not see an additional image in the viewer, as we chose no proper input image to the module SoView2D using the module ConstantImage with value 0. Thus, we pretend to have a 2D image, which we can annotate. Now, we use the module SoView2DAnnotation to annotate the pretended 2D image, displayed in the viewer of SoExaminerViewer. We already used the module SoView2DAnnotation in Contour Example 4.\nIn the SoView2D module, you need to uncheck the option Draw image data.\nAnnotation modules Now, change the Annotation Mode to User, as we like to insert custom annotations. In addition, disable to Show vertical ruler.\nSelect annotation mode Next, open the tab Input and draw parameter connections from the results of the distance calculations, which can be found in the panel of WEMSufaceDistance, to the input fields in the panel of SoView2DAnnotation.\nDefine annotation parameters You can design the annotation overlay as you like in the tab User. We decided to only display the minimal and maximal distance between both WEMs.\nExtra Infos:\u0026nbsp; The WEMSurfaceDistance module measures minimal distances only; when talking about the maximal distance, we mean the maximal minimal distance: this is the maximum of all minimum distances. Annotation design As we use a 2D annotation module to annotate a 3D viewer, it is important to get rid of all 2D orientation annotations, which you can edit in the tab Orientation.\nDisable 2D orientation annotations Now, you can see the result in the viewer. If the annotations are not visible, press a a few times to change the annotation mode. Display surface distance in viewer Summary There are several modules to modify and process WEMs, e.g., WEMModify, WEMSmooth. To calculate the minimal and maximal (the maximum minimum) surface distance between two WEMs, use the module WEMSurfaceDistance. To create annotations in 3D, the module SoView2DAnnotation can be used when adapted to be used in combination with a 3D viewer. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1679356800","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/surfaces/surfaceexample3/","title":"Surface Example 3: Interactions With WEM","summary":"Surface Example 3: Interactions with WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In these examples, we are showing two different possibilities to interact with the visualization of the WEM:\nScale, rotate, and move a WEM\u0026rsquo;s visualization in a scene Modify a WEM in a scene Scale, Rotate, and Move a WEM in a Scene We are using a SoTransformerDragger module to apply transformations on the visualizations of a 3D WEM object via mouse interactions.\n","content":"Surface Example 3: Interactions with WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In these examples, we are showing two different possibilities to interact with the visualization of the WEM:\nScale, rotate, and move a WEM\u0026rsquo;s visualization in a scene Modify a WEM in a scene Scale, Rotate, and Move a WEM in a Scene We are using a SoTransformerDragger module to apply transformations on the visualizations of a 3D WEM object via mouse interactions.\nAdd a SoCube and a SoBackground module and connect both to a SoExaminerViewer. For a better understanding, you should also add a SoCoordinateSystem module and connect it to the viewer. Change the User Transform Mode to User Transform Instead Of Input and set User Scale to 2 for x, y, and z.\nInitial Network The SoExaminerViewer shows your cube and the world coordinate system. You can interact with the camera (rotate, zoom, and pan), the visualization of the cube itself does not change. It remains in the center of the coordinate system.\nInitial Cube Scaling, rotating, and translating the visualization of the cube can be done by using the module SoTransformerDragger.\nAdditionally, add a SoTransform module to your network. Add all modules except the SoCoordinateSystem to a SoSeparator, so that transformations are not applied to the coordinate system.\nSoTransformerDragger and SoTransform Draw parameter connections from Translation, Scale Factor, and Rotation of the SoTransformerDragger to the same fields of the SoTransform module.\nOpening your SoExaminerViewer now allows you to use handles of the SoTransformerDragger to scale, rotate, and move the visualization of the cube. The cube itself remains unchanged in memory, a matrix for translation is applied to the original 3D object\u0026rsquo;s visualization.\nYou can additionally interact with the camera as already done before.\nInfo:\u0026nbsp; You need to change the active tool on the right side of the SoExaminerViewer. Use the Pick Mode for applying transformations and the View Mode for adjusting the camera. Moved, Rotated, and Scaled Cube You can also try the other So*Dragger modules in MeVisLab for variations of the SoTransformerDragger.\n\u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Interactively Modify WEMs The big difference to the previously described scenario, where we modified the visualization of the WEM, is that this example modifies the WEM itself.\nWe are using the WEMBulgeEditor module to interactively modify the WEM via mouse interactions.\nAdd the modules WEMInitialize, SoWEMRenderer, and SoBackground to your workspace and connect them to a SoExaminerViewer as seen below. Select model Icosahedron for the WEMInitialize module.\nWEMLoad and SoWEMRenderer You can see the WEM and interact with it in the viewer (zoom, move, and rotate). In the case the object does not rotate around its center, trigger the field viewAll of the SoExaminerViewer.\nAdd a WEMBulgeEditor and a SoWEMBulgeEditor to your network and connect them as seen below.\nWEMBulgeEditor and SoWEMBulgeEditor Opening the viewer, you can still not edit the object.\nWe need a lookup table (LUT) to interact with the WEM. Add a WEMGenerateStatistics between the WEMInitialize and the WEMBulgeEditor. The module WEMGenerateStatistics generates node, edge, and face statistics of a WEM and stores the information in the WEM\u0026rsquo;s Primitive Value Lists.\nInfo:\u0026nbsp; More information about Primitive Value Lists (PVL) can be found in Surface Example 5. Check New node PVL and set New PVL Name to myPVL.\nWEMGenerateStatistics In the WEMBulgeEditor, set PVL Used as LUT Values to previously generated myPVL.\nWEMBulgeEditor PVL Add a SoLUTEditor and connect it to SoWEMRenderer. You also have to connect the WEMGenerateStatistics to the SoWEMRenderer. Set SoWEMRenderer Color Mode to Lut Values and select PVL Used as LUT Values to myPVL.\nFinal Network Open the panel of the SoLUTEditor. Configure New Range Min as -1 and New Range Max as 1 in Range tab. Apply the new range. Define the LUT as seen below in Editor tab.\nSoLUTEditor Now, your Primitive Value List is used to colorize the affected region for your tansformations. You can see the region by the color on hovering the mouse over the WEM.\nAffected region colored The size of the region can be changed via ALT and mouse wheel . Make sure that the Influence Radius in WEMBulgeEditor is larger than 0.\nInfo:\u0026nbsp; You need to change the active tool on the right side of the SoExaminerViewer. Use the Pick Mode for applying transformations and the View Mode for adjusting the camera. Modify WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. A much more complex example using medical images and allowing to modify in 3D and on 2D slices can be seen by opening the example network of the WEMBulgeEditor.\nWEMBulgeEditor Example Network Info:\u0026nbsp; For other interaction possibilities, you can play around with the example networks of the modules SoCSODrawOnSurface, SoVolumeCutting and WEMExtrude. Summary MeVisLab provides multiple options to interact with 3D surfaces. Modules of the So\\*Dragger family allow to scale, rotate, and translate a WEM. You can always use a SoCoordinateSystem to see the current world coordinates. The WEMBulgeEditor allows you to interactively modify a WEM via mouse. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/surfaces/surfaceexample4/","title":"Surface Example 4: Interactively Moving WEM","summary":"Surface Example 4: Interactively Moving WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to interactively move WEMs using SoDragger modules inside a viewer.\nDevelop Your Network Interactively Translating Objects in 3D Using SoDragger Modules Add and connect the following modules as shown. On the panel of the module WEMInitialize, select the Model Octasphere. After that, open the viewer SoExaminerViewer and make sure to select the Interaction Mode. Now, you are able to click on the presented Octasphere and move it alongside one axis. The following modules are involved in the interactions:\n","content":"Surface Example 4: Interactively Moving WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to interactively move WEMs using SoDragger modules inside a viewer.\nDevelop Your Network Interactively Translating Objects in 3D Using SoDragger Modules Add and connect the following modules as shown. On the panel of the module WEMInitialize, select the Model Octasphere. After that, open the viewer SoExaminerViewer and make sure to select the Interaction Mode. Now, you are able to click on the presented Octasphere and move it alongside one axis. The following modules are involved in the interactions:\nSoMITranslate1Dragger: This module allows interactive translation of the object alongside one axis. You can select the axis for translation in the panel of the module. SoMIDraggerContainer: This module is responsible for actually changing the translation values of the object. Interactive dragging of objects Interactively Translating a WEM Alongside Three Axes We like to be able to interactively move a WEM alongside all three axes. In MeVisLab, there is the module SoMITranslate2Dragger, which allows translations alongside two axes, but there is no module that allows object translation in all three directions. Therefore, we will create a network that solves this task. The next steps will show you how you create three planes intersecting the objects. Dragging one plane will drag the object alongside one axis. In addition, these planes will only be visible when hovering over them.\nCreation of Planes Intersecting an Object We start creating a plane that will allow dragging in x-direction. In order to do that, modify your network as shown: Add the modules WEMModify and SoBackground, and connect the module SoCube to the dragger modules. You can select the translation direction in the panel of SoMITranslate1Dragger.\nInteractive dragging of objects We will modify the cube to be able to use it as a dragger plane. In order to do this, open the panel of SoCube and reduce the Width to be 0. This sets a plane in y- and z-direction.\nInteractive dragging of objects We want to move the object when dragging the plane. Thus, we need to modify the translation of our object when moving the plane. Open the panels of the modules WEMModify and SoMIDraggerContainer and draw a parameter connection from one Translation vector to the other.\nInteractive dragging of objects As a next step, we want to adapt the size of the plane to the size of the object we have. Add the modules WEMInfo and DecomposeVector3 to your workspace and open their panels. The module WEMInfo presents information about the given WEM, for example, its position and size. The module DecomposeVector3 splits a 3D vector into its components. Now, draw a parameter connection from Size of WEMInfo to the vector in DecomposeVector3. As a next step, open the panel of SoCube and draw parameter connections from the fields Y and Z of DecomposeVector3 to Height and Depth of SoCube. Now, the size of the plane adapts to the size of the object.\nInteractive dragging of objects The result can be seen in the next image. You can now select the plane in the Interaction Mode of the module SoExaminerViewer and move the plane together with the object alongside the x-axis.\nInteractive dragging of objects Modifying the Appearance of the Plane For changing the visualization of the dragger plane, add the modules SoGroup, SoSwitch, and SoMaterial to your network and connect them as shown. In addition, group all modules together that are responsible for the translation in the x-direction.\nInteractive dragging of objects We want to switch the visualization of the plane dependent on the mouse position in the viewer. In other words, when hovering over the plane, the plane should be visible, when the mouse is in another position and the possibility to drag the object is not given, the plane should be invisible. We use the module SoMaterial to edit the appearance of the plane. Open the panel of the module SoMITranslate1Dragger. The box of the field Highlighted is ticked when the mouse hovers over the plane. Thus, we can use the field\u0026rsquo;s status to switch between different presentations of the plane. In order to do this, open the panel of SoSwitch and draw a parameter connection from Highlighted of SoMITranslate1Dragger to Which Child of SoSwitch.\nInteractive dragging of objects Open the panels of the modules SoMaterial. Change the Transparency of the first SoMaterial module to make the plane invisible when not hovering over the plane. Furthermore, we changed the Diffuse Color of the module SoMaterial1 to red, so that the plane appears in red when hovering over it.\nInteractive dragging of objects When hovering over the plane, the plane becomes visible and the option to move the object alongside the x-axis is given. When you do not hover over the plane, the plane is invisible.\nInteractive dragging of objects Interactive Object Translation in Three Dimensions We do not only want to move the object in one direction, we like to be able to do interactive object translations in all three dimensions. For this, copy the modules responsible for the translation in one direction and change the properties to enable translations in other directions.\nWe need to change the size of SoCube1 and SoCube2 to form planes that cover surfaces in x- and z-, as well as x- and y-directions. To do that, draw the respective parameter connections from DecomposeVector3 to the fields of the modules SoCube. In addition, we need to adapt the field Direction in the panels of the modules SoMITranslate1Dragger.\nInteractive dragging of objects Change width, height, and depth of the three cubes, so that each of them represents one plane. The values need to be set to (0, 2, 2), (2, 0, 2), and (2, 2, 0).\nAs a next step, we like to make sure that all planes always intersect the object, even though the object is moved. To do this, we need to synchronize the field Translation of all SoMIDraggerContainer modules and the module WEMModify. Draw parameter connections from one Translation field to the next, as shown below.\nInteractive dragging of objects We like to close the loop, so that a change in one field Translation causes a change in all the other Translation fields. To do this, we need to include the module SyncVector. The module SyncVector avoids an infinite processing loop causing a permanent update of all fields Translation.\nAdd the module SyncVector to your workspace and open its panel. Draw a parameter connection from the field Translation of the module SoMIDraggerContainer2 to Vector1 of SyncVector. The field Vector1 is automatically synchronized to the field Vector2. Now, connect the field Vector2 to the field Translate of the module WEMModify. Your synchronization network is now established.\nInteractive dragging of objects To enable transformations in all directions, we need to connect the modules SoMIDraggerContainer to the viewer. First, connect the modules to SoGroup, after that connect SoGroup to SoExaminerViewr.\nInteractive dragging of objects As a next step, we like to enlarge the planes to make them exceed the object. For that, add the module CalculateVectorFromVectors to your network. Open its panel and connect the field Size of WEMInfo to Vector 1. We like to enlarge the size by one, so we add the vector (1, 1, 1), by editing the field Vector 2. Now, connect the Result to the field V of the module DecomposeVector3.\nInteractive dragging of objects At last, we can condense all the modules enabling the transformation into one local macro module. For that, group all the modules together and convert the group into a macro module as shown in Chapter I: Basic Mechanisms.\nInteractive dragging of objects The result can be seen in the next image. This module can now be used for interactive 3D transformations for all kinds of WEMs.\nInteractive dragging of objects Summary A family of SoDragger modules is available that can be used to interactively modify Open Inventor objects. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/surfaces/surfaceexample5/","title":"Surface Example 5: WEM - Primitive Value Lists","summary":"Surface Example 5: WEM - Primitive Value Lists \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction WEMs do not only contain the coordinates of nodes and surfaces, they can also contain additional information. That information is stored in so-called Primitive Value Lists (PVLs). Every node, every surface, and every edge can contain such a list. In these lists, you can, for example, store the color of the node or specific patient information. This information can be used for visualization or for further statistical analysis.\n","content":"Surface Example 5: WEM - Primitive Value Lists \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction WEMs do not only contain the coordinates of nodes and surfaces, they can also contain additional information. That information is stored in so-called Primitive Value Lists (PVLs). Every node, every surface, and every edge can contain such a list. In these lists, you can, for example, store the color of the node or specific patient information. This information can be used for visualization or for further statistical analysis.\nIn this example we like to use PVLs to color-code and visualize the distance between two WEMs.\nSteps to Do Develop Your Network We start our network by initializing two WEMs using WEMInitialize. We chose an Octasphere and a resized Cube. Use the modules SoWEMRenderer, SoExaminerViewer, and SoBackground to visualize the WEMs.\nWEMInitialize Subdividing WEM Edges As a next step, add and connect two modules WEMSubdivide to further divide edges and surfaces. With this step we increase the node density to have an accurate distance measurement.\nWEMSubdivide The difference when selecting different maximum edge lengths can be seen in the following images.\nEdgeLength1 EdgeLength01 Distances Between WEMs are Stored in PVLs Now, add the modules WEMSurfaceDistance and WEMInfo to your workspace and connect them as shown. WEMSurfaceDistance calculates the minimum distance between the nodes of both WEM. The distances are stored in the nodes\u0026rsquo; PVLs as LUT values.\nDistances between surfaces Open the panels of the modules WEMSurfaceDistance and WEMInfo. In the panel of WEMInfo select the tab Statistics. You can see the statistics of the stored PVLs. The Minimum Value and the Maximum Value are similar to the calculated Min Dist. and Max. Dist. of WEMSurfaceDistance.\nWEM information Color-coding the Distance Between WEMs What can we do with this information? We can use the calculated distances, stored in LUT values, to color-code the distance between the WEMs. For this, add and connect the module SoLUTEditor. Each LUT value from the PVLs will in the next step be translated into a color. But first, open the panel of SoWEMRenderer to select the Color Mode LUT Values. Now, the module SoLUTEditor defines the coloring of the WEM.\nSoWEMRenderer To translate the LUT values from the PVLs into color, open the panel of SoLUTEditor and select the tab Range. We need to define the value range we like to work with. As the distance and thus the PVL value is expected to be 0 when the surfaces of both WEMs meet, we set the New Range Min to 0. As the size of the WEMs does not exceed 3, we set the New Range Max to 3. After that, press Apply New Range.\nSoLUTEditor Our goal is to colorize faces of the Octasphere in red if they are close to or even intersect the cubic WEM. And we like to colorize faces of the Octasphere in green if these faces are far away from the cubic WEM.\nOpen the tab Editor of the panel of SoLUTEditor. This tab allows to interactively select a color for each PVL value. Select the color point on the left side. Its Position value is supposed to be 0, so we like to select the Color red in order to color-code small distances between the WEMs in red. In addition to that, increase the Opacity of this color point. Next, select the right color point. Its Position is supposed to be 3 and thus equals the value of the field New Range Max. As these color points colorize large distances between WEMs, select the Color green. You can add new color points by clicking on the colorized bar in the panel. Select, for example, the Color yellow for a color point in the middle. Select and shift the color points to get the desired visualization.\nChanging the LUT Add the module WEMModify to your workspace and connect the module as shown. If you now shift the WEM using WEMModify, you can see that the colorization adapts.\nWEMModify Interactive Shift of WEMs As a next step, we like to implement the interactive shift of the WEM. Add the modules SoTranslateDragger1 and SyncVector. Connect all translation vectors: Draw connections from the field Translate of SoTranslateDragger1 to Vector1 of SyncVector, from Vector2 of SyncVector to Translate of WEMModify, and at last from Translate of WEMModify to Translate of SoTranslateDragger1.\nYou can now interactively drag the WEM inside the viewer.\nDragging the WEM At last, exchange the WEMInitialize module showing the cube with WEMLoad and load venus.off. You can decrease the Face Alpha in the panel of SoWEMRenderer1 to make that WEM transparent.\nWEM transparency The result can be seen in the next image.\nYour final result Summary Additional information of WEMs can be stored in Primitive Value Lists (PVL), attached to nodes, edges, or faces. The module WEMSurfaceDistance stores the minimum distance between nodes of different WEMs in PVLs as LUT values. PVLs containing LUT values can be used to color-code additional information on the WEM surface. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM","PVM","Primitive Value Lists","LUT"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/testing/","title":"Chapter VI: Testing","summary":"MeVisLab Tutorial Chapter VI Testing, Profiling, and Debugging in MeVisLab The MeVisLab Integrated Development Environment (IDE) provides tools to write automated tests in Python, to profile your network performance, and to debug your Python code. All of these funtionalities will be addressed in this chapter.\nTesting The MeVisLab TestCenter is the starting point of your tests. Select [ File \u0026rarr; Run TestCaseManager ] to open the user interface of the TestCaseManager.\n","content":"MeVisLab Tutorial Chapter VI Testing, Profiling, and Debugging in MeVisLab The MeVisLab Integrated Development Environment (IDE) provides tools to write automated tests in Python, to profile your network performance, and to debug your Python code. All of these funtionalities will be addressed in this chapter.\nTesting The MeVisLab TestCenter is the starting point of your tests. Select [ File \u0026rarr; Run TestCaseManager ] to open the user interface of the TestCaseManager.\nMeVisLab TestCaseManager Test Selection The Test Selection allows you to define a selection of test cases to be executed. The list can be configured by defining a filter, manually selecting the packages (see Example 2.1: Package Creation) to be scanned for test cases. All test cases found in the selected packages are shown.\nOn the right side of the Test Selection tab, you can see a list of functions in the test case. Each list entry is related to a Python function. You can select the functions to be executed. If your test case contains a network, you can open the .mlab file or edit the Python file in MATE.\nTest Reports The results of your tests are shown as a report after execution.\nTest Creation You can create your own test cases here. A package is necessary to store your network and Python file.\nConfiguration Here you can configure details of your tests and reports. The filepath to the directory of your MeVisLab installation is configured automatically.\nCheck:\u0026nbsp; If you have multiple versions installed, make sure to check and, if needed, alter the automatically configured filepath. Profiling Profiling allows you to get detailed information on the behavior of your modules and networks. You can add the Profiling view via [ View \u0026rarr; Views \u0026rarr; Profiling ]. The Profiling will be displayed in the Views area of the MeVisLab IDE.\nMeVisLab Profiling With enabled profiling, your currently opened network will be inspected and the CPU and memory usage and many more details of each module and function are logged.\nDebugging Debugging can be enabled whenever the integrated text editor MATE is opened. Having a Python file opened, you can enable debugging via [ Debug \u0026rarr; Enable Debugging ]. You can define break points in Python, add variables to your watchlist, and walk through your break points just like in other editors and debuggers.\nMeVisLab Debugging ","tags":["Beginner","Tutorial","Testing","Python","Automated Tests","Profiling","Debugging"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/markerobjects/","title":"Marker Objects","summary":"Markers in MeVisLab In MeVisLab you can attach markers to images and other data objects. In this example you will see how to create, process, and use markers.\nCreation and Rendering To create markers, you can use a marker editor, for example, the SoView2DMarkerEditor. Connect this editor to a viewer as shown below. Now you can interactively create new markers. Connect the module XMarkerListContainer to your marker editor to store markers in a list.\n","content":"Markers in MeVisLab In MeVisLab you can attach markers to images and other data objects. In this example you will see how to create, process, and use markers.\nCreation and Rendering To create markers, you can use a marker editor, for example, the SoView2DMarkerEditor. Connect this editor to a viewer as shown below. Now you can interactively create new markers. Connect the module XMarkerListContainer to your marker editor to store markers in a list.\nCreate Markers Using the StylePalette module, you can define a style for your markers. In order to set different styles for different markers, change the field Color Mode in the panel of SoView2DMarkerEditor to Index.\nStyle of Markers With the help of the module So3DMarkerRenderer, markers of an XMarkerList can be rendered in 3D.\nRendering of Markers Working With Markers Info:\u0026nbsp; It is possible to convert other data objects into markers and also to convert markers into other data objects. It is, for example, possible to set markers by using the MaskToMarkers module and later on generate a surface object from a list of markers using the MaskToSurface module. Marker conversion can also be done by various other modules, listed in [/Modules/Geometry/Markers]. Learn how to convert markers by building the following network. Press the Reload buttons of the modules MaskToMarkers and MarkersToSurface to enable the conversion. Now you can see both the markers and the created surface in the module SoExaminerViewer. Use the toggle options of the modules SoToggle and SoWEMRenderer to enable or disable the visualization of markers and surface.\nInfo:\u0026nbsp; Make sure to set Lower Threshold of the MaskToMarkers module to 1000, so that the 3D object is rendered correctly. Convert Markers Exercise Get the HU value of the image at your markers location.\nSummary Markers are single point objects located at a defined location in your image. Markers can be converted to be rendered in 3D. ","tags":["Beginner","Tutorial","Data Objects","2D","Marker"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/markers/markerexample1/","title":"Example 1: Distance Between Markers","summary":"Example 1: Calculating the Distance Between Markers \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will measure the distance between one position in an image to a list of markers.\nSteps to Do Develop Your Network Add the following modules and connect them as shown.\nWe changed the names of the modules SoView2DMarkerEditor and XMarkerListContainer, to distinguish these modules from two similar modules we will add later on. Open the panel of SoView2DMarkerEditor and select the tab Drawing. Now choose the Color red.\n","content":"Example 1: Calculating the Distance Between Markers \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will measure the distance between one position in an image to a list of markers.\nSteps to Do Develop Your Network Add the following modules and connect them as shown.\nWe changed the names of the modules SoView2DMarkerEditor and XMarkerListContainer, to distinguish these modules from two similar modules we will add later on. Open the panel of SoView2DMarkerEditor and select the tab Drawing. Now choose the Color red.\nMarker Color As a next step, add two more modules: SoView2DMarkerEditor and XMarkerListContainer.\nChange their names and the marker color to green and connect them as shown. We also like to change the mouse button you need to press in order to create a marker. This allows to place both types of markers, the red ones and the green ones. In order to do this, open the panel of GreenMarker. Under Buttons, you can adjust which button needs to be pressed in order to place a marker. Select the Button2 (the middle button of your mouse ) instead of Button1 (the left mouse button ).\nIn addition to that, we like to allow only one green marker to be present. If we place a new marker, the old marker should vanish. For this, select the Max Size to be one and select Overflow Mode: Remove All.\nMarker Editor Settings Create Markers of Different Type Now, we can place as many red markers as we like, using the left mouse button and only one green marker using the middle mouse button .\nTwo Types of Markers Calculate the Distance Between Markers We like to calculate the minimum and maximum distance of the green marker to all red markers. In order to do this, add the module DistanceFromXMarkerList and connect it to RedMarkerList. Open the panels of DistanceFromXMarkerList and GreenMarkerList. Now, draw a parameter connection from the coordinate of the green marker, which is stored in the field Current Item -\u0026gt; Position in the panel of GreenMarkerList, to the field Position of DistanceFromXMarkerList. You can now press Calculate Distance in the panel of DistanceFromXMatkerList to see the result, meaning the distance of the green marker to all red markers in the panel of DistanceFromXMarkerList.\nModule DistanceFromXMarkerList Automation of Distance Calculation To automatically update the calculation when placing a new marker, we need to tell the module DistanceFromXMarkerList when a new green marker is placed. Open the panels of DistanceFromXMarkerList and GreenMarker and draw a parameter connection from the field Currently busy in the panel of GreenMarker to Calculate Distance in the panel of DistanceFromXMarkerList. If you now place a new green marker, the distance from the new green marker to all red markers is calculated automatically. Calculation of Distance between Markers Additional Information:\u0026nbsp; Another example for using a SoView2DMarkerEditor module can be found at Image Processing - Example 3: Region Growing Summary Markers can be created using SoView2DMarkerEditor. Markers can be stored and managed using XMarkerListContainer. The distance between markers can be calculated using DistanceFromXMarkerList. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","3D","Marker"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/testing/testingexample1/","title":"Example 1: Writing a Simple Test Case in MeVisLab","summary":"Example 1: Writing a Simple Test Case in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you will learn how to write an automated test for a simple network using the DicomImport, MinMaxScan, and View3D modules. Afterward, you will be able to write test cases for any other module and network yourself.\nSteps to Do Creating the Network to be Used for Testing Add the following modules to your workspace and connect them as seen below:\n","content":"Example 1: Writing a Simple Test Case in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you will learn how to write an automated test for a simple network using the DicomImport, MinMaxScan, and View3D modules. Afterward, you will be able to write test cases for any other module and network yourself.\nSteps to Do Creating the Network to be Used for Testing Add the following modules to your workspace and connect them as seen below:\nTestcase network Save your network as NetworkTestCase.mlab.\nTest Creation Open the MeVisLab TestCaseManager via menu [ File \u0026rarr; Run TestCaseManager ]. The following window will appear.\nTestCaseManager window Change to the Test Creation tab and enter details of your test case as seen below. Make sure to have a package available already.\nInfo:\u0026nbsp; Details on package creation can be found in Example 2.1: Package creation. Select your saved NetworkTestCase.mlab file.\nTest Creation window Click Create. The MeVisLab text editor MATE will automatically open and display the Python file of your test. Add the below listed code to the Python file.\nNetworkTestCase.py\nfrom mevis import * from TestSupport import Base, Fields, Logging from TestSupport.Macros import * filePath=\u0026#34;C:/Program Files/MeVisLab3.6.0/Packages/MeVisLab/Resources/DemoData/BrainT1Dicom\u0026#34; def OpenFiles(): ctx.field(\u0026#34;DicomImport.inputMode\u0026#34;).value = \u0026#34;Directory\u0026#34; ctx.field(\u0026#34;DicomImport.source\u0026#34;).value = filePath ctx.field(\u0026#34;DicomImport.triggerImport\u0026#34;).touch() MLAB.processEvents() while not ctx.field(\u0026#34;DicomImport.ready\u0026#34;).value: MLAB.sleep(1) Base.ignoreWarningAndError(MLAB.processEvents) ctx.field(\u0026#34;DicomImport.selectNextItem\u0026#34;).touch() MLAB.log(\u0026#34;Files imported from: \u0026#34;+ctx.field(\u0026#34;DicomImport.source\u0026#34;).value) def TEST_DicomImport(): expectedValue=1.0 OpenFiles() currentValue=ctx.field(\u0026#34;DicomImport.progress\u0026#34;).value ASSERT_FLOAT_EQ(expectedValue,currentValue) The filePath variable defines the absolute path to the DICOM files that will be given to source field of the DicomImport module in the second step of the OpenFiles function.\nThe OpenFiles function first defines the DicomImport field inputMode to be a Directory. If you want to open single files, set this field\u0026rsquo;s value to Files. Then, the source field is set to your previously defined filePath. After clicking triggerImport, the DicomImport module needs some time to load all images in the directory and process the DICOM tree. We have to wait until the field ready is True. While the import is not ready yet, we wait for 1 millisecond at a time and check again. MLAB.processEvents() lets MeVisLab continue execution while waiting for the DicomImport to be ready.\nWhen calling the function TEST_DicomImport, an expected value of 1.0 is defined. Then, the DICOM files are opened.\nCheck:\u0026nbsp; Call Base.ignoreWarningAndError(MLAB.processEvents) instead of MLAB.processEvents() if you receive error messages regarding invalid DICOM tags. When ready is true, the test touches the selectNextItem trigger, so that the first images of the patient are selected and shown. The source directory will be written on the console as an additional log message for informative purposes.\nThe value of our DicomImports progress field is saved as the currentValue variable and compared to the expectedValue variable by calling ASSERT_FLOAT_EQ(expectedValue,currentValue) to determine if the DICOM import has finished (currentValue and expectedValue are equal) or not.\nRun Your Test Case Open the TestCase Manager und run your test by selecting your test case and clicking on the Play button in the bottom right corner.\nRun Test Case After execution, the ReportViewer will open automatically displaying your test\u0026rsquo;s results.\nReportViewer Writing a Test for Global Macro Modules Please observe that field access through Python scripting works differently for global macros. Instead of accessing a field directly by calling their respective module, the module itself needs to be accessed as part of the global macro first.\nNetworkTestCase.py\n... # Testing a network file ctx.field(\u0026#34;DicomImport.inputMode\u0026#34;).value = \u0026#34;Directory\u0026#34; # Testing a macro module ctx.field(\u0026#34;\u0026lt;MACRO_MODULE_NAME\u0026gt;.DicomImport.inputMode\u0026#34;).value = \u0026#34;Directory\u0026#34; Imagine unpeeled nuts in a bag as a concept - the field as a nut, their module as their nutshell, and the bag as the global macro.\nInfo:\u0026nbsp; Example 2.2: Global macro modules provides additional info on global macro modules and their creation. Exercise Create a global macro module and implement the following test objectives for both (network and macro module):\nCheck if the file exists. Check if the max value of file is greater than zero. Check if the View3D input and DicomImport output have the same data. Summary MeVisLab provides a TestCenter for writing automated tests in Python. Tests can be executed on networks and macro modules. The test results are shown in a ReportViewer. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Testing","Python","Automated Tests"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/testing/testingexample2/","title":"Example 2: Profiling in MeVisLab","summary":"Example 2: Profiling in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we are using the MeVisLab Profiler to inspect the memory and CPU consumption of the modules in an example network.\nSteps to Do Creating the Network to be Used for Profiling You can open any network you like, here we are using the example network of the module MinMaxScan for profiling. Add the module MinMaxScan to your workspace, open the example network via right-click and select [ Help \u0026rarr; Show Example Network ].\n","content":"Example 2: Profiling in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we are using the MeVisLab Profiler to inspect the memory and CPU consumption of the modules in an example network.\nSteps to Do Creating the Network to be Used for Profiling You can open any network you like, here we are using the example network of the module MinMaxScan for profiling. Add the module MinMaxScan to your workspace, open the example network via right-click and select [ Help \u0026rarr; Show Example Network ].\nMinMaxScan Example Network Enable Profiling Next, enable the MeVisLab Profiler via menu item [ View \u0026rarr; Views \u0026rarr; Profiling ]. The Profiler is opened in your views area but can be detached and dragged over the workspace holding the left mouse button .\nMeVisLab Profiling Enable profiling by checking Enable in the top left corner of the Profiling window.\nInspect Your Network Now open the View2D module\u0026rsquo;s panel via double-click and scroll through the slices. Inspect the Profiler.\nMeVisLab Profiling Network The Profiler shows detailed information about each module in your network.\nInfo:\u0026nbsp; Macro modules are not profiled on default. You can check the Show macros option in order to have View2D and LocalImage profiled. Also, filtering by module name is handy when you are working with larger networks. Field values and their changes for all modules in your network can be inspected in the Fields tab:\nMeVisLab Profiling Fields In addition to the Profiler window, your modules also provide a tiny bar indicating their current memory and time consumption.\nMeVisLab Profiling Module Info:\u0026nbsp; More information about profiling in MeVisLab can be found here Attention:\u0026nbsp; You need to uncheck the Enable checkbox in the top left corner to stop profiling. Closing the window will not automatically end the profiling. Summary Profiling allows you to inspect the behavior of modules and networks including CPU and memory consumption. Field value changes can be observed in the Profiler\u0026rsquo;s Fields tab. ","tags":["Beginner","Tutorial","Profiling"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/testing/testingexample3/","title":"Example 3: Iterative Tests in MeVisLab With Screenshots","summary":"Example 3: Iterative Tests in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you are writing an iterative test. Iterative test functions run a function for every specified input. They return a tuple consisting of the function object called and the inputs iterated over. The iterative test functions are useful if the same function should be applied to different input data. These could be input values, names of input images, etc.\n","content":"Example 3: Iterative Tests in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you are writing an iterative test. Iterative test functions run a function for every specified input. They return a tuple consisting of the function object called and the inputs iterated over. The iterative test functions are useful if the same function should be applied to different input data. These could be input values, names of input images, etc.\nSteps to Do Creating the Network to be Used for Testing Add a LocalImage and a DicomTagViewer module to your workspace and connect them.\nExample Network Test Case Creation Open the panel of the DicomTagViewer and set Tag Name to WindowCenter. The value of the DICOM tag from the current input image is automatically set as value.\nSave the network.\nStart MeVisLab TestCaseManager and create a new test case called IterativeTestCase as seen in Example 1: Writing a simple testcase in MeVisLab.\nDicomTagViewer Defining the Test Data In TestCaseManager open the test case Python file via Edit File.\nAdd a list for test data to be used as input and a prefix for the path of the test data as seen below.\nIterativeTestCase.py\nfrom mevis import * from TestSupport import Base, Fields, ScreenShot, Logging from TestSupport.Macros import * patientPathPrefix = \u0026#34;$(DemoDataPath)/BrainMultiModal/\u0026#34; testData = { \u0026#34;ProbandT1\u0026#34;:(\u0026#34;ProbandT1.dcm\u0026#34;, \u0026#34;439.9624938965\u0026#34;), \u0026#34;ProbandT2\u0026#34;:(\u0026#34;ProbandT2.dcm\u0026#34;, \u0026#34;234.91\u0026#34;)} The above list contains an identifier for the test case (ProbandT1/2), the file names, and a number value. The number value is the value of the DICOM tag (0028,1050) WindowCenter for each file.\nCreate Your Iterative Test Function Add the Python function to your .script file: IterativeTestCase.py\ndef ITERATIVETEST_TestWindowCenter(): return testData, testPatient This function defines that testPatient shall be called for each entry available in the defined list testData. Define the function testPatient: IterativeTestCase.py\ndef testPatient(path, windowCenter): ctx.field(\u0026#34;LocalImage.name\u0026#34;).value = patientPathPrefix + path tree = ctx.field(\u0026#34;LocalImage.outImage\u0026#34;).getDicomTree() importValue = str(tree.getTag(\u0026#34;WindowCenter\u0026#34;).value()) dicomValue = str(ctx.field(\u0026#34;DicomTagViewer.tagValue0\u0026#34;).value) ASSERT_EQ(windowCenter, importValue, \u0026#34;Checking expected WindowCenter value against DICOM tree value.\u0026#34;) ASSERT_EQ(windowCenter, dicomValue, \u0026#34;Checking expected WindowCenter value against DicomTagViewer value.\u0026#34;) Initially, the path and filename for the module LocalImage are set. The data is loaded automatically, because the module has the AutoLoad flag enabled by default. LocalImage Then, the DICOM tree of the loaded file is used to get the WindowCenter value (importValue). The previously defined value of the DicomTagViewer is set as dicomValue. The final test functions ASSERT_EQ evaluate if the given values are equal. Info:\u0026nbsp; You can use many other ASSERT* possibilities, just try using the MATE autocompletion and play around with them. Run Your Iterative Test Open MeVisLab TestCase Manager and select your package and test case. You will see two test functions on the right side.\nIterative Test The identifiers of your test functions are shown as defined in the list (ProbandT1/2). The TestWindowCenter now runs for each entry in the list and calls the function testPatient for each entry using the given values.\nAdding Screenshots to Your TestReport Now, extend your network by adding a View2D module and connect it with the LocalImage module. Add the following lines to the end of your function testPatient: IterativeTestCase.py\ndef testPatient(path, windowCenter): ... Fields.setValue(\u0026#34;View2D.startSlice\u0026#34;, 0) result = ScreenShot.createOffscreenScreenShot(\u0026#34;View2D.self\u0026#34;, \u0026#34;screentest.png\u0026#34;) Logging.showImage(\u0026#34;My screenshot\u0026#34;, result) Logging.showFile(\u0026#34;Link to screenshot file\u0026#34;, result) Your ReportViewer now shows a screenshot of the image in the View2D.\nScreenshot in ReportViewer Summary Iterative tests allow you to run the same test function on multiple input entries. It is possible to add screenshots to test cases. ","tags":["Advanced","Tutorial","Testing","Python","Automated Tests","Iterative Test","Screenshot"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/curves/","title":"Curves","summary":"Curves in MeVisLab Introduction Curves can be used in MeVisLab to print the results of a function as two-dimensional mathematical curves into a diagram.\nCurves in MeVisLab ","content":"Curves in MeVisLab Introduction Curves can be used in MeVisLab to print the results of a function as two-dimensional mathematical curves into a diagram.\nCurves in MeVisLab ","tags":["Beginner","Tutorial","Data Objects","2D","Curves"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/dataobjects/curves/curvesexample1/","title":"Example 1: Drawing Curves","summary":"Example 1: Drawing Curves \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will draw one or more curves into a diagram and define different styles for the curves.\nSteps to Do Develop Your Network A curve requires x- and y-coordinates to be printed. You can use the CurveCreator module as input for these coordinates. The SoDiagram2D draws the curves into a SoRenderArea. You can also define the style of the curves by using the StylePalette module.\n","content":"Example 1: Drawing Curves \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will draw one or more curves into a diagram and define different styles for the curves.\nSteps to Do Develop Your Network A curve requires x- and y-coordinates to be printed. You can use the CurveCreator module as input for these coordinates. The SoDiagram2D draws the curves into a SoRenderArea. You can also define the style of the curves by using the StylePalette module.\nAdd the modules to your workspace and connect them as seen below.\nExample Network Creating a Curve Click on the output of the CurveCreator module and open the Output Inspector.\nEmpty Output Inspector Double-click on the CurveCreator module and open the Panel.\nCurveCreator Module You can see a large input field Curve Table. Here you can enter the x- and y-values of your curve. The values of the first column will become the x-values and the 2nd any further column will become the y-series. Comment lines start with a \u0026lsquo;#\u0026rsquo; character.\nEnter the following into the Curve Table: Curve Table\n# My first curve 0 0 1 1 2 2 3 3 4 4 5 5 10 10 50 50 Now, your Output Inspector shows a yellow line through the previously entered coordinates. Exactly the same curve is shown in the SoRenderArea.\nSoRenderArea Creating Multiple Curves Now, update the Curve Table, so that you are using three columns and click Update : Curve Table\n# My first curves 0 0 0 1 1 2 2 2 4 3 3 6 4 4 8 5 5 10 10 10 20 50 50 100 You can see two curves. The second and third columns are printed as separate curves. Both appear yellow. After checking Split columns into data sets, you will see one yellow and one red curve.\nbefore_split after_split If the flag Split columns into data sets is set to TRUE, then a table with more than two columns is split into different CurveData objects. This gives the user the possibility to assign a different style and title for each series.\nTitles and Styles Let\u0026rsquo;s do this. Open the panel of the SoDiagram2D module and check Draw legend. Enter \u0026ldquo;Curve1 Curve2\u0026rdquo; into the Title(s) text box of the CurveCreator module and click Update .\nSoRenderArea with Legend You can also define a different location of the legend and set font sizes.\nNow, open the panel of the StylePalette module.\nStylePalette The StylePalette module allows you to define twelve different styles for curves. Initially, without manual changes, the styles are applied one after the other. The first curve gets style 1, the second curve style 2, and so on.\nOpen the panel of your CurveCreator module again and define Curve Style(s) as \u0026ldquo;3 6\u0026rdquo;. Update your curves.\nStylePalette applied You now applied the style three for your first curve and style six for the second. This is how you can create twelve different curves with unique appearance.\nUsing Multiple Tables for Curve Generation In addition to adding multiple columns for different y-coordinates, you can also define multiple tables as input, so that you can also have different x-coordinates for multiple curves.\nUpdate the Curve Table as defined below and click Update : Curve Table\n# My first curves 0 0 0 1 1 2 2 2 4 3 3 6 4 4 8 5 5 10 10 10 20 50 50 100 --- # My third curve 0 0 1 1 2 4 3 9 4 16 5 25 6 36 7 49 8 64 9 81 10 100 Also add another title to your curves and define a third style.\nMultiple tables as input Additional Information:\u0026nbsp; For more complex visualizations, you can also use Matplotlib. See examples at Third-party - Matplotlib. Summary Curves can be created to draw two-dimensional diagrams. The StylePalette module allows you to define the appearance of a curve. Details of the different curves can be visualized by using the SoDiagram2D module. Additional Information:\u0026nbsp; The attached example network shows the curves after clicking Update on CurveCreator module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Curves"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/","title":"Chapter VII: Application Development","summary":"MeVisLab Tutorial Chapter VII Summary This chapter will summarize all previous chapters and you will develop an entire application in MeVisLab. The complete workflow from developing a prototype to delivering your final application to your customer is explained step-by-step.\nPrototype to Product Licensing:\u0026nbsp; Some of the features described here will require a separate license. Building an installable executable requires the MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK, so that you can generate an installer of your developed macro module.\n","content":"MeVisLab Tutorial Chapter VII Summary This chapter will summarize all previous chapters and you will develop an entire application in MeVisLab. The complete workflow from developing a prototype to delivering your final application to your customer is explained step-by-step.\nPrototype to Product Licensing:\u0026nbsp; Some of the features described here will require a separate license. Building an installable executable requires the MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK, so that you can generate an installer of your developed macro module.\nFree evaluation licenses of the MeVisLab ApplicationBuilder, time-limited to three months, can be requested at sales(at)mevislab.de.\nPrototype Step 1: Develop your network In the first step, you are developing an application based on the following requirements:\nRequirement 1: The application shall be able to load DICOM data Requirement 2: The application shall provide a 2D and a 3D viewer Requirement 3: The 2D viewer shall display the loaded images Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a region growing algorithm Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the region growing algorithm Requirement 4.2: It shall be possible to define a threshold for the region growing algorithm Requirement 5: The 2D viewer shall display the segmentation results as a semitransparent overlay Requirement 5.1: It shall be possible to define the color of the overlay Requirement 6: The 3D viewer shall visualize the loaded data in a three-dimensional volume rendering Requirement 7: The 3D viewer shall additionally show the segmentation result as a three-dimensional mesh Requirement 8: The total volume of the segmented area shall be calculated and shown (in ml) Requirement 9: It shall be possible to toggle the visible 3D objects Requirement 9.1: Original data Requirement 9.2: Segmentation results Requirement 9.3: All Step 2: Create Your Macro Module Your network will be encapsulated in a macro module for later application development. For details about macro modules, see Example 2.2: Global macro modules.\nStep 3: Develop a User Interface and Add Python Scripting Develop the UI and Python Scripts based on your requirements from Step 1. The resulting UI will look like the below mockup:\nUser Interface Design Review Step 4: Write Automated Tests for Your Macro Module Test your macro module in MeVisLab. Your requirements from Step 1 are translated into test cases written in Python. The fields accessible via Python as defined in Step 2 shall be used to test your application.\nStep 5: Create an Installable Executable Create a standalone application by using the MeVisLab ApplicationBuilder and install the application on another system.\nRefine Step 6: Update Your Network and Macro Module Integrate feedback from customers having installed your executable and adapt your test cases from Step 4.\nStep 7: Update Your Installable Executable Rebuild your executable and release a new version of your application.\nThe above loop can easily be repeated until your product completely fulfills your defined requirements.\n","tags":["Advanced","Tutorial"],"section":"tutorials"},{"date":"1673740800","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary1/","title":"Step 1: Prototyping - Develop Your Network","summary":"Step 1: Prototyping - Develop Your Network \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will develop a network that fulfills the requirements mentioned on the overview page. The network will be developed by reusing existing modules and defining basic field values.\nSteps to Do 2D Viewer The 2D viewer shall visualize the loaded images. In addition to that, it shall be possible to click into the image to trigger a region growing algorithm to segment parts of the loaded image based on a position and a threshold.\n","content":"Step 1: Prototyping - Develop Your Network \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will develop a network that fulfills the requirements mentioned on the overview page. The network will be developed by reusing existing modules and defining basic field values.\nSteps to Do 2D Viewer The 2D viewer shall visualize the loaded images. In addition to that, it shall be possible to click into the image to trigger a region growing algorithm to segment parts of the loaded image based on a position and a threshold.\nThe following requirements from the overview will be implemented:\nRequirement 1: The application shall be able to load DICOM data Requirement 3: The 2D viewer shall display the loaded images Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a region growing algorithm Requirement 4.1: It shall be possible to click into the image to set a marker position to start the region growing algorithm Requirement 4.2: It shall be possible to define a threshold for the region growing algorithm Requirement 5: The 2D viewer shall display the segmentation results as a semitransparent overlay Requirement 5.1: It shall be possible to define the color of the overlay Add a LocalImage and a View2D module to your workspace. You are now able to load an image and view the slices.\nLoading an image Region growing requires a SoView2DMarkerEditor, a SoView2DOverlay, and a RegionGrowing module. Add them to your network and connect them as seen below. Configure the RegionGrowing module to use a 3D-6-Neighborhood (x,y,z) relation and an automatic threshold value of 1.500. Also select Auto-Update.\nSet SoView2DMarkerEditor to allow only one marker by defining Max Size = 1 and Overflow Mode = Remove All. For our application we only want one marker to be set for defining the RegionGrowing.\nIf you now click into your loaded image via left mouse button , the RegionGrowing module segments all neighborhood voxels with a mean intensity value plus/minus the defined percentage value from your click position.\nThe overlay is shown in white.\nRegionGrowing via marker editor Open the SoView2DOverlay module, change Blend Mode to Blend, and select any color and Alpha Factor for your overlay. The applied changes are immediately visible. Overlay color and transparency The segmented results from the RegionGrowing module might contain gaps because of differences in the intensity value of neighboring voxels. You can close these gaps by adding a CloseGap module. Connect it to the RegionGrowing and the SoView2DOverlay module and configure Filter Mode as Binary Dilatation, Border Handling as Pad Dst Fill, and set KernelZ to 3.\nLastly, we want to calculate the volume of the segmented parts. Add a CalculateVolume module to the CloseGap module. The 2D viewer now provides the basic functionalities.\nYou can group the modules in your network for an improved overview by selecting [ Grouping \u0026rarr; Add to new Group... ]. Leave LocalImage out of the group and name it 2D Viewer. Your network should now look like this:\nGroup 2D Viewer 3D Viewer The 3D viewer shall visualize your loaded image in 3D and additionally provide the possibility to render your segmentation results. You will be able to decide for different views, displaying the image and the segmentation, only the image or only the segmentation. The volume (in ml) of your segmentation results shall be calculated.\nThe following requirements from overview will be implemented:\nRequirement 2: The application shall provide a 2D and a 3D viewer Requirement 6: The 3D viewer shall visualize the loaded data in a three-dimensional volume rendering Requirement 7: The 3D viewer shall additionally show the segmentation result as a three-dimensional mesh Requirement 8: The total volume of the segmented area shall be calculated and shown (in ml) Requirement 9: It shall be possible to toggle the visible 3D objects Requirement 9.1: Original data Requirement 9.2: Segmentation results Requirement 9.3: All Add a SoExaminerViewer, a SoWEMRenderer, and an IsoSurface module to your existing network and connect them to the LocalImage module. Configure the IsoSurface to use an IsoValue of 200, a Resolution of 1 and check Auto-Update and Auto-Apply.\n3D Viewer The result should be a three-dimensional rendering of your image.\nSoExaminerViewer Info:\u0026nbsp; If the rendering is not immediately applied, click Apply in your IsoSurface module. Define the field instanceName of your IsoSurface module as IsoSurfaceImage and add another IsoSurface module to your network. Set the instanceName to IsoSurfaceSegmentation and connect the module to the output of the CloseGap module from the image segmentation. Set IsoValue to 420, Resolution to 1, and check Auto-Update and Auto-Apply.\nSet instanceName of the SoWEMRenderer module to SoWEMRendererImage and add another SoWEMRenderer module. Set this instanceName to SoWEMRendererSegmentation and connect it to the IsoSurfaceSegmentation module. Selecting the output of the new SoWEMRenderer shows the segmented parts as a 3D object in the output inspector.\nSegmentation preview in output inspector Once again, we should group the modules used for 3D viewing and name the new group 3D Viewer.\nGrouped network We now want to allow the user to toggle the different 3D visualizations as defined by the requirements above. It shall be possible to show:\nOriginal data only Segmentation only Original data and segmentation combined Add a SoSwitch module to your network. Connect the switch to both of your SoWEMRenderer modules and to the SoExaminerViewer.\nSoSwitch The default input of the switch is None. Your 3D viewer remains black. Using the arrows on the SoSwitch allows you to toggle between the segmentation and the image. Input 0 shows the segmented brain, input 1 shows the head. You are now able to toggle between them. A view with both objects is still missing.\nExample1_Segmentation Example1_Image Add a SoGroup module and connect both SoWEMRenderer modules as input. The output needs to be connected to the right input of the SoSwitch module.\nSoGroup You can now also toggle input 2 of the switch showing both 3D objects. The only problem is: You cannot see the brain because it is located inside the head. Open the SoWEMRendererImage module panel and set faceAlphaValue to 0.5. The viewer now shows the head in a semitransparent manner, so that you can see the brain. Certain levels of opacity are difficult to render. Add a SoDepthPeelRenderer module and connect it to the semitransparent SoWEMRendererImage module. Set Layers of the renderer to 1.\nSoDepthPeelRenderer You have a 2D and a 3D viewer now. Let\u0026rsquo;s define the colors of the overlay to be reused for the 3D segmentation.\nParameter Connections for Visualization Open the panels of the SoView2DOverlay and the SoWEMRendererSegmentation module. Draw a parameter connection from SoView2DOverlay.baseColor to SoWEMRendererSegmentation.faceDiffuseColor.\nSynchronized segmentation colors Now, the 3D visualization uses the same color as the 2D overlay.\nSummary You built a network providing the basic functionalities of your application. Actions inside your application need to be executed by changing fields in your network or by manually touching a trigger. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Prototyping"],"section":"tutorials"},{"date":"1673827200","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary2/","title":"Step 2: Prototyping - Create a Macro Module","summary":"Step 2: Prototyping - Create a Macro Module \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we encapsulate the previously developed prototype network into a macro module for future application development and automated testing.\nSteps to Do Make sure to have your .mlab file from the previous tutorial available.\nPackage Creation Packages are described in detail in Example 2.1: Package creation. If you already have your own package, you can skip this part and continue creating a macro module.\n","content":"Step 2: Prototyping - Create a Macro Module \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we encapsulate the previously developed prototype network into a macro module for future application development and automated testing.\nSteps to Do Make sure to have your .mlab file from the previous tutorial available.\nPackage Creation Packages are described in detail in Example 2.1: Package creation. If you already have your own package, you can skip this part and continue creating a macro module.\nOpen the Project Wizard via [ File \u0026rarr; Run Project Wizard... ] and select New Package. Run the Wizard and enter details of your new package and click Create.\nPackage wizard MeVisLab reloads and you can start creating your macro module.\nCreate a Macro Module Open the Project Wizard via [ File \u0026rarr; Run Project Wizard... ] and select macro module. Run the Wizard and enter details of your new macro module.\nMacro module wizard Select the created package and click Next.\nMacro module wizard Select your .mlab file from Step 1 and check Add Python file. Click Next.\nMacro module wizard You do not have to define fields of your macro module now, we will do that later. Click Create. The file explorer opens showing the directory of your macro module. It should be the same directory you selected for your Package.\nDirectory Structure of a Macro Module The directory structure for a macro module is as follows:\nFrom Package Wizard: Package target directory is the root directory of the module The next directory is the package group and package name From macro module Wizard: The name of the macro module defines the directory containing all files of your module An additional directory Modules is created containing the following files: \u0026lt;MACRO_NAME\u0026gt;.def \u0026lt;MACRO_NAME\u0026gt;.mlab \u0026lt;MACRO_NAME\u0026gt;.py \u0026lt;MACRO_NAME\u0026gt;.script Directory Structure Definition (.def) File The initial .def file contains information you entered into the Wizard for the macro module.\n\u0026lt;MACRO_NAME\u0026gt;.def\nMacro module TutorialSummary { genre = \u0026#34;VisualizationMain\u0026#34; author = \u0026#34;MeVis Medical Solutions AG\u0026#34; comment = \u0026#34;Macro module for MeVisLab tutorials\u0026#34; keywords = \u0026#34;2D 3D RegionGrowing\u0026#34; seeAlso = \u0026#34;\u0026#34; externalDefinition = \u0026#34;$(LOCAL)/TutorialSummary.script\u0026#34; } An externalDefinition to a script file is also added (see below for the .script file).\nMeVisLab Network (.mlab) File The .mlab file is a copy of the .mlab file you developed in Step 1 and reused in the wizard. In the next chapters, this file will be used as internal network.\nPython (.py) File The initial .py file only contains the import of MeVisLab-specific objects and functions. In the future steps, we will add functionalities to our application in Python.\n\u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * Script (.script) File The script (.script) file defines fields accessible from outside the macro module, inputs and outputs, and allows you to develop a user interface for your prototype and your final application.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters {} } Commands { source = $(LOCAL)/TutorialSummary.py } The source also defines your Python file to be used when calling functions and events from the user interface.\nUsing Your Macro Module As you created a global macro module, you can search for it in the MeVisLab Module Search.\nModule Search We did not define inputs or outputs. You cannot connect your module to others. In addition to that, we did not develop a user interface. Double-clicking your module only opens the automatic panel showing the instanceName.\nAutomatic Panel Right-click on your module allows you to open the internal network as developed in Step 1.\nSummary Macro modules encapsulate an entire MeVisLab network including all modules. The internal network can be shown (and edited) via right-click [ Show Internal Network ] The Wizard already creates the necessary folder structure and generates files for user interface and Python development. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","Macro modules"],"section":"tutorials"},{"date":"1673913600","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary3/","title":"Step 3: Prototyping - User Interface and Python Scripting","summary":"Step 3: Prototyping - User Interface and Python Scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step, we will develop a user interface and add Python scripting to the macro module you created in Step 2.\nSteps to Do Develop the User Interface A mockup of the user interface you are going to develop is available here. The interface provides the possibility to load files and shows a 2D and a 3D viewer. In addition to that, some settings and information for our final application are available.\n","content":"Step 3: Prototyping - User Interface and Python Scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step, we will develop a user interface and add Python scripting to the macro module you created in Step 2.\nSteps to Do Develop the User Interface A mockup of the user interface you are going to develop is available here. The interface provides the possibility to load files and shows a 2D and a 3D viewer. In addition to that, some settings and information for our final application are available.\nSearch for your macro module and add it to your workspace. Right-click and select [ Related Files \u0026rarr; \u0026lt;MACRO_MODULE_NAME\u0026gt;.script ].\nThe MeVisLab text editor MATE opens showing the .script file of your module.\nLayout You can see that the interface is divided into four parts in vertical direction:\nSource or file/directory selection Viewing (2D and 3D) Settings Info Inside the vertical parts, the elements are placed next to each other horizontally.\nAdd a Window section to your .script file. Inside the Window, we need a Vertical for the four parts and a Box for each part. Name the boxes \u0026ldquo;Source\u0026rdquo;, \u0026ldquo;Viewing\u0026rdquo;, \u0026ldquo;Settings\u0026rdquo;, and \u0026ldquo;Info\u0026rdquo;. The layout inside each Box shall be Horizontal.\nIn addition to that, we define the minimal size of the Window as 400 x 300 pixels.\n\u0026lt;MACRO_NAME\u0026gt;.script\nWindow { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal } Box Viewing { layout = Horizontal } Box Settings { layout = Horizontal } Box Info { layout = Horizontal } } } You can preview your initial layout in MeVisLab by double-clicking your module .\nInitial Window Layout You can see the four vertical aligned parts as defined in the .script file. Now, we are going to add the content of the boxes.\nAdditional Info:\u0026nbsp; An overview over the existing layout elements in MeVisLab Definition Language (MDL) can be found here Adding the UI Elements Source The Source Box shall provide the possibility to select a file for loading into the viewers. You have many options to achieve that in MeVisLab and Python. The easiest way is to reuse the existing field of the LocalImage module in your internal network.\nAdd a field to the Parameters section of your .script file. Name the field openFile and set type to String and internalName to LocalImage.name.\nThen, add another field to your Box for the Source and use the field name from Parameters section, in this case openFile. Set browseButton = True and browseMode = open and save your script.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { Field openFile { type = String internalName = LocalImage.name } } } ... Window { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } } Box Viewing { layout = Horizontal } Box Settings { layout = Horizontal } Box Info { layout = Horizontal } } } Again, you can preview your user interface in MeVisLab directly. You can already select a file to open. The image is available at the output of the LocalImage module in your internal network but the viewers are missing in our interface.\nSource Box Viewing Add the two viewer modules to the Viewing section of your .script file and define their field as View2D.self and SoExaminerViewer.self. Set expandX = Yes and expandY = Yes for both viewing modules. We want them to resize in the case the size of the Window changes.\nSet the 2D viewer\u0026rsquo;s type to SoRenderArea and the 3D viewer\u0026rsquo;s type to SoExaminerViewer and inspect your new user interface in MeVisLab.\n\u0026lt;MACRO_NAME\u0026gt;.script\n... Box Viewing { layout = Horizontal Viewer View2D.self { expandX = True expandY = True type = SoRenderArea } Viewer SoExaminerViewer.self { expandX = True expandY = True type = SoExaminerViewer } } ... 2D and 3D Viewer The images selected in the Source section are shown in 2D and 3D. We simply reused the existing fields and viewers from your internal network and are already able to interact with the images. As the View2D of your internal network itself provides the possibility to accept markers and starts the RegionGrowing, this is also already possible and the segmentations are shown in 2D and 3D.\nSettings Let\u0026rsquo;s define the Settings section. Once again, we first define the necessary fields. For automated tests that we are going to develop later, it makes sense to make some of the fields of the internal network available from outside.\nThe following shall be accessible as Field for our macro module:\nFilename to be opened Color of the 2D overlay and 3D segmentation Transparency of the 3D image Threshold to be used for RegionGrowing Isovalue of the 3D surface to use for rendering Position of the marker to use for RegionGrowing Selection for 3D visualization (image, segmentation, or both) Trigger to reset the application to its initial state We already defined the filename as a field. Next we want to change the color of the overlay. Add another field to your Parameters section as selectOverlayColor. Define internalName = SoView2DOverlay.baseColor and type = Color. You may also define a title for the field, for example, Color.\nThe baseColor field of the SoView2DOverlay already has a parameter connection to the color of the SoWEMRendererSegmentation. This has been done in the internal network. The defined color is used for 2D and 3D automatically.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { ... Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } } } ... Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } } ... The next elements follow the same rules; therefore, the final script will be available at the end for completeness.\nIn order to set the transparency of the 3D image, we need another field reusing the SoWEMRendererImage.faceAlphaValue. Add a field imageAlpha to the Parameters section. Define internalName = SoWEMRendererImage.faceAlphaValue, type = Integer, min = 0, and max = 1.\nAdd the field to the Settings Box and set step = 0.1 and slider = True.\nFor the RegionGrowing threshold, add the field thresholdInterval to Parameters section and set type = Integer, min = 1, max = 100, and internalName = RegionGrowing.autoThresholdIntervalSizeInPercent.\nAdd the field to the Settings UI, and define step = 0.1 and slider = True.\nDefine a field isoValueImage in the Parameters section and set internalName = IsoSurfaceImage.isoValue, type = Integer, min = 1, and max = 1000.\nIn the Settings section of the UI, set step = 2 and slider = True.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { Field openFile { type = String internalName = LocalImage.name } Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } Field imageAlpha { internalName = SoWEMRendererImage.faceAlphaValue type = Integer min = 0 max = 1 } Field thresholdInterval { internalName = RegionGrowing.autoThresholdIntervalSizeInPercent type = Integer min = 0 max = 100 } Field isoValueImage { internalName = IsoSurfaceImage.isoValue type = Integer min = 0 max = 1000 } } } Commands { source = $(LOCAL)/TutorialSummary.py } Window { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } } Box Viewing { layout = Horizontal Viewer View2D.self { expandX = True expandY = True type = SoRenderArea } Viewer SoExaminerViewer.self { expandX = True expandY = True type = SoExaminerViewer } } Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } Field imageAlpha { step = 0.1 slider = True } Field thresholdInterval { step = 0.1 slider = True } Field isoValueImage { step = 2 slider = True } } Box Info { layout = Horizontal } } } Your user interface of the macro module should now look similar to this:\nUser Interface without Python Scripting For the next elements, we require Python scripting. Nevertheless, you are already able to use your application and perform the basic functionalities without writing any line of code.\nPython Scripting Python scripting is always necessary in the case you do not want to reuse an existing field for your user interface but implement functions to define what happens in the case of any event.\nEvents can be raised by the user (e.g., by clicking a button) or by the application itself (e.g., when the window is opened).\n3D Visualization Selection You will now add a selection possibility for the 3D viewer. This allows you to define the visibility of the 3D objects File, Segmented, or Both.\nAdd another field to your Parameters section. Define the field as selected3DView and set type = Enum and values =Segmented,File,Both.\nAdd a ComboBox to your Settings and use the field name defined above. Set alignX = Left and editable = False and open the Window of the macro module in MeVisLab.\nThe values of the field can be selected, but nothing happens in our viewers. We need to implement a FieldListener in Python that reacts on any value changes of the field selected3DView.\nOpen your script file and go to the Commands section. Add a FieldListener and reuse the name of our internal field selected3DView. Add a Command to the FieldListener calling a Python function viewSelectionChanged.\n\u0026lt;MACRO_NAME\u0026gt;.script\nCommands { source = $(LOCAL)/TutorialSummary.py FieldListener selected3DView { command = viewSelectionChanged } } Right-click the command select [ Create Python Function \u0026#39;viewSelectionChanged\u0026#39; ]. MATE automatically opens the Python file of your macro module and creates a function viewSelectionChanged.\n\u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def viewSelectionChanged(field): if field.value == \u0026#34;Segmented\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 0 if field.value == \u0026#34;File\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 1 if field.value == \u0026#34;Both\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 2 The function sets the SoSwitch to the child value depending on the selected field value from the ComboBox and you should now be able to switch the 3D rendering by selecting an entry in the user interface.\nSetting the Marker The marker for the RegionGrowing is defined by the clicked position as Vector3. Add another field markerPosition to the Parameters section and define type = Vector3.\nThen, add a trigger field applyMarker to your Parameters section. Set type = Trigger and title = Add.\n\u0026lt;MACRO_NAME\u0026gt;.script\n... Field markerPosition { type = Vector3 } Field applyMarker { type = Trigger title = Add } ... Add another FieldListener to both fields: \u0026lt;MACRO_NAME\u0026gt;.script\n... FieldListener markerPosition { command = insertPosition } FieldListener applyMarker { command = applyPosition } ... Finally, add both fields to the Settings section of your user interface: \u0026lt;MACRO_NAME\u0026gt;.script\n... Field markerPosition {} Field applyMarker {} ... The Python functions should look like this: \u0026lt;MACRO_NAME\u0026gt;.py\n... def insertPosition(field): ctx.field(\u0026#34;SoView2DMarkerEditor.newPosXYZ\u0026#34;).value = field.value def applyPosition(): ctx.field(\u0026#34;SoView2DMarkerEditor.useInsertTemplate\u0026#34;).value = True ctx.field(\u0026#34;SoView2DMarkerEditor.add\u0026#34;).touch() ... Whenever the field markerPosition changes its value, the value is automatically applied to the SoView2DMarkerEditor.newPosXYZ. Clicking SoView2DMarkerEditor.add adds the new position to the SoView2DMarkerEditor and the region growing starts.\nInfo:\u0026nbsp; The Field SoView2DMarkerEditor.useInsertTemplate needs to be set to True in order to allow adding markers via Python. Reset Add a new field resetApplication to the Parameters section and set type = Trigger and title = Reset.\nAdd another FieldListener to your Commands and define command = resetApplication.\nAdd the field to your Source region.\n\u0026lt;MACRO_NAME\u0026gt;.script\n... Parameters { Field resetApplication { type = Trigger title = Reset } } ... Commands { ... FieldListener resetApplication { command = resetApplication } } ... Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } Field resetApplication { } } ... What shall happen when we reset the application?\nThe loaded image shall be unloaded, the viewer shall be empty The marker shall be reset if available Add the Python function resetApplication and implement the following: \u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() You can also reset the application to initial state by adding a initCommand to your Window. Call the resetApplication function here, too, and whenever the window is opened, the application is reset to its initial state.\n\u0026lt;MACRO_NAME\u0026gt;.script\nWindow { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 initCommand = resetApplication ... } This can also be used for setting/resetting to default values of the application. For example, update your Python function resetApplication the following way:\n\u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() ctx.field(\u0026#34;imageAlpha\u0026#34;).value = 0.5 ctx.field(\u0026#34;thresholdInterval\u0026#34;).value = 1.0 ctx.field(\u0026#34;isoValueImage\u0026#34;).value = 200 ctx.field(\u0026#34;selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; Information In the end, we want to provide some information about the volume of the segmented area (in ml).\nAdd one more field to your Parameters section and reuse the internal network fields CalculateVolume.totalVolume. Set field to editable = False\nAdd the field to the Info section of your window.\nOpening the window of your macro module in MeVisLab now provides all functionalities we wanted to achieve. You can also play around in the window and define some additional boxes or other MDL controls but the basic application prototype is now finished.\nFinal Macro module MeVisLab GUI Editor MATE provides a powerful GUI editor showing a preview of your current user interface and allowing to reorder elements in the UI via drag and drop. In MATE, open [ Extras \u0026rarr; Enable GUI Editor ].\nMeVisLab GUI Editor Changing the layout via drag and drop automatically adapts your .script file. Save and reload the script and your changes are applied.\nInfo:\u0026nbsp; If the GUI editor is not shown in MATE, make sure to check [View → Preview]. Final Script and Python Files \u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { Field openFile { type = String internalName = LocalImage.name } Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } Field imageAlpha { internalName = SoWEMRendererImage.faceAlphaValue type = Integer min = 0 max = 1 } Field thresholdInterval { internalName = RegionGrowing.autoThresholdIntervalSizeInPercent type = Integer min = 0 max = 100 } Field isoValueImage { internalName = IsoSurfaceImage.isoValue type = Integer min = 0 max = 1000 } Field selected3DView { type = Enum values = Segmented,File,Both } Field totalVolume { internalName = CalculateVolume.totalVolume editable = False } Field resetApplication { type = Trigger title = Reset } Field markerPosition { type = Vector3 } Field applyMarker { type = Trigger title = Add } } } Commands { source = $(LOCAL)/\u0026lt;MACRO_NAME\u0026gt;.py FieldListener selected3DView { command = viewSelectionChanged } FieldListener resetApplication { command = resetApplication } FieldListener markerPosition { command = insertPosition } FieldListener applyMarker { command = applyPosition } } Window { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 initCommand = resetApplication // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } Field resetApplication { } } Box Viewing { layout = Horizontal Viewer View2D.self { expandX = True expandY = True type = SoRenderArea } Viewer SoExaminerViewer.self { expandX = True expandY = True type = SoExaminerViewer } } Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } Field imageAlpha { step = 0.1 slider = True } Field thresholdInterval { step = 0.1 slider = True } Field isoValueImage { step = 2 slider = True } Field markerPosition {} Field applyMarker {} ComboBox selected3DView { alignX = Left editable = False } } Box Info { layout = Horizontal Field totalVolume {} } } } \u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def viewSelectionChanged(field): if field.value == \u0026#34;Segmented\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 0 if field.value == \u0026#34;File\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 1 if field.value == \u0026#34;Both\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 2 def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() ctx.field(\u0026#34;imageAlpha\u0026#34;).value = 0.5 ctx.field(\u0026#34;thresholdInterval\u0026#34;).value = 1.0 ctx.field(\u0026#34;isoValueImage\u0026#34;).value = 200 ctx.field(\u0026#34;selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; def insertPosition(field): ctx.field(\u0026#34;SoView2DMarkerEditor.newPosXYZ\u0026#34;).value = field.value def applyPosition(): ctx.field(\u0026#34;SoView2DMarkerEditor.useInsertTemplate\u0026#34;).value = True ctx.field(\u0026#34;SoView2DMarkerEditor.add\u0026#34;).touch() Summary You now added a user interface to your macro module. The window opens automatically on double-click . Fields defined in the Parameters section can be modified in the MeVisLab Module Inspector. Python allows to implement functions executed on events raised by the user or by the application itself. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","User Interface","Python","GUI Editor"],"section":"tutorials"},{"date":"1674000000","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary4/","title":"Step 4: Review - Automated Tests","summary":"Step 4: Review - Automated Tests \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous chapters you developed a macro module with a user interface and Python scripting. In this step you will see how to implement an automated test to verify and validate the requirements defined in Overview.\nSteps to Do Create a Test Network Using Your Macro Module Create a new and empty network and save it as .mlab file. Remember the location.\n","content":"Step 4: Review - Automated Tests \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous chapters you developed a macro module with a user interface and Python scripting. In this step you will see how to implement an automated test to verify and validate the requirements defined in Overview.\nSteps to Do Create a Test Network Using Your Macro Module Create a new and empty network and save it as .mlab file. Remember the location.\nUse Module Search and add your macro module developed in previous steps to your workspace.\nMacro module You can see that the module does not have any inputs or outputs. You cannot connect it to other modules. For testing purposes it makes sense to provide the viewers and images as outputs, so that you can use them for generating screenshots.\nOpen the .script file in MATE as already explained in Step 3. In the Outputs section, add the following:\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs { Field out2D { internalName = LocalImage.outImage } Field out3D { internalName = SoSwitch.self } Field outSegmentationMask { internalName = CloseGap.output0 } } ... } Macro module with outputs You can now add a viewer or any other module to your macro module and use them for testing. In our example, we add a CalculateVolume module to the segmentation mask and a SoCameraInteraction with two OffscreenRenderer modules to the 3D output. In the end, we need an ImageCompare module to compare expected and real image in our test.\nTest Network Create Test Case Open MeVisLab TestCaseManager via [ File \u0026rarr; Run TestCaseManager... ]. On the tab Test Creation, define a name of your test case, for example, TutorialSummaryTest. Select \u0026ldquo;Type\u0026rdquo; as Macros, define the package and use the same as for your macro module, select Import Network, and select your saved .mlab file from the step above. Click Create.\nTest Creation MATE automatically opens the Python file of your test case and it appears in MeVisLab TestCaseManager.\nTest Creation Write Test Functions in Python Preparations Before writing a test case, we need some helper functions in Python, which we will use in our test cases. The first thing we need is a function to load images.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\nfrom mevis import * from TestSupport import Base, Fields, Logging, ScreenShot from TestSupport.Macros import * path_to_image = \u0026#34;$(DemoDataPath)/BrainMultiModal/ProbandT1.dcm\u0026#34; marker_location = [-29, -26, 45] marker_location_new = [-20, -30, 35] new_color = [0.5, 0.5, 0] def loadImage(full_path): MLAB.log(\u0026#34;Setting image path to \u0026#39;\u0026#34; + full_path + \u0026#34;\u0026#39;...\u0026#34;) ctx.field(\u0026#34;TutorialSummary.openFile\u0026#34;).value = full_path We define the path to a file to be loaded. The function loadImage sets the openFile field of the TutorialSummary module.\nThe arrays for the marker location and color will be used later.\nNext, we need a function to check if the loaded image available at the first output of our macro module (out2D) is valid.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def isImageValid(): MLAB.log(\u0026#34;Checking if image is valid...\u0026#34;) data_valid = ctx.field(\u0026#34;TutorialSummary.out2D\u0026#34;).isValid() if data_valid: return True else: return False ... We also need to set a marker in our macro module.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def setMarkerPosition(vector): MLAB.log(\u0026#34;Setting marker position to [\u0026#34; + str(vector[0]) + \u0026#34;,\u0026#34; + str(vector[1]) + \u0026#34;,\u0026#34; + str(vector[2]) + \u0026#34;]...\u0026#34;) ctx.field(\u0026#34;TutorialSummary.markerPosition\u0026#34;).setValue(vector[0], vector[1], vector[2]) ctx.field(\u0026#34;TutorialSummary.applyMarker\u0026#34;).touch() MLAB.processEvents() while not ctx.field(\u0026#34;TutorialSummary.outSegmentationMask\u0026#34;).isValid(): MLAB.msleep(100) MLAB.processEvents() MLAB.log(\u0026#34;Marker position set to \u0026#39;\u0026#34; + str(ctx.field(\u0026#34;TutorialSummary.markerPosition\u0026#34;).value) + \u0026#34;\u0026#39;...\u0026#34;) ... The setMarkerPosition function gets a three-dimensional vector and sets the markerPosition field of our module. Then, the applyMarker trigger is touched. As the region growing algorithm might need some time to segment, we need to wait until the outSegmentationMask output field is valid, meaning that there is a valid segmentation mask at the segmentation mask output of our macro module.\nFinally, we need to reset the application to its initial state, so that each test case has the initial start conditions of the application. A test case should never depend on another test case so that they all can be executed exclusively.\nExample: Having one test case for the requirement to load images and one for setting the marker depending on the image to be loaded by the previous test case, you will never be able to execute the marker test case without executing the load image first.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def reset(): MLAB.log(\u0026#34;Resetting application...\u0026#34;) ctx.field(\u0026#34;TutorialSummary.resetApplication\u0026#34;).touch() ... For a reset, we just touch the resetApplication field of our macro module TutorialSummary.\nRequirement 1: The application shall be able to load DICOM data The first requirement we want to test is the possibility to load DICOM data. After setting the file to be loaded, the output provides a valid image. Resetting the application shall unload the image.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 1: The application shall be able to load DICOM data def TEST_LoadDICOMData(): # Set path to image and expect a valid image loadImage(path_to_image) ASSERT_TRUE(isImageValid()) # Reset again and expect an invalid image reset() ASSERT_FALSE(isImageValid()) ... Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a region growing algorithm Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the region growing algorithm This test case shall make sure the RegionGrowing module calculates the total volume and number of voxels to be larger than 0 in the case a marker has been set. Without loading an image or after resetting the application, the values shall be 0.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a RegionGrowing algorithm # Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the RegionGrowing def TEST_RegionGrowing(): # Load image and expect volumes and voxels without marker to be 0 loadImage(path_to_image) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) # Set marker and expect volumes and voxels to be larger than 0 reset() setMarkerPosition(marker_location) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_GT(region_growing_voxels, 0) ASSERT_GT(region_growing_volume, 0) # Reset application and expect volumes and voxels to be 0 again reset() region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) ... Requirement 4.2: It shall be possible to define a threshold for the region growing algorithm For the threshold of the region growing it makes sense to extend the previous test case instead of writing a new one. We already have a segmentation based on the default threshold value and can just change the threshold and compare the resulting volumes.\nIncreasing the threshold shall result in larger volumes, decreasing shall result in smaller values.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a RegionGrowing algorithm # Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the RegionGrowing # Requirement 4.2: It shall be possible to define a threshold for the RegionGrowing algorithm def TEST_RegionGrowing(): # Load image and expect volumes and voxels without marker to be 0 loadImage(path_to_image) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) # Set marker and expect volumes and voxels to be larger than 0 setMarkerPosition(marker_location) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_GT(region_growing_voxels, 0) ASSERT_GT(region_growing_volume, 0) # Test the threshold functionality by changing the value and comparing the results current_threshold = ctx.field(\u0026#34;TutorialSummary.thresholdInterval\u0026#34;).value current_threshold = current_threshold + 0.5 ctx.field(\u0026#34;TutorialSummary.thresholdInterval\u0026#34;).value = current_threshold region_growing_voxels_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_GT(region_growing_voxels_new, region_growing_voxels) ASSERT_GT(region_growing_volume_new, region_growing_volume) current_threshold = current_threshold - 0.7 ctx.field(\u0026#34;TutorialSummary.thresholdInterval\u0026#34;).value = current_threshold region_growing_voxels_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_LT(region_growing_voxels_new, region_growing_voxels) ASSERT_LT(region_growing_volume_new, region_growing_volume) # Reset application and expect volumes and voxels to be 0 again reset() region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) ... Requirement 5: The 2D viewer shall display the segmentation results as a semitransparent overlay Requirement 5.1: It shall be possible to define the color of the overlay The requirement 5 cannot be tested automatically. Transparencies should be tested by a human being.\nNevertheless, we can write an automated test checking the possibility to define the color of the overlay and the 3D segmentation.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def TEST_OverlayColor(): reset() loadImage(path_to_image) setMarkerPosition(marker_location) ctx.field(\u0026#34;SoCameraInteraction.viewAll\u0026#34;).touch() ctx.field(\u0026#34;SoCameraInteraction.viewFromLeft\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() MLAB.processInventorQueue() current_color = ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).setValue(new_color) ctx.field(\u0026#34;SoCameraInteraction.viewAll\u0026#34;).touch() ctx.field(\u0026#34;SoCameraInteraction.viewFromLeft\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() MLAB.processInventorQueue() ASSERT_NE(current_color, ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value) ASSERT_EQ(ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value, ctx.field(\u0026#34;TutorialSummary.SoView2DOverlay.baseColor\u0026#34;).value) ASSERT_EQ(ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value, ctx.field(\u0026#34;TutorialSummary.SoWEMRendererSegmentation.faceDiffuseColor\u0026#34;).value) ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ... Again, we reset the application to an initial state, load the image, and set a marker. We remember the initial color and set a new color for our macro module. Then, we check if the new color differs from the old color and if the colors used by the internal modules SoWEMRendererSegmentation and SoView2DOverlay changed to our new color.\nFinally, an image comparison is done for the 3D rendering using the old and the new color. The images shall differ.\nThe call MLAB.processInventorQueue() is sometimes necessary if an Open Inventor scene changed via Python scripting, because the viewers might not update immediately after changing the field. MeVisLab is now forced to process the queue in Open Inventor and to update the renderings.\nRequirement 8: The total volume of the segmented volume shall be calculated and shown (in ml) For the correctness of the volume calculation, we added the CalculateVolume module to our test network. The volume given by our macro is compared to the volume of the segmentation from output outSegmentationMask calculated by the CalculateVolume module.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 8: The total volume of the segmented volume shall be calculated and shown (in ml) def TEST_VolumeCalculation(): # Reset and expect all volumes and number of voxels to be 0 reset() reference_volume = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value ASSERT_EQ(reference_volume, 0) # Load patient, set marker and expect all volumes and number of voxels to be \u0026gt; 0 loadImage(path_to_image) reference_volume = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value ASSERT_EQ(reference_volume, 0) setMarkerPosition(marker_location) reference_volume = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value current_volume = ctx.field(\u0026#34;TutorialSummary.totalVolume\u0026#34;).value # Expect the total volume of the application to be the same as our additional CalculateVolume module ASSERT_GT(reference_volume, 0) ASSERT_EQ(reference_volume, current_volume) #set marker to a different location and check if volumes change. setMarkerPosition(marker_location_new) reference_volume_new = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value current_volume_new = ctx.field(\u0026#34;TutorialSummary.totalVolume\u0026#34;).value ASSERT_NE(reference_volume, reference_volume_new) ASSERT_NE(current_volume, current_volume_new) ASSERT_EQ(reference_volume_new, current_volume_new) ... Requirement 9: It shall be possible to toggle the visible 3D objects Requirement 9.1: Original data Requirement 9.2: Segmentation results Requirement 9.3: All In the end, we want to develop a testcase for the 3D toggling of the view. We cannot exactly test if the rendering is correct; therefore, we will check if the 3D rendering image changes when toggling the 3D view. We will use the modules OffscreenRenderer, ImageCompare, and SoCameraInteraction, which we added to our test network.\nInitially, without any marker and segmentation, the views Both and Head show the same result. After adding a marker, we are going to test if different views result in different images.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 9: It shall be possible to toggle the visible 3D objects # Requirement 9.1: Original data # Requirement 9.2: Segmentation results # Requirement 9.3: All def TEST_Toggle3DVolumes(): # Set ImageCompare.postErrorOnDiff to False because otherwise differences will lead to a failed test ctx.field(\u0026#34;ImageCompare.postErrorOnDiff\u0026#34;).value = False # Reset application and check if number of voxels is 0 on output reset() loadImage(path_to_image) # Without marker, the content of the 3D viewer should be the same for File and All ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; MLAB.processInventorQueue() ctx.field(\u0026#34;SoCameraInteraction.viewFromLeft\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;File\u0026#34; MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_TRUE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) # With marker, the content of the 3D viewer should be different setMarkerPosition(marker_location) ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;File\u0026#34; ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Segmented\u0026#34; ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ... Sorting Order in TestCaseManager The MeVisLab TestCaseManager sorts your test cases alphabetically. Your test cases should look like this now:\nTestCaseManager Sorting Generally, test cases should not depend on each other and the order of their execution should not matter. Sometimes it makes sense though to execute tests in a certain order, for example, for performance reasons. In this case, you can add numeric prefixes to your test cases. This might look like this then:\nTestCaseManager Custom Sorting Not Testable Requirements As already mentioned, some requirements cannot be tested in an automated environment. Human inspection cannot be replaced completely.\nIn our application, the following tests have not been tested automatically:\nRequirement 2: The application shall provide a 2D and a 3D viewer Requirement 3: The 2D viewer shall display the loaded images Requirement 5: The 2D viewer shall display the segmentation results as a semitransparent overlay Requirement 6: The 3D viewer shall visualize the loaded data in a three-dimensional volume rendering Requirement 7: The 3D viewer shall additionally show the segmentation result as a three-dimensional mesh Test Reports The results of your tests are shown in a Report Viewer. You can also export the results to JUnit for usage in build environments like Jenkins.\nReportViewer Screenshots You can also add screenshots of your Open Inventor scene to the report. Add the following to your Python script wherever you want to capture the content of the SoCameraInteraction module and a snapshot of your 3D scene is attached to your test report:\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... result = ScreenShot.createOffscreenScreenShot(\u0026#34;SoCameraInteraction.self\u0026#34;, \u0026#34;screenshot.png\u0026#34;) Logging.showImage(\u0026#34;My screenshot\u0026#34;, result) Logging.showFile(\u0026#34;Link to screenshot file\u0026#34;, result) ... Summary Define accessible fields for macro modules, so that they can be set in Python tests. Add outputs to your macro modules for automated testing and connecting testing modules. Testcase numbering allows you to sort them and define execution order. Info:\u0026nbsp; Additional information about MeVisLab TestCenter can be found in TestCenter Manual \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","Automated Tests","Python"],"section":"tutorials"},{"date":"1674086400","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary5/","title":"Step 5: Review - Installer creation","summary":"Step 5: Review - Installer creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Your macro module has been tested manually and/or automatically? Then, you should create your first installable executable and deliver it to your customer(s) for final evaluation.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK, so that you can generate an installer of your developed macro module.\n","content":"Step 5: Review - Installer creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Your macro module has been tested manually and/or automatically? Then, you should create your first installable executable and deliver it to your customer(s) for final evaluation.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK, so that you can generate an installer of your developed macro module.\nFree evaluation licenses of the MeVisLab ApplicationBuilder, time-limited to three months, can be requested at sales(at)mevislab.de.\nSteps to Do Install Tools Necessary for Installer Generation The MeVisLab Project Wizard for standalone applications [ File \u0026rarr; Run Project Wizard... \u0026rarr; Standalone Application ] provides a check for all necessary tools you need to install before generating an installer.\nMeVisLab Project Wizard Click on Check if required tools are installed. The following dialog opens:\nCheck required tools You can see that NSIS and either Dependency Walker or Dependencies are necessary to create an installable executable. MeVisLab provides information about the necessary version(s).\nDownload and install/extract NSIS and Dependency Walker or Dependencies. Add both executables to your PATH environment variable, for example, C:\\Program Files\\depends and C:\\Program Files (x86)\\NSIS.\nRestart MeVisLab and open Project Wizard again. All required tools should now be available.\nUse MeVisLab Project Wizard to Generate the Installer Select your macro module and the package and click Next.\nWelcome The general settings dialog allows you to define a name for your application. You can also define a version, in our case, we decide not to be finished and have a version 0.5. You can include debug files and decide to build a desktop or web application. We want to build an Application Installer for a desktop system. You can decide to precompile your Python files and you have to select your MeVisLab MeVisLab ApplicationBuilder license.\nGeneral Settings Define your license text that is shown during installation of your executable. You can decide to use our predefined text, select a custom file, or do not include any license text.\nLicense Text The next dialog can be skipped for now, you can include additional files into your installer that are not automatically added by MeVisLab from the dependency analysis.\nManual File Lists Define how the window of your application shall look.\nApplication Options Skip the next dialog, we do not need additional installer options.\nInstaller Options The MeVisLab ToolRunner starts generating your installer. After finishing installer generation, you will find a link to the target directory.\nMeVisLab ToolRunner The directory contains the following files (and some more maybe):\nBatch (.bat) file Installer (.exe) file MeVisLab Install (.mlinstall) file Shell (.sh) script Software Bill of Materials [SBOM] (_sbom.json) Batch File The batch file allows you to generate the executable again via a Windows batch file. You do not need the Project Wizard anymore now.\nInstaller File The resulting installer file for your application is an executable.\nMeVisLab Install File The .mlinstall file provides all information you just entered into the wizard. We will need this in Step 7: Refine - Rebuild Installer again.\nThe file is initially generated by the Project Wizard. Having a valid file already, you can create new versions by using the MeVisLab ToolRunner.\nShell Skript The shell skript allows you to generate the executable again via a Unix shell like bash. You do not need the Project Wizard anymore now.\nSoftware Bill of Materials [SBOM] The SBOM file includes a list of all third-party components, libraries and dependencies included into your installer by MeVisLab. We use the standard format CycloneDX which allows to import this file to standard evaluation tools like Dependency-Track.\nInstall Your Executable You can now execute the installer of your application.\nThe installer initially shows a welcome screen showing the name and version of your application.\nInstaller Next, you will see your selected license agreement from the project wizard and a selection to install for anyone or just for the current user.\nLicense Agreement You can also select to create shortcuts and desktop icons.\nShortcuts and icons The last step is to select the target directory for your application.\nTarget directory After the installer finished the setup, you will find a desktop icon and a start menu entry for your application.\nStartmenu Desktop Licensing:\u0026nbsp; MeVisLab executables require an additional MeVisLab Runtime license. It makes sure that your resulting application needs to be licensed, too.\nFree evaluation licenses of the MeVisLab ApplicationBuilder and MeVisLab Runtime licenses for testing purposes can be requested at sales(at)mevislab.de.\nRuntime License After entering your license file, the application runs and you can use it on a customer system.\nInstalled Application Info:\u0026nbsp; By default, your user interface uses a standard stylesheet for colors and appearance of your user interface elements. The style can be customized easily. Summary The MeVisLab ApplicationBuilder allows you to create installable executables from your MeVisLab networks. The resulting application can be customized to your needs via the Project Wizard. Your application will be licensed separately, so that you can completely control the usage. ","tags":["Advanced","Tutorial","Prototyping","Application Builder","Installer"],"section":"tutorials"},{"date":"1674172800","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary6/","title":"Step 6: Refine - Update Application","summary":"Step 6: Refine - Update Application \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous step you developed an application that can be installed on your customers systems for usage. In this step we are going to integrate simple feedback into our executable and recreate the installer.\nWe want to show you how easy it is to update your application using MeVisLab.\nYour customer requests an additional requirement to define the transparency of your 2D overlay in addition to defining the color.\n","content":"Step 6: Refine - Update Application \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous step you developed an application that can be installed on your customers systems for usage. In this step we are going to integrate simple feedback into our executable and recreate the installer.\nWe want to show you how easy it is to update your application using MeVisLab.\nYour customer requests an additional requirement to define the transparency of your 2D overlay in addition to defining the color.\nRequirement 5.2: It shall be possible to define the alpha value of the overlay Steps to Do Adapt Your Macro Module Use the module search to add your macro module to your workspace. We need an additional UI element for setting the alpha value of the overlay.\nRight-click your module and select [ Related Files \u0026rarr; \u0026lt;MACRO_NAME\u0026gt;.script ].\nIn MATE, add another field to your Parameters section and reuse the field by setting the internalName. Add the field to the Settings section of your Window, maybe directly after the color selection.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { ... Parameters { ... Field selectOverlayTransparency { internalName = SoView2DOverlay.alphaFactor } ... } } Window { ... Box Settings { ... Field selectOverlayTransparency { title = Alpha } ... } ... } Back in MeVisLab IDE, your user interface should now provide the possibility to define an alpha value of the overlay. Changes are applied automatically because you reused the field of the SoView2DOverlay module directly.\nUpdated User Interface You can also update your Python files for new or updated requirements. In this example we just want to show the basic principles; therefore, we only add this new element to the .script file.\nIf you want to write an additional Python test case, you can also do that.\nSummary Your application can be updated by modifying the macro module and/or internal network of your application. Any changes will be applied to your installable executable in the next step. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping"],"section":"tutorials"},{"date":"1674259200","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary7/","title":"Step 7: Refine - Rebuild Installer","summary":"Step 7: Refine - Rebuild Installer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step you are recreating your application installer after changing the UI in previous Step 6: Refine - Update Application.\nSteps to Do Update the .mlinstall File You do not need to use the Project Wizard now, because you already have a valid .mlinstall file. The location should be in your package under .\\Configuration\\Installers\\TutorialSummary. Open the file in any text editor and search for the $VERSION 0.5. Change the version to something else, in our case, we now have our first major release 1.0.\n","content":"Step 7: Refine - Rebuild Installer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step you are recreating your application installer after changing the UI in previous Step 6: Refine - Update Application.\nSteps to Do Update the .mlinstall File You do not need to use the Project Wizard now, because you already have a valid .mlinstall file. The location should be in your package under .\\Configuration\\Installers\\TutorialSummary. Open the file in any text editor and search for the $VERSION 0.5. Change the version to something else, in our case, we now have our first major release 1.0.\nInfo:\u0026nbsp; You can also run the Project Wizard again but keep in mind that manual changes on your .mlinstall file might be overwritten. The wizard recreates your .mlinstall file whereas the ToolRunner just uses it. Use MeVisLab ToolRunner Save the file and open MeVisLab ToolRunner.\nMeVisLab ToolRunner Open the .mlinstall file in ToolRunner and select the file. Click Run on Selection.\nRun on Selection The ToolRunner automatically builds your new installer using version 1.0.\nInstall Application Again Execute your installable executable again. You do not have to uninstall previous version(s) of your application first. Already existing applications will be replaced by new installation - at least if you select the same target directory.\nInstall new version The installer already shows your updated version 1.0. It is not necessary to select your runtime license again because it has not been touched during update.\nApplication version 1.0 The new installed application now provides your new UI element for defining the alpha value of the overlay.\nSummary Updates of your application installer can be applied by using the MeVisLab ToolRunner. The executable can be updated on your customers system(s) and your changes on the macro module and network(s) are applied. ","tags":["Advanced","Tutorial","Prototyping","Tool Runner","Installer"],"section":"tutorials"},{"date":"1677196800","url":"https://mevislab.github.io/examples/pull/133/tutorials/summary/summary8/","title":"Extra: Run Your Application in a Browser","summary":"Extra: Run Your Application in a Browser \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This step explains how to run your developed application in a browser. The MeVisLab network remains the same, only some adaptations are necessary for running any macro module in a browser window.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab Webtoolkit license. It extends the MeVisLab SDK, so that you can develop web macro modules.\n","content":"Extra: Run Your Application in a Browser \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This step explains how to run your developed application in a browser. The MeVisLab network remains the same, only some adaptations are necessary for running any macro module in a browser window.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab Webtoolkit license. It extends the MeVisLab SDK, so that you can develop web macro modules.\nFree evaluation licenses of the MeVisLab Webtoolkit, time-limited to three months, can be requested at sales(at)mevislab.de.\nSteps to Do Make sure to have your macro module from previous Step 2 available.\nCreate a Web Macro Module Open Project Wizard via [ File \u0026rarr; Run Project Wizard... ] and select Web Macro module. Run the Wizard and enter details of your new macro module.\nWeb macro module wizard Run the Wizard and enter details of your web macro module.\nWeb macro module properties Click Next and select optional web plugin features. Click Create.\nWeb macro module The folder of your project automatically opens in an Explorer window.\nUsing Your Web Macro Module As you created a global web macro module, you can search for it in the MeVisLab Module Search. In the case the module cannot be found, select [ Extras \u0026rarr; Reload Module Database (Clear Cache) ].\nWeb macro module The internal network of your module is empty. We will reuse the internal network of your macro module developed in Step 2.\nAdd the Internal Network of Your Application Open the internal network of your previously created macro module from Step 2. Select all and copy to your internal network of the TutorialSummaryBrowser module. Save the internal network and close the tab in MeVisLab.\nInternal network We are going to develop a web application; therefore, we need special RemoteRendering modules for the viewer. Add two RemoteRendering modules and a SoCameraInteraction to your workspace and connect them to your existing modules as seen below.\nRemote Rendering Additional Info:\u0026nbsp; We are using the hidden outputs of the View2D and the SoExaminerViewer. You can show them by pressing the SPACE key. Develop the User Interface Make sure to have both macro modules visible in MeVisLab SDK, we are reusing the .script and .py files developed in Step 3.\nMacro modules Right-click the module TutorialSummaryBrowser and select [ Related Files \u0026rarr; TutorialSummaryBrowser.script ].\nThe file opens in MATE and you will see that it looks similar to the .script file of a normal macro module. The only difference is an additional Web section at the end of the file. It defines the locations of some JavaScript libraries and the URL to be used for a preview of your website.\nTutorialSummaryBrowser.script\nWeb { plugin = \u0026#34;$(MLAB_MeVisLab_Private)/Sources/Web/application/js/jquery/Plugin.js\u0026#34; plugin = \u0026#34;$(MLAB_MeVisLab_Private)/Sources/Web/application/js/yui/Plugin.js\u0026#34; // Specify web plugins here. If you have additional JavaScript files, you can load them from // the plugin. It is also possible to load other plugins here. plugin = \u0026#34;$(LOCAL)/www/js/Plugin.js\u0026#34; Deployment { // Deploy the www directory recursively when building web application installers directory = \u0026#34;$(LOCAL)/www\u0026#34; } // The developer URL is used by the startWorkerService.py user script. developerUrl = \u0026#34;MeVis/TutorialSummary/Projects/TutorialSummaryBrowser/Modules/www/TutorialSummaryBrowser.html\u0026#34; } Open the script file of the TutorialSummary module from Step 3. Copy the output section to your web macro and define internalName as the output of your RemoteRendering modules.\nYou can also copy all fields from Parameters section to your web macro module script.\nTutorialSummaryBrowser.script\nInterface { Inputs {} Outputs { Field out2D { internalName = RemoteRendering2D.output } Field out3D { internalName = RemoteRendering3D.output } Field outSegmentationMask { internalName = CloseGap.output0 } } Parameters { Field openFile { type = String internalName = LocalImage.name } Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } Field selectOverlayTransparency { internalName = SoView2DOverlay.alphaFactor } Field imageAlpha { internalName = SoWEMRendererImage.faceAlphaValue type = Integer min = 0 max = 1 } Field thresholdInterval { internalName = RegionGrowing.autoThresholdIntervalSizeInPercent type = Integer min = 0 max = 100 } Field isoValueImage { internalName = IsoSurfaceImage.isoValue type = Integer min = 0 max = 1000 } Field selected3DView { type = Enum values = Segmented,File,Both } Field totalVolume { internalName = CalculateVolume.totalVolume editable = False } Field resetApplication { type = Trigger title = Reset } Field markerPosition { type = Vector3 } Field applyMarker { type = Trigger title = Add } } } Reloading your web macro in MeVisLab SDK now shows the same outputs as the original macro module. The only difference is the type of your output. It changed from MLImage and Open Inventor scene to MLBase from your RemoteRendering modules.\nMacro modules The internal network of your web macro should look like this:\nMacro modules You can emulate the final viewer by adding a RemoteRenderingClient module to the outputs of your web macro.\nRemoteRenderingClient Open the .script files of your macro modules and copy the FieldListeners from the Commands section of your TutorialSummary.script to TutorialSummaryBrowser.script.\nTutorialSummaryBrowser.script\nCommands { source = $(LOCAL)/TutorialSummaryBrowser.py FieldListener selected3DView { command = viewSelectionChanged } FieldListener resetApplication { command = resetApplication } FieldListener markerPosition { command = insertPosition } FieldListener applyMarker { command = applyPosition } } Also copy the Window section to your web macro module. The Box of the Viewing tab needs to be modified because we are now using the RemoteRendering outputs instead of the View3D and SoExaminerViewer outputs.\nTutorialSummaryBrowser.script\nWindow \u0026#34;MainPanel\u0026#34; { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 initCommand = resetApplication // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } Field resetApplication { } } Box Viewing { layout = Horizontal RemoteRendering out2D { expandX = True expandY = True } RemoteRendering out3D { expandX = True expandY = True } } Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } Field selectOverlayTransparency { title = Alpha } Field imageAlpha { step = 0.1 slider = True } Field thresholdInterval { step = 0.1 slider = True } Field isoValueImage { step = 2 slider = True } Field markerPosition {} Field applyMarker {} ComboBox selected3DView { alignX = Left editable = False } } Box Info { layout = Horizontal Field totalVolume {} } } } Python Functions After we reused the scripts, we now need to copy the Python functions from TutorialSummary.py to TutorialSummaryBrowser.py. Open the Python file of your web macro. You will see an additional import from MLABRemote, which is required for remote rendering calls. The MLABRemote context is already setup automatically and can be used.\nTutorialSummaryBrowser.py\nfrom mevis import * from MLABRemote import MLABRemote, allowedRemoteCall MLABRemote.setup(ctx) Copy the Python functions from TutorialSummary.py to TutorialSummaryBrowser.py. They can remain unchanged but require an additional @allowedRemoteCall function. This is necessary to explicitly allow remote execution of the function and is disabled by default for security reasons.\nTutorialSummaryBrowser.py\nfrom mevis import * from MLABRemote import MLABRemote, allowedRemoteCall MLABRemote.setup(ctx) @allowedRemoteCall def viewSelectionChanged(field): if field.value == \u0026#34;Segmented\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 0 if field.value == \u0026#34;File\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 1 if field.value == \u0026#34;Both\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 2 @allowedRemoteCall def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() ctx.field(\u0026#34;imageAlpha\u0026#34;).value = 0.5 ctx.field(\u0026#34;thresholdInterval\u0026#34;).value = 1.0 ctx.field(\u0026#34;isoValueImage\u0026#34;).value = 200 ctx.field(\u0026#34;selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; @allowedRemoteCall def insertPosition(field): ctx.field(\u0026#34;SoView2DMarkerEditor.newPosXYZ\u0026#34;).value = field.value @allowedRemoteCall def applyPosition(): ctx.field(\u0026#34;SoView2DMarkerEditor.useInsertTemplate\u0026#34;).value = True ctx.field(\u0026#34;SoView2DMarkerEditor.add\u0026#34;).touch() Run Your Application in a Browser MeVisLab provides a local webserver and you can preview your application in a browser by selecting the module and open [ Scripting \u0026rarr; Web \u0026rarr; Start Module Through Webservice ]. The integrated webserver starts and your default browser opens the local website showing your application.\nWebserver preview Select your web macro TutorialSummaryBrowser and right-click to select [ Related Files \u0026rarr; Show Definition Folder ]. You can see the folder structure of your web macro and modify the stylesheet depending on your needs.\nOpen the Current Web Instance in MeVisLab SDK If you want to inspect the internal state of the modules and your internal network, open the console of your browser and enter MLAB.GUI.Application.module(\u0026lsquo;TutorialSummaryBrowser\u0026rsquo;).showIDE(). MeVisLab opens and you can change your internal network while all modifications are applied on the website on-the-fly.\nMeVisLab SDK Summary MeVisLab macro modules can easily be adapted to run in a browser window. MeVisLab RemoteRendering allows to run in a browser or embedded into other application user interfaces. It does so by sending updated images to a client and receiving input events from this client. Clients can be emulated by using a RemoteRenderingClient module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","Browser","Web"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/","title":"Chapter VIII: Third-party Components","summary":"MeVisLab Tutorial Chapter VIII Using Third-party Software Integrated into MeVisLab MeVisLab is equipped with a lot of useful software right out of the box, like the Insight Segmentation and Registration Toolkit (ITK) or the Visualization Toolkit (VTK). This chapter works as a guide on how to use some of the third-party components integrated in MeVisLab for your projects via Python scripting.\nAdditional Information:\u0026nbsp; You will also find instructions to install and use any Python package (e.g., PyTorch) in MeVisLab using the PythonPip module. OpenCV OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV includes, among others, algorithms to:\n","content":"MeVisLab Tutorial Chapter VIII Using Third-party Software Integrated into MeVisLab MeVisLab is equipped with a lot of useful software right out of the box, like the Insight Segmentation and Registration Toolkit (ITK) or the Visualization Toolkit (VTK). This chapter works as a guide on how to use some of the third-party components integrated in MeVisLab for your projects via Python scripting.\nAdditional Information:\u0026nbsp; You will also find instructions to install and use any Python package (e.g., PyTorch) in MeVisLab using the PythonPip module. OpenCV OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV includes, among others, algorithms to:\ndetect and recognize faces identify objects classify human actions on video track camera movements track moving objects extract 3D models of objects produce 3D point clouds from stereo cameras stitch images together to produce a high resolution image of an entire scene find similar images from an image database remove red eyes from images taken using flash follow eye movements recognize scenery establish markers to overlay with augmented reality assimp The THE ASSET IMPORTER LIBRARY supports loading and processing geometric scenes from various well known 3D formats. MeVisLab uses assimp to import these files and reuses the scenes directly in MeVisLab.\nA list of supported formats can be found here.\nPyTorch [not integrated initially] PyTorch is a machine learning framework based on the Torch library, used for applications such as Computer Vision and Natural Language Processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.\nThe tutorials available here shall provide examples on how to integrate AI into MeVisLab. You can also integrate other Python AI packages the same way.\nMatplotlib Matplotlib is a library for creating static, animated, and interactive visualizations in Python.\ncreate publication quality plots Make interactive figures that can be zoomed, panned, and updated Customize visual style and layout Export to many file formats ","tags":["Advanced","Tutorial","Third-party"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/opencv/","title":"OpenCV","summary":"Open Source Computer Vision Library (OpenCV) Introduction OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\nThis chapter provides some examples how to use OpenCV in MeVisLab.\nOther Resources You can find a lot of OpenCV examples and tutorials on their website.\n","content":"Open Source Computer Vision Library (OpenCV) Introduction OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\nThis chapter provides some examples how to use OpenCV in MeVisLab.\nOther Resources You can find a lot of OpenCV examples and tutorials on their website.\n","tags":["Advanced","Tutorial","OpenCV","Python"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/opencv/thirdpartyexample1/","title":"Example 1: Webcam Access with OpenCV","summary":"Example 1: Webcam Access with OpenCV Introduction In this example, we are using the PythonImage module and access your webcam to show the video in a View2D.\nSteps to Do Creating the Network to be Used for Testing Add the modules to your workspace and connect them as seen below.\nExample Network The viewer is empty because the image needs to be set via Python scripting.\n","content":"Example 1: Webcam Access with OpenCV Introduction In this example, we are using the PythonImage module and access your webcam to show the video in a View2D.\nSteps to Do Creating the Network to be Used for Testing Add the modules to your workspace and connect them as seen below.\nExample Network The viewer is empty because the image needs to be set via Python scripting.\nInfo:\u0026nbsp; More information about the PythonImage module can be found here Create a Macro Module Now you need to create a macro module from your network. You can either group your modules, create a local macro, and convert it to a global macro module, or you use the Project Wizard and load your .mlab file.\nInfo:\u0026nbsp; A tutorial on how to create your own macro modules can be found in Example 2.2: Global macro modules. Make sure to add a Python file to your macro module. Add the View2D to Your UI Next, we need to add the View2D to a Window of your macro module. Right-click on your module , open the context menu and select [ Related Files \u0026rarr; \u0026lt;YOUR_MODULE_NAME\u0026gt;.script ]. The text editor MATE opens. You can see the .script file of your module.\nAdd the following to your file: \u0026lt;YOUR_MODULE_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters {} } Commands { source = $(LOCAL)/\u0026lt;YOUR_MODULE_NAME\u0026gt;.py } Window { h = 500 w = 500 initCommand = setupInterface destroyedCommand = releaseCamera Vertical { Horizontal { Button { title = Start command = startCapture } Button { title = Pause command = stopCapture } } Horizontal { expandX = True expandY = True Viewer View2D.self { type = SoRenderArea } } } } Now open the Python file of your module and define the commands to be called from the .script file: \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# from mevis import * # Set up the interface for PythonImage module def setupInterface(): pass # Release camera in the end def releaseCamera(_): pass # Start capturing webcam def startCapture(): pass # Stop capturing webcam def stopCapture(): pass Use OpenCV Your View2D is still empty, let\u0026rsquo;s get access to the webcam and show the video in your module. Open the Python file of your network again and enter the following code: \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# from mevis import * import cv2 import OpenCVUtils _interfaces = [] camera = None # Set up the interface for PythonImage module def setupInterface(): global _interfaces _interfaces = [] interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) _interfaces.append(interface) # Release camera in the end def releaseCamera(_): pass # Start capturing webcam def startCapture(): pass # Stop capturing webcam def stopCapture(): pass We now imported cv2 and OpenCVUtils, so that we can use them in Python. Additionally, we defined a list of _interfaces and a camera. The import of mevis is not necessary for this example.\nThe setupInterfaces function is called whenever the Window of your module is opened. Here we are getting the interface of the PythonImage module and append it to our global list.\nAccessing the Webcam Now we want to start capturing the camera. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# Start capturing webcam def startCapture(): global camera if not camera: camera = cv2.VideoCapture(0) ctx.callWithInterval(0.1, grabImage) # Grab image from camera and update def grabImage(): _, img = camera.read() updateImage(img) # Update image in interface def updateImage(image): _interfaces[0].setImage(OpenCVUtils.convertImageToML(image), minMaxValues = [0,255]) The startCapture function gets the camera from the cv2 object if not already available. Then, it calls the current MeVisLab network context and creates a timer that calls a grabImage function every 0.1 seconds.\nThe grabImage function reads an image from the camera and calls updateImage. The interface from the PythonImage module is used to set the image from the webcam. The MeVisLab OpenCVUtils converts the OpenCV image to the MeVisLab image format MLImage.\nNext, we define what happens if you click the Pause button. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n... # Stop capturing webcam def stopCapture(): ctx.removeTimers() ... As we started a timer in our network context that updates the image every 0.1 seconds, we just stop this timer and the camera is paused.\nIn the end, we need to release the camera whenever you close the Window of your macro module. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n... # Release camera in the end def releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None ... Again, the timers are removed, all interfaces are reset, and the camera is released. The light indicating webcam usage should turn off.\nOpening your macro module via double-click should now allow to start and pause your webcam video in MeVisLab. You can modify your internal network using a Convolution filter module or any other module available in MeVisLab for modifying the stream on-the-fly.\nSummary The PythonImage module allows to use Python for defining the image output. OpenCV can be used in MeVisLab via Python scripting. Images and videos from OpenCV can be used in MeVisLab networks. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","OpenCV","Python","Webcam","Macro","Macro modules","Global Macro"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/opencv/thirdpartyexample2/","title":"Example 2: Face Detection with OpenCV","summary":"Example 2: Face Detection with OpenCV Introduction This example uses the OpenCV webcam Python script and adds a basic face detection.\nInfo:\u0026nbsp; The Python code used in this example has been taken from Towards Data Science. Steps to Do Open Example 1 Add the macro module developed in Example 1 to your workspace.\n","content":"Example 2: Face Detection with OpenCV Introduction This example uses the OpenCV webcam Python script and adds a basic face detection.\nInfo:\u0026nbsp; The Python code used in this example has been taken from Towards Data Science. Steps to Do Open Example 1 Add the macro module developed in Example 1 to your workspace.\nDownload Trained Classifier XML File Initially, you need to download the trained classifier XML file. It is available in the OpenCV GitHub repository. Save the file somewhere and remember the path for later usage in Python.\nExtend Python File Right-click on your module , open the context menu, and select [ Related Files \u0026rarr; \u0026lt;YOUR_MODULE_NAME\u0026gt;.py ]. The text editor MATE opens. You can see the Python file of your module.\nYou have to load the previously downloaded XML file first. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# from mevis import * import cv2 import OpenCVUtils _interfaces = [] camera = None face_cascade = cv2.CascadeClassifier(\u0026#39;\u0026lt;YOUR_PATH\u0026gt;/haarcascade_frontalface_default.xml\u0026#39;) After loading the file, go to the previously implemented grabImage function and extend it as follows: \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\ndef grabImage(): _, img = camera.read() updateImage(img) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray, 1.1, 4) for (x, y, w, h) in faces: cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) # Display the output cv2.imshow(\u0026#39;img\u0026#39;, img) In the end, destroy all OpenCV windows in releaseCamera function. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\ndef releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None cv2.destroyAllWindows() Opening your macro module and pressing Start should now open your webcam stream and an additional OpenCV window, which shows a blue rectangle around a detected face.\nFace Detection in MeVisLab using OpenCV Summary This is just one example for using OpenCV in MeVisLab. You will find lots of other examples and tutorials online, we just wanted to show one possibility. Info:\u0026nbsp; You can download the Python file here ","tags":["Advanced","Tutorial","OpenCV","Python","Webcam","Face Detection"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/assimp/","title":"assimp","summary":"Asset-Importer-Lib (assimp) Introduction Assimp (Asset-Importer-Lib) is a library to load and process geometric scenes from various 3D data formats.\nThis chapter provides some examples of how 3D formats can be imported into MeVisLab. In general, you always need a SoSceneLoader module. The SoSceneLoader allows to load meshes as Open Inventor points/lines/triangles/faces using the Open Asset Import Library.\nSoSceneLoader You can also use the SoSceneWriter module to export your 3D scenes from MeVisLab in a number of output formats.\n","content":"Asset-Importer-Lib (assimp) Introduction Assimp (Asset-Importer-Lib) is a library to load and process geometric scenes from various 3D data formats.\nThis chapter provides some examples of how 3D formats can be imported into MeVisLab. In general, you always need a SoSceneLoader module. The SoSceneLoader allows to load meshes as Open Inventor points/lines/triangles/faces using the Open Asset Import Library.\nSoSceneLoader You can also use the SoSceneWriter module to export your 3D scenes from MeVisLab in a number of output formats.\nFile Formats The assimp library currently supports the following file formats:\n3D Manufacturing Format (.3mf) Collada (.dae, .xml) Blender (.blend) Biovision BVH (.bvh) 3D Studio Max 3DS (.3ds) 3D Studio Max ASE (.ase) glTF (.glTF) glTF2.0 (.glTF) KHR_lights_punctual ( 5.0 ) KHR_materials_pbrSpecularGlossiness ( 5.0 ) KHR_materials_unlit ( 5.0 ) KHR_texture_transform ( 5.1 under test ) FBX-Format, as ASCII and binary (.fbx) Stanford Polygon Library (.ply) AutoCAD DXF (.dxf) IFC-STEP (.ifc) Neutral File Format (.nff) Sense8 WorldToolkit (.nff) Valve Model (.smd, .vta) Quake I (.mdl) Quake II (.md2) Quake III (.md3) Quake 3 BSP (.pk3) RtCW (.mdc) Doom 3 (.md5mesh, .md5anim, .md5camera) DirectX X (.x) Quick3D (.q3o, .q3s) Raw Triangles (.raw) AC3D (.ac, .ac3d) Stereolithography (.stl) Autodesk DXF (.dxf) Irrlicht Mesh (.irrmesh, .xml) Irrlicht Scene (.irr, .xml) Object File Format ( .off ) Wavefront Object (.obj) Terragen Terrain ( .ter ) 3D GameStudio Model ( .mdl ) 3D GameStudio Terrain ( .hmp ) Ogre ( .mesh.xml, .skeleton.xml, .material ) OpenGEX-Fomat (.ogex) Milkshape 3D ( .ms3d ) LightWave Model ( .lwo ) LightWave Scene ( .lws ) Modo Model ( .lxo ) CharacterStudio Motion ( .csm ) Stanford Ply ( .ply ) TrueSpace (.cob, .scn) XGL-3D-Format (.xgl) ","tags":["Beginner","Tutorial","assimp","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/assimp/assimpexample1/","title":"Example 1: 3D Printing in MeVisLab","summary":"Example 1: 3D Printing in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This example uses the assimp library to load a 3D file and save the file as .stl for 3D printing.\nSteps to Do Develop Your Network Add the modules SoSceneLoader, SoBackground, and SoExaminerViewer to your workspace and connect them as seen below.\nExample Network Open the 3D File Select the file vtkCow.obj from MeVisLab demo data directory. Open SoExaminerViewer and inspect the scene. You will see a 3D cow.\n","content":"Example 1: 3D Printing in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This example uses the assimp library to load a 3D file and save the file as .stl for 3D printing.\nSteps to Do Develop Your Network Add the modules SoSceneLoader, SoBackground, and SoExaminerViewer to your workspace and connect them as seen below.\nExample Network Open the 3D File Select the file vtkCow.obj from MeVisLab demo data directory. Open SoExaminerViewer and inspect the scene. You will see a 3D cow.\nInfo:\u0026nbsp; In the case you cannot see the cow, it might be located outside your current camera location. Trigger the field rescanScene in the case the cow is not visible. Cow in SoExaminerViewer Add a SoSphere to the workspace and connect it to your viewer. Define the Radius of your sphere to 2 and inspect your viewer.\nCow and Sphere in SoExaminerViewer You can also define a material for your sphere but what we wanted to show is: You can use the loaded 3D files in MeVisLab Open Inventor scenes.\nCow and red Sphere in SoExaminerViewer Save Your Scene as .stl File for 3D Printing Add a SoSceneWriter module to your workspace. The SoExaminerViewer has a hidden output that can be shown on pressing SPACE . Connect the SoSceneWriter to the output.\nName your output .stl file and select Stl Ascii as output format, so that we can inspect the result afterward.\nSoSceneWriter Info:\u0026nbsp; The SoSceneWriter can save node color information when saving in Open Inventor (ASCII or binary) or in VRML format. The SoSceneWriter needs to be attached to a SoWEMRenderer that renders in ColorMode:NodeColor.\nThere are tools to convert from at least VRML to STL available for free.\nWrite your scene and open the resulting file in your preferred editor. As an alternative, you can also open the file in an .stl file reader like Microsoft 3D Viewer.\nMicrosoft 3D-Viewer Load the File Again For loading your .stl file, you can use a SoSceneLoader and a SoExaminerViewer.\nInfo:\u0026nbsp; More information about the .stl format can be found here SoSceneLoader Summary MeVisLab is able to load and write many different 3D file formats including .stl format for 3D printing. Open Inventor scenes can be saved by using a SoExaminerViewer together with a SoSceneWriter. ","tags":["Beginner","Tutorial","assimp","3D","3D Printing","stl"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/pytorch/","title":"PyTorch","summary":"PyTorch Introduction PyTorch is a machine learning framework based on the Torch library, used for applications such as Computer Vision and Natural Language Processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.\nA lot of AI frameworks can be used within MeVisLab. We currently do not provide a preintegrated AI framework though as we try to avoid compatibility issues, and AI frameworks are very fast-moving by nature.\n","content":"PyTorch Introduction PyTorch is a machine learning framework based on the Torch library, used for applications such as Computer Vision and Natural Language Processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.\nA lot of AI frameworks can be used within MeVisLab. We currently do not provide a preintegrated AI framework though as we try to avoid compatibility issues, and AI frameworks are very fast-moving by nature.\nMaybe also take a look at:\nTensorFlow Keras scikit-learn Attention:\u0026nbsp; We are not explaining PyTorch itself. These tutorials are examples for how to integrate and use PyTorch in MeVisLab. Detailed tutorials for using PyTorch can be found here. Available Tutorials Install PyTorch by Using the PythonPip Module The first example shows how to install torch and torchvision by using the MeVisLab module PythonPip. This module can be used to install Python packages not integrated into MeVisLab.\nUse Pretrained PyTorch Networks in MeVisLab In this example, we are using a pretrained network from torch.hub to generate an AI based image overlay of a brain parcellation map.\nSegment Persons in Webcam Videos The second tutorial adapts the Example 2: Face Detection with OpenCV to segment a person shown in a webcam stream. The network has been taken from torchvision.\n","tags":["Advanced","Tutorial","PyTorch","AI"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/pytorch/pytorchexample1/","title":"Example 1: Installing PyTorch Using the PythonPip Module","summary":"Example 1: Installing PyTorch using the PythonPip module Introduction The module PythonPip allows you to install additional Python packages to be used in MeVisLab.\nWarning:\u0026nbsp; You should not use the general Python pip command from a locally installed Python, because MeVisLab will not know these packages and they cannot be used in MeVisLab directly. The module either allows to install packages into the global MeVisLab installation directory, or into your defined user package. We will use the user package directory, because then the installed packages remain available in your packages even if you uninstall or update MeVisLab. In addition to that, no administrative rights are necessary if you did install MeVisLab for all users.\n","content":"Example 1: Installing PyTorch using the PythonPip module Introduction The module PythonPip allows you to install additional Python packages to be used in MeVisLab.\nWarning:\u0026nbsp; You should not use the general Python pip command from a locally installed Python, because MeVisLab will not know these packages and they cannot be used in MeVisLab directly. The module either allows to install packages into the global MeVisLab installation directory, or into your defined user package. We will use the user package directory, because then the installed packages remain available in your packages even if you uninstall or update MeVisLab. In addition to that, no administrative rights are necessary if you did install MeVisLab for all users.\nWarning:\u0026nbsp; Installing additional Python packages into MeVisLab by using the PythonPip module requires administrative rights if you do not install into a user package. In addition to that, the installed packages are removed when uninstalling MeVisLab. Steps to Do The PythonPip Module Add a PythonPip module to your workspace.\nPythonPip module Double-click the module and inspect the panel.\nPythonPip panel The panel shows all currently installed Python packages including their version and the MeVisLab package they are saved in. You can see a warning that the target package is set to read-only in the case you are selecting a MeVisLab package. Changing to one of your user packages (see Example 2.1: Package creation for details) makes the warning disappear.\nSelect user package Additional information:\u0026nbsp; Additional information on the PythonPip module can be found in Example 4: Install additional Python packages via PythonPip module. Install Torch and Torchvision For our tutorials, we need to install torch and torchvision. Enter torch torchvision into the Command textbox and press Install.\nInfo:\u0026nbsp; We are using the CPU version of PyTorch for our tutorials as we want them to be as accessible as possible. If you happen to have a large GPU capacity (and CUDA support), you can also use the GPU version. You can install the necessary packages by using the PyTorch documentation available here. Continuing with CUDA support: Command\ntorch torchvision --index-url https://download.pytorch.org/whl/cu117 Attention:\u0026nbsp; If you are behind a proxy server, you may have to set the HTTP_PROXY and HTTPS_PROXY environment variables to the hostname and port of your proxy. These are used by pip when accessing the internet.\nAlternatively, you can also add a parameter to pip install command: \u0026ndash;proxy https://proxy:port\nInstall torch and torchvision After clicking Install, the pip console output opens and you can follow the process of the installation.\nPython pip output After the installation has finished with exit code 0, you should see the new packages in the PythonPip module.\nPyTorch installed Summary PyTorch can be installed using the PythonPip module. There are different versions available (CPU and GPU) depending on the hardware that is used. Additional steps have to be taken depending on the version one wishes to install. The module displays newly installed packages as soon as the installation was successful. ","tags":["Advanced","Tutorial","PyTorch","Python","PythonPip","AI"],"section":"tutorials"},{"date":"1688083200","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/pytorch/pytorchexample2/","title":"Example 2: Brain Parcellation Using PyTorch","summary":"Example 2: Brain Parcellation Using PyTorch Introduction In this example, you are using a pretrained PyTorch deep learning model (HighRes3DNet) to perform a full brain parcellation.\nHighRes3DNet is a 3D residual network presented by Li et al. in On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task.\nSteps to Do Add a LocalImage module to your workspace and select the file MRI_Head.dcm. For PyTorch it is necessary to resample the data to a defined size. Add a Resample3D module to the LocalImage and open the panel. Change Keep Constant to Voxel Size and define Image Size as 176, 217, 160.\n","content":"Example 2: Brain Parcellation Using PyTorch Introduction In this example, you are using a pretrained PyTorch deep learning model (HighRes3DNet) to perform a full brain parcellation.\nHighRes3DNet is a 3D residual network presented by Li et al. in On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task.\nSteps to Do Add a LocalImage module to your workspace and select the file MRI_Head.dcm. For PyTorch it is necessary to resample the data to a defined size. Add a Resample3D module to the LocalImage and open the panel. Change Keep Constant to Voxel Size and define Image Size as 176, 217, 160.\nResample3D module .\nThe coordinates in PyTorch are also a little bit different than in MeVisLab; therefore, you have to rotate the image. Add an OrthoSwapFlip module and connect it to the Resample3D module. Change View to Other and set Orientation to YXZ. Also check Flip horizontal, Flip vertical, and Flip depth. Apply your changes.\nOrthoSwapFlip module .\nYou can use the Output Inspector to see the changes on the images after applying the resample and a swap or flip.\nOriginal Resample3D OrthoSwapFlip Add an OrthoView2D module to your network and save the .mlab file.\nOrthoView2D module .\nIntegrate PyTorch and Scripting For integrating PyTorch and Python scripting, we need a PythonImage module. Add it to your workspace. Right-click on the PythonImage module and select [ Grouping \u0026rarr; Add to new Group... ]. Right-click your new group and select [ Grouping \u0026rarr; Add to new Group... ]. Name your new local macro DemoAI, select a directory for your project, and leave all settings as default.\nOur new module does not provide any input or output.\nDemoAI local macro .\nAdding an Interface to the Local Macro Right-click the local macro and select [ Related Files \u0026rarr; DemoAI.script ]. MATE opens showing the .script file of our module. Add an input Field of type Image, an output Field using the internalName of the output of our PythonImage, and a Trigger to start the segmentation.\nYou should also already add a Python file in the Commands section.\nDemoAI.script\nInterface { Inputs { Field inputImage { type = Image } } Outputs { Field outImage { internalName = PythonImage.output0 } } Parameters { Field start { type = Trigger } } } Commands { source = $(LOCAL)/DemoAI.py } In MATE, right-click the Project Workspace and add a new file DemoAI.py to your project. The workspace now contains an empty Python file.\nProject Workspace .\nSwitch back to MeVisLab IDE, right-click the local macro, and select [ Reload Definition ]. Your new input and output interface is now available and you can connect images to your module.\nDemoAI local macro with interfaces .\nExtend Your Network We want to show the segmentation results as an overlay on the original image. Add a SoView2DOverlayMPR module and connect it to your DemoAI macro. Connect the output of the SoView2DOverlayMPR to a SoGroup. We also need a lookup table for the colors to be used for the overlay. We already prepared an .xml file you can simply use. Download the lut.xml file and save it in your current working directory of the project.\nAdd a LoadBase module and connect it to a SoMLLUT module. The SoMLLUT needs to be connected to the SoGroup, so that it is applied to our segmentation results.\nFinal network .\nInfo:\u0026nbsp; If your PC is equipped with less than 16GBs of RAM/working memory, we recommend to add a SubImage module between the OrthoSwapFlip and the Resample3D module. You should configure less slices in the z-direction to prevent your system from running out of memory.\nSubImage module .\nInspect the output of the LoadBase module in the Output Inspector to see if the lookup table has been loaded correctly.\nLUT in LoadBase .\nWrite Python Script You can now execute the pretrained PyTorch network on your image. Right-click the local macro and select [ Related Files \u0026rarr; DemoAI.script ]. The Python function is supposed to be called whenever the Trigger is touched.\nAdd the following code to your Commands section:\nDemoAI.script\nCommands { source = $(LOCAL)/DemoAI.py FieldListener start { command = onStart } } The FieldListener always calls the Python function onStart when the Trigger start is touched. We now need to implement the Python function. Right-click the command onStart and select [ Create Python Function \u0026#39;onStart\u0026#39; ].\nThe Python file opens automatically and the function is created.\nDemoAI.py\nimport torch def onStart(): # Step 1: Get input image image = ctx.field(\u0026#34;inputImage\u0026#34;).image() imageArray = image.getTile((0, 0, 0, 0, 0, 0), image.imageExtent()) inputImage = imageArray[0,0,0,:,:,:].astype(\u0026#34;float\u0026#34;) # Step 2: Normalize input image values = inputImage[inputImage \u0026gt; inputImage.mean()] inputImage = (inputImage - values.mean()) / values.std() # Step 3: Convert into torch tensor of size: [Batch, Channel, z, y, x] inputTensor = torch.Tensor(inputImage[None, None, :, :, :]) # Step 4: Load and prepare AI model device = torch.device(\u0026#34;cpu\u0026#34;) model = torch.hub.load(\u0026#34;fepegar/highresnet\u0026#34;, \u0026#34;highres3dnet\u0026#34;, pretrained=True, trust_repo=True) model.to(device).eval() output = model(inputTensor.to(device)) brainParcellationMap = output.argmax(dim=1, keepdim=True).cpu()[0] print(\u0026#39;...done.\u0026#39;) # Step 6: Set output image to module interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) interface.setImage(brainParcellationMap.numpy(), voxelToWorldMatrix=image.voxelToWorldMatrix()) Warning:\u0026nbsp; When executing your Python script for the first time, you will get a ScriptError message in the MeVisLab console. This only happens because the file of the trained network is missing and downloaded initially. You can ignore the message. Info:\u0026nbsp; The script uses the CPU; in the case you want to use CUDA, you can replace the line device = torch.device(\u0026ldquo;cpu\u0026rdquo;) with: device = torch.device(\u0026lsquo;cuda\u0026rsquo; if torch.cuda.is_available() else \u0026lsquo;cpu\u0026rsquo;) The function does the following:\nGet the input image of the module PythonImage Normalize the input image Convert the image into a torch tensor of size: [Batch, Channel, z, y, x] Load and prepare AI model Set output image to module output Execute the Segmentation Change the alpha value of your SoView2DOverlayMPR to have a better visualization of the results.\nSwitch back to the MeVisLab IDE and select your module DemoAI. In Module Inspector, click Trigger for start and wait a little bit until you can see the results.\nFinal result .\nWithout adding a SubImage, the segmentation results should look like this:\nResults .\nSummary Pretrained PyTorch networks can be used directly in MeVisLab via PythonImage module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","PyTorch","Python","PythonPip","AI"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/pytorch/pytorchexample3/","title":"Example 3: Segment Persons in Webcam Videos","summary":"Example 3: Segment Persons in Webcam Videos Introduction This tutorial is based on Example 2: Face Detection with OpenCV. You can reuse some of the scripts already developed in the other tutorial.\nSteps to Do Add the macro module developed in the previous example to your workspace.\nWebcamTest module Open the internal network of the module via middle mouse button and right-click on the tab of the workspace showing the internal network. Select Show Enclosing Folder.\n","content":"Example 3: Segment Persons in Webcam Videos Introduction This tutorial is based on Example 2: Face Detection with OpenCV. You can reuse some of the scripts already developed in the other tutorial.\nSteps to Do Add the macro module developed in the previous example to your workspace.\nWebcamTest module Open the internal network of the module via middle mouse button and right-click on the tab of the workspace showing the internal network. Select Show Enclosing Folder.\nShow Enclosing Folder The file browser opens showing the files of your macro module. Copy the .mlab file somewhere you can remember.\nCreate the Macro Module Open the the Project Wizard via [ File \u0026rarr; Run Project Wizard ] and select Macro Module. Click Run Wizard.\nProject Wizard Define the module properties as shown below, although you can choose your own name. Click Next.\nModule Properties Define the module properties and select the copied .mlab file. Make sure to add a Python file and click Next.\nMacro Module Properties Leave the module field reference as is and click Create. Close Project Wizard and select [ Extras \u0026rarr; Reload Module Database (Clear Cache) ].\nScript and Python Code Open the script file of the WebcamTest module and copy the contents to your new PyTorch module. The result should be something like this:\nPyTorchSegmentationExample.script\nInterface { Inputs {} Outputs {} Parameters {} } Commands { source = $(LOCAL)/PyTorchSegmentationExample.py } Window { h = 500 w = 500 destroyedCommand = releaseCamera initCommand = setupInterface Vertical { Horizontal { Button { title = Start command = startCapture } Button { title = Pause command = stopCapture } } Horizontal { expandX = True expandY = True Viewer View2D.self { type = SoRenderArea } } } } If you open the panel of your new module, you can see the UI elements added. You cannot use the buttons, because they require the Python function called. Copy the Python code to your new module, too.\nPyTorchSegmentationExample.py\n# from mevis import * import cv2 import OpenCVUtils _interfaces = [] camera = None face_cascade = cv2.CascadeClassifier(\u0026#39;C:/tmp/haarcascade_frontalface_default.xml\u0026#39;) # Set up the interface for PythonImage module def setupInterface(): global _interfaces _interfaces = [] interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) _interfaces.append(interface) # Grab image from camera and update def grabImage(): _, img = camera.read() updateImage(img) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray, 1.1, 4) for (x, y, w, h) in faces: cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) # Display the output cv2.imshow(\u0026#39;img\u0026#39;, img) # Update image in interface def updateImage(image): _interfaces[0].setImage(OpenCVUtils.convertImageToML(image), minMaxValues = [0,255]) # Start capturing webcam def startCapture(): global camera if not camera: camera = cv2.VideoCapture(0) ctx.callWithInterval(0.1, grabImage) # Stop capturing webcam def stopCapture(): ctx.removeTimers() # Release camera in the end def releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None cv2.destroyAllWindows() You should now have the complete functionality of the Example 2: Face Detection with OpenCV.\nAdapt the Network For PyTorch, we require some additional modules in our network. Open the internal network of your module and add another PythonImage module. Connect a Resample3D and an ImagePropertyConvert module.\nIn Resample3D module, define the Image Size 693, 520, 1. Change VoxelSize for all dimensions to 1.\nResample3D Open the Panel of the ImagePropertyConvert module and check World Matrix.\nImagePropertyConvert Then, add a SoView2DOverlayMPR module and connect it to the ImagePropertyConvert and the View2D. Change Blend Mode to Blend, Alpha to something between 0 and 1, and define a color for the overlay.\nSoView2DOverlayMPR Save the internal network.\nRemove OpenCV-specific Code We want to use PyTorch for segmentation; therefore, you need to add all necessary imports.\nPyTorchSegmentationExample.py\nimport cv2 import OpenCVUtils from torchvision.io.image import read_image from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights from torchvision.transforms.functional import to_pil_image import torch Additionally, remove the face_cascade parameter from your Python code. This was necessary for detecting a face in OpenCV and is not necessary anymore in PyTorch. The only parameters you need here are:\nPyTorchSegmentationExample.py\n_interfaces = [] camera = None You can also remove the OpenCV-specific lines in grabImage. The function should look like this now:\nPyTorchSegmentationExample.py\n# Grab image from camera and update def grabImage(): _, img = camera.read() updateImage(img) Adapt the function releaseCamera and remove the line cv2.destroyAllWindows().\nPyTorchSegmentationExample.py\n# Release camera in the end def releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None Implement PyTorch Segmentation The first thing we need is a function for starting the camera. It closes the previous segmentation and calls the existing function startCapture.\nPyTorchSegmentationExample.py\ndef startWebcam(): # Close previous segmentation ctx.module(\u0026#34;PythonImage1\u0026#34;).call(\u0026#34;getInterface\u0026#34;).unsetImage() # Start webcam startCapture() As this function is not called in our user interface, we need to update the .script file. Change the first Button to below script:\nPyTorchSegmentationExample.script\nButton { title = \u0026#34;Start Webcam\u0026#34; command = startWebcam } Now, your new function startWebcam is called whenever touching the left button. As a next step, define a Python function segmentSnapshot. We are using a pretrained network from Torchvision. In the case you want to use other PyTorch possibilities, you can find lots of examples on their website.\nPyTorchSegmentationExample.py\ndef segmentSnapshot(): # Step 1: Get image from webcam capture stopCapture() inImage = ctx.field(\u0026#34;PythonImage.output0\u0026#34;).image() img = inImage.getTile((0,0,0,0,0,0), inImage.imageExtent())[0,0,:,0,:,:] # Step 2: Convert image into torch tensor img = torch.Tensor(img).type(torch.uint8) # Step 3: Initialize model with the best available weights weights = FCN_ResNet50_Weights.DEFAULT model = fcn_resnet50(weights=weights) model.eval() # Step 4: Initialize the inference transforms preprocess = weights.transforms() # Step 5: Apply inference preprocessing transforms batch = preprocess(img).unsqueeze(0) # Step 6: Use the model to segment persons in snapshot prediction = model(batch)[\u0026#34;out\u0026#34;] normalized_masks = prediction.softmax(dim=1) class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\u0026#34;categories\u0026#34;])} mask = normalized_masks[0, class_to_idx[\u0026#34;person\u0026#34;]] # Step 7: Set output image to module interface = ctx.module(\u0026#34;PythonImage1\u0026#34;).call(\u0026#34;getInterface\u0026#34;) interface.setImage(mask.detach().numpy()) # Step 8: Resize network output to original image size origImageSize = inImage.imageExtent() ctx.field(\u0026#34;Resample3D.imageSize\u0026#34;).value = (origImageSize[0], origImageSize[1], origImageSize[2]) In order to call this function, we have to change the command of the right button by adapting the .script file.\nPyTorchSegmentationExample.script\nButton { title = \u0026#34;Segment Snapshot\u0026#34; command = segmentSnapshot } In step 5, we selected the class person. Whenever you click Segment Snapshot, PyTorch will try to segment all persons in the video.\nAdditional information:\u0026nbsp; The following classes are available:\naeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor The final result of the segmentation should be a semitransparent red overlay of the persons segmented in your webcam stream.\nFinal Segmentation result Summary You can install additional Python AI packages by using the PythonPip module. PyTorch trained networks can be used directly in MeVisLab via PythonImage module. You can integrate AI algorithms into your MeVisLab networks. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","PyTorch","Python","PythonPip","AI","Segmentation","WebCam"],"section":"tutorials"},{"date":"1762992000","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/monai/","title":"MONAI","summary":"MONAI Introduction MONAI (Medical Open Network for AI) is an open-source framework built on PyTorch designed for developing and deploying AI models in medical imaging.\nCreated by NVIDIA and the Linux Foundation, it provides specialized tools for handling medical data formats like DICOM and NIfTI, along with advanced preprocessing, augmentation, and 3D image analysis capabilities.\nMONAI includes ready-to-use deep learning models (such as UNet and SegResNet) and utilities for segmentation, classification, and image registration. It supports distributed GPU training and ensures reproducible research workflows.\n","content":"MONAI Introduction MONAI (Medical Open Network for AI) is an open-source framework built on PyTorch designed for developing and deploying AI models in medical imaging.\nCreated by NVIDIA and the Linux Foundation, it provides specialized tools for handling medical data formats like DICOM and NIfTI, along with advanced preprocessing, augmentation, and 3D image analysis capabilities.\nMONAI includes ready-to-use deep learning models (such as UNet and SegResNet) and utilities for segmentation, classification, and image registration. It supports distributed GPU training and ensures reproducible research workflows.\nAvailable Tutorials Example 1: Install MONAI using PythonPip module Example 2: Applying a spleen segmentation model from MONAI in MeVisLab ","tags":["Advanced","Tutorial","MONAI","AI"],"section":"tutorials"},{"date":"1762992000","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/monai/monaiexample1/","title":"Example 1: Installing MONAI Using the PythonPip Module","summary":"Example 1: Installing MONAI Using the PythonPip Module Introduction With the PythonPip module, you can import additional Python libraries into MeVisLab.\nSteps to Do Install PyTorch As MONAI requires PyTorch, install it by using the PythonPip module as described here.\nInstall MONAI After installing torch and torchvision, we install MONAI.\nFor installing MONAI enter \u0026quot;monai\u0026quot; into the Command textbox and press Install.\nInstall MONAI ","content":"Example 1: Installing MONAI Using the PythonPip Module Introduction With the PythonPip module, you can import additional Python libraries into MeVisLab.\nSteps to Do Install PyTorch As MONAI requires PyTorch, install it by using the PythonPip module as described here.\nInstall MONAI After installing torch and torchvision, we install MONAI.\nFor installing MONAI enter \u0026quot;monai\u0026quot; into the Command textbox and press Install.\nInstall MONAI After clicking Install, the pip console output opens and you can follow the process of the installation.\nAttention:\u0026nbsp; If you are behind a proxy server, you may have to set the HTTP_PROXY and HTTPS_PROXY environment variables to the hostname and port of your proxy. These are used by pip when accessing the internet.\nAlternatively, you can also add a parameter to pip install command: \u0026ndash;proxy https://proxy:port\nPythonPip MONAI After the installation has finished with exit code 0, you should see the new packages in the PythonPip module.\nMONAI installed Summary MONAI can be installed and directly used in MeVisLab by using the PythonPip module. ","tags":["Advanced","Tutorial","MONAI","Python","PythonPip","AI"],"section":"tutorials"},{"date":"1762992000","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/monai/monaiexample2/","title":"Example 2: Applying a Spleen Segmentation Model from MONAI in MeVisLab","summary":"Example 2: Applying a Spleen Segmentation Model from MONAI in MeVisLab Introduction In the following, we will perform a spleen segmentation using a model from the MONAI Model Zoo. The MONAI Model Zoo is a collection of pretrained models for medical imaging, offering standardized bundles for tasks like segmentation, classification, and detection across MRI, CT, and pathology data, all built for easy use and reproducibility within the MONAI framework. Further information and the required files can be found here.\n","content":"Example 2: Applying a Spleen Segmentation Model from MONAI in MeVisLab Introduction In the following, we will perform a spleen segmentation using a model from the MONAI Model Zoo. The MONAI Model Zoo is a collection of pretrained models for medical imaging, offering standardized bundles for tasks like segmentation, classification, and detection across MRI, CT, and pathology data, all built for easy use and reproducibility within the MONAI framework. Further information and the required files can be found here.\nThis example shows how to use the model for Spleen CT Segmentation directly in MeVisLab.\nSteps to Do Download Necessary Files Create a folder named spleen_ct_segmentation somewhere on your system. Inside this folder, create two subfolders, one named configs and another one named models, and remember their paths.\nDirectory Structure .\nDownload all config files from MONAI-Model-Zoo and save them in your local configs directory.\nDownload model files from NVIDIA Download Server and save it in your local models directory.\nAdditional information:\u0026nbsp; The path to the latest model .pt file can be found in large_files.yml. Download Example Images The recommended CT images used for training the algorithm can be found here.\nCreate a Macro Module and Add Inputs and Outputs Add a PythonImage module and save the network as MONAISpleenSegmentation.mlab.\nPythonImage module .\nNow, right-click on the PythonImage module, select [ “Grouping” \u0026rarr; Add to new Group ], and name the group MONAIDemo.\nRight-click on the group\u0026rsquo;s name and choose Convert to Local Macro using the same name.\nOur new module does not provide any input or output.\nLocal Macro Module MONAIDemo Right-click on the macro module and select [ Related Files \u0026rarr; MONAIDemo.script ].\nAdd the following code into the .script file and save.\nMONAIDemo.script\nInterface { Inputs { Field inputImage { type = Image } } Outputs { Field outImage { internalName = PythonImage.output0 } } Parameters { Field start { type = Trigger } } } If you now reload your module in MeVisLab, you can see the new input and output.\nMONAIDemo with input and output Add a Commands section to your .script file.\nMONAIDemo.script\n... Commands { source = $(LOCAL)/MONAIDemo.py } ... Right-click on the MONAIDemo.py and select [ Open File $(LOCAL)/MONAIDemo.py ]. An empty Python file is created and opens automatically. Save the empty Python file.\nCreate the Network for the Segmentation Right-click on the macro module and select [ Related Files \u0026rarr; MONAIDemo.mlab ]. Create the network seen below.\nMonaiDemo Network Fields of the internal network can be left with default values; we will change them later.\nThe left part defines actions executed on the input image, the right part defines what shall happen on the output after the MONAI segmentation has been done. A detailed description will be provided later.\nOpen your .script file via right-click on the macro module and select [ Related Files \u0026rarr; MONAIDemo.script ].\nDefine your input image field to reuse the internal name of the left input of the Resample3D module.\nMONAIDemo.script\nInterface { Inputs { Field inputImage { internalName = Resample3D.input0 } } ... } If you now open the internal network of your macro module, you can see that the input image is connected to the input of the Resample3D module.\nMonaiDemo Internal Network Again, open the .script file and change the internal name of your outImage field to reuse the field Resample3D1.output0.\nMONAIDemo.script\nInterface { ... Outputs { Field outImage { internalName = Resample3D1.output0 } } ... } If you now open the internal network of your macro module, you can see that the output image is connected to the output of the Resample3D1 module.\nMonaiDemo Internal Network Adapt Input Image to MONAI Parameters from Training The model has been trained for strictly defined assumptions for the input image. All values can normally be found in the inference.json file in your configs directory.\nUse the itkImageFileReader module to load the file Task09_Spleen/Task09_SpleenimagesTr/spleen_7.nii.gz from dowloaded example patients. The Output Inspector shows the image and additional information about the size.\nWe can see that the image size is 512 x 512 x 114 and the voxel size is 0.9766 x 0.9766 x 2.5.\nOutput Inspector Connect the module to your local macro module MonaiDemo. The result of the segmentation shall be visualized as a semitransparent overlay on your original image.\nAdd a SoView2DOverlay and a View2D module and connect them to your local macro module MonaiDemo.\nFinal network The Spleen CT Segmentation network expects images having a defined voxel size of 1.5 x 1.5 x 2. We want to define these values via fields in the Module Inspector.\nOpen the .script file and add the fields start and voxelSize to your local macro module MonaiDemo:\nMONAIDemo.script\nInterface { ... Parameters { Field start { type = Trigger } Field voxelSize { internalName = Resample3D.voxelSize } } ... } If you reload your module now, we can set the voxel size to use for the segmentation directly in our macro module MonaiDemo. Additionally, we can trigger a start function for running the segmentation. This is implemented later.\nVoxel Size If you select the output field of the Resample3D module in the internal network, you can see the extent of the currently opened image after changing the voxel size to 1.5 x 1.5 x 2. It shows 333 x 333 x 143.\nOriginal Image Size The algorithm expects image sizes of 160 x 160 x 160. We add this expected size of the image to our macro module in the same way.\nOpen the .script file and add the following fields to your local macro module MonaiDemo:\nMONAIDemo.script\nInterface { ... Parameters { ... Field sizeX { type = Int } Field sizeY { type = Int } Field sizeZ { type = Int } ... } ... } Reload your macro module and enter the following values for your new fields:\nsizeX = 160 sizeY = 160 sizeZ = 160 Next, we change the gray values of the image, because the algorithm has been trained on values between -57 and 164. Again, the values can be found in the inference.json file in your configs directory.\nOpen the .script file and add the following fields to your local macro module MonaiDemo:\nMONAIDemo.script\nInterface { ... Parameters { ... Field thresholdMin { internalName = IntervalThreshold.threshMin } Field thresholdMax { internalName = IntervalThreshold.threshMax } ... } ... } As already done before, we can now defined the threshold values for our module via Module Inspector. Set the following:\nthresholdMin = -57 thresholdMax = 164 As defined in the inference.json file in your configs directory, the gray values in the image must be between 0 and 1.\nOpen the .script file and add the following fields to your local macro module MonaiDemo:\nMONAIDemo.script\nInterface { ... Parameters { ... Field scaleMin { internalName = Scale.outputMin } Field scaleMax { internalName = Scale.outputMax } ... } ... } Set the following:\nscaleMin = 0 scaleMax = 1 The algorithm expects NumPy images. NumPy uses the order Z, Y, X, other than MeVisLab. We are using X, Y, Z. The image needs to be transformed.\nOpen the panel of the SwapFlipDimensions module and select X as Axis 1 and Z as Axis 2.\nSwapFlipDimensions After the algorithm has finished, we have to flip the images back to the original order. Open the panel of the SwapFlipDimensions1 module and select X as Axis 1 and Z as Axis 2.\nFinally, we want to show the results of the algorithm as a semitransparent overlay on the image. Open the panel of the View2DOverlay and define the following settings:\nBlend Mode: Blend Alpha Factor: 0.5 Base Color: red View2DOverlay Field Listeners We add some Field Listeners to our Commands section of the .script file. They are necessary to react on changes the user makes on the fields of our module.\nMONAIDemo.script\n... Commands { source = $(LOCAL)/MONAIDemo.py FieldListener start { command = onStart } FieldListener sizeX { command = _sizeChanged } FieldListener sizeY { command = _sizeChanged } FieldListener sizeZ { command = _sizeChanged } FieldListener inputImage { command = _setDefaultValues } } ... If the user touches the trigger start, a Python function onStart will be executed. Whenever the size of our image is changed, we call a function called _sizeChanged and if the input image changes, we want to reset the module to its default values.\nPython Scripting The next step is to write our Python code.\nRight-click MONAIDemo.py in Commands section line source. MATE opens showing the .py file of our module.\nInsert the following code:\nMONAIDemo.py\nimport torch import numpy as np import mevislab from monai.bundle import load_bundle_config # Paths MODEL_DIR = r\u0026#34;C:\\tmp\\spleen_ct_segmentation\u0026#34; MODEL_PATH = MODEL_DIR + r\u0026#34;\\models\\model_spleen_ct_segmentation_v1.pt\u0026#34; TRAIN_JSON = MODEL_DIR + r\u0026#34;\\configs\\train.json\u0026#34; # using CPU or Cuda DEVICE = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) def onStart(): print(\u0026#34;\\n--- Start ---\u0026#34;) def _setDefaultValues(): print(\u0026#34;\\n--- Reset ---\u0026#34;) def _getImage(): print(\u0026#34;\\n--- Get Image ---\u0026#34;) def _sizeChanged(): print(\u0026#34;\\n--- Size Changed ---\u0026#34;) These functions should be enough to run the module. You can try them by changing the input image of our module, by changing any of the size values in Module Inspector, or by clicking start.\nLet\u0026rsquo;s implement the _getImage function first:\nMONAIDemo.py\n... def _getImage(): if ctx.field(\u0026#34;SwapFlipDimensions.output0\u0026#34;).isValid(): # Get image after all modifications have been done image = ctx.field(\u0026#34;SwapFlipDimensions.output0\u0026#34;).image() return image else: return None ... We want to use the image that has been modified according to our pretrained network requirements discussed above. We use the output image of the SwapFlipDimensions module when clicking start.\nMONAIDemo.py\n... def onStart(): print(\u0026#34;\\n--- Start ---\u0026#34;) try: inputImage = _getImage() if inputImage: imageArray = inputImage.getTile( (0, 0, 0, 0, 0, 0), inputImage.imageExtent() ) # We only need x, y and z-dimensions image = imageArray[0, 0, 0, :, :, :] print(f\u0026#34;Using image {image.shape}\u0026#34;) # prepare tensor inputTensor = torch.tensor(image[None, None, :, :, :]).to(DEVICE) print(f\u0026#34; Tensorform: {tuple(inputTensor.shape)}\u0026#34;) # Load Bundle-Configuration parser = load_bundle_config(MODEL_DIR, \u0026#34;train.json\u0026#34;) # Create network from train.json model = parser.get_parsed_content(\u0026#34;network_def\u0026#34;) model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE)) model.to(DEVICE) model.eval() print(\u0026#34;Model loaded and initialized.\u0026#34;) # Inference with torch.no_grad(): output = model(inputTensor) prediction = output.argmax(dim=1, keepdim=True).cpu().numpy()[0, 0] print(\u0026#34;Inference done.\u0026#34;) # Result back into MeVisLab interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) interface.setImage( prediction, voxelToWorldMatrix=inputImage.voxelToWorldMatrix() ) print(\u0026#34;--- Segmentation done ---\\n\u0026#34;) except Exception as e: print(\u0026#34;Error:\u0026#34;, e) import traceback traceback.print_exc() ... This function now already calculates the segmentation using the MONAI model. The problem is that it may happen that our subimage with the size 160 x 160 x 160 is located somewhere in our original image where no spleen is visible.\nWe have to calculate a bounding box in our ROISelect module and need to be able to move this bounding box to the correct location.\nMONAIDemo.py\n... def _sizeChanged(field: \u0026#34;mevislab.MLABField\u0026#34;): if ctx.field(\u0026#34;Resample3D.output0\u0026#34;).isValid(): voxelSizeImage = ctx.field(\u0026#34;Resample3D.output0\u0026#34;).image() # Get the size of this image voxelSizeImageExtent = voxelSizeImage.imageExtent() # Calculate region of interest by defining start point and size roiStartX = voxelSizeImageExtent[0] - ctx.field(\u0026#34;sizeX\u0026#34;).value roiStartY = voxelSizeImageExtent[1] - ctx.field(\u0026#34;sizeY\u0026#34;).value roiStartZ = voxelSizeImageExtent[2] - ctx.field(\u0026#34;sizeZ\u0026#34;).value ctx.field(\u0026#34;ROISelect.startVoxelX\u0026#34;).value = roiStartX ctx.field(\u0026#34;ROISelect.startVoxelY\u0026#34;).value = roiStartY ctx.field(\u0026#34;ROISelect.startVoxelZ\u0026#34;).value = roiStartZ # Subtract 1 because the voxel values start with 0 ctx.field(\u0026#34;ROISelect.endVoxelX\u0026#34;).value = voxelSizeImageExtent[0] - 1 ctx.field(\u0026#34;ROISelect.endVoxelY\u0026#34;).value = voxelSizeImageExtent[1] - 1 ctx.field(\u0026#34;ROISelect.endVoxelZ\u0026#34;).value = voxelSizeImageExtent[2] - 1 ... Whenever our size fields are modified, the bounding box is recalculated using the size of the given image and the values of the sizes defined by the user. The calculated bounding box is not positioned. This needs to be done manually, if necessary.\nOpen the .script file and add a Window section. In this window, we reuse the panel of the ROISelect module to manually correct the location of our calculated bounding box.\nMONAIDemo.script\n... Window { height = 100 width = 100 Category { Viewer ROISelect.scene.self { type = SoRenderArea expandX = True expandY = True } } } ... If you now open the panel of our MONAIDemo module, we can manually move the box in all three dimensions.\nMONAIDemo panel .\nBack to Python, we now need to reset our module to default in the case the input image changes. This also removes previous segmentations from the PythonImage module.\nMONAIDemo.py\n... def _setDefaultValues(): ctx.field(\u0026#34;voxelSize\u0026#34;).value = [1.5, 1.5, 2] ctx.field(\u0026#34;sizeX\u0026#34;).value = 160 ctx.field(\u0026#34;sizeY\u0026#34;).value = 160 ctx.field(\u0026#34;sizeZ\u0026#34;).value = 160 ctx.field(\u0026#34;thresholdMin\u0026#34;).value = -57 ctx.field(\u0026#34;thresholdMax\u0026#34;).value = 164 ctx.field(\u0026#34;scaleMin\u0026#34;).value = 0 ctx.field(\u0026#34;scaleMax\u0026#34;).value = 1 interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) interface.unsetImage() ... Execute the Segmentation If you now load an image using the itkImageFileReader module, you can manually adapt your bounding box to include the spleen and start segmentation.\nThe results are shown as a semitransparent overlay.\nSegmentation result .\nYou can also use the other examples from MONAI Model Zoo the same way, just make sure to apply the necessary changes on the input images like size, voxel size, and other parameters defined in the inference.json file of the model.\nSummary Pretrained MONAI networks can be used directly in MeVisLab via PythonImage module. The general principles are always the same for all models. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","MONAI","Python","PythonPip","AI"],"section":"tutorials"},{"date":"1684972800","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/matplotlib/","title":"Matplotlib","summary":"Matplotlib Matplotlib, introduced by John Hunter in 2002 and initially released in 2003, is a comprehensive data visualization library in Python. It is widely used in the scientific world as it is easy to grasp for beginners and provides high quality plots and images that are widely customizable.\nInfo:\u0026nbsp; The documentation on Matplotlib along with general examples, cheat sheets, and a starting guide can be found here. As MeVisLab supports the integration of Python scripts, e.g., for test automation, Matplotlib can be used to visualize any data you might want to see. And as it is directly integrated into MeVisLab, you don\u0026rsquo;t have to install it (via PythonPip module) first.\n","content":"Matplotlib Matplotlib, introduced by John Hunter in 2002 and initially released in 2003, is a comprehensive data visualization library in Python. It is widely used in the scientific world as it is easy to grasp for beginners and provides high quality plots and images that are widely customizable.\nInfo:\u0026nbsp; The documentation on Matplotlib along with general examples, cheat sheets, and a starting guide can be found here. As MeVisLab supports the integration of Python scripts, e.g., for test automation, Matplotlib can be used to visualize any data you might want to see. And as it is directly integrated into MeVisLab, you don\u0026rsquo;t have to install it (via PythonPip module) first.\nIn the following tutorial pages on Matplotlib, you will be shown how to create a module in MeVisLab that helps you plot greyscale distributions of single slices or defined sequences of slices of a DICOM image and layer the grayscale distributions of two chosen slices for comparison.\nThe module that is adapted during the tutorials is set up in the Example 1: Module Setup tutorial. The panel and two-dimensional plotting functionality is added in Example 2: 2D Plotting. In Example 3: Slice Comparison, the comparison between two chosen slices is enabled by overlaying their grayscale distributions. Example 4: 3D Plotting adds an additional three-dimensional plotting functionality to the panel. Check:\u0026nbsp; Notice that for the Matplotlib tutorials, the previous tutorial always works as a foundation for the following one. ","tags":["Advanced","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685059200","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/matplotlib/modulesetup/","title":"Example 1: Module Setup","summary":"Example 1: Module Setup Introduction To be able to access the data needed for our grayscale distribution plots, we need a network consisting of a module that imports DICOM data, a module that differentiates between slices, and another module that ouputs histogram data.\nSteps to Do Open up your MeVisLab workspace and add the modules LocalImage, SubImage, and Histogram to it. Connect the output of LocalImage to the input of SubImage, and the output of SubImage with the input of Histogram. If you feel like using a shortcut, you can also download the base network below and open it in your MeVisLab.\n","content":"Example 1: Module Setup Introduction To be able to access the data needed for our grayscale distribution plots, we need a network consisting of a module that imports DICOM data, a module that differentiates between slices, and another module that ouputs histogram data.\nSteps to Do Open up your MeVisLab workspace and add the modules LocalImage, SubImage, and Histogram to it. Connect the output of LocalImage to the input of SubImage, and the output of SubImage with the input of Histogram. If you feel like using a shortcut, you can also download the base network below and open it in your MeVisLab.\nYour finished network should look like this:\n\u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Excursion on the Concept Behind Modules To be able to build on the foundation we just set, it can be useful to understand how modules are conceptualized: You will have noticed how for every module, a panel will pop up if you double-click it. The modules panel contains all of its functional parameters and enables you, as the user, to change them within a graphical user interface (GUI). We will do something similar later on.\nBut where and how is a module panel created? To answer this question, please close the module panel and right-click on the module. A context menu will open, click on \u0026ldquo;Related Files\u0026rdquo;.\nAs you can see, each module has a .script and a .py file named like the module itself:\nThe .script file is where the appearance and structure of the module panel as well as their commands are declared. The .py file contains Python functions and methods, which are triggered by their referenced commands within the .script file. Some modules also reference an .mlab file, which usually contains their internal network as the module is a macro.\nLet\u0026rsquo;s continue with our module setup now:\nIf your network is ready, group it by right-clicking on your group\u0026rsquo;s title and select \u0026ldquo;Grouping\u0026rdquo;, then \u0026ldquo;Add To A New Group\u0026rdquo;. Afterward, convert your grouped network into a macro module.\nInfo:\u0026nbsp; Information on how to convert groups into macros can be found here. Depending on whether you like to reuse your projects in other workspaces, it can make sense to convert them. We\u0026rsquo;d recommend to do so.\nNow open the .script file of your newly created macro through the context menu. The file will be opened within MATE (MeVisLab Advanced Text Editor). Add this short piece of code into your .script file and make sure that the .script and the .py are named exactly the same as the module they are created for.\nBaseNetwork.script\nCommands{ source = $(LOCAL)/BaseNetwork.py } Click the \u0026ldquo;Reload\u0026rdquo; button, which is located above the script for the .py file to be added into the module definition folder, then open it using the \u0026ldquo;Files\u0026rdquo; button on the same bar as demonstrated below: Info:\u0026nbsp; The MDL Reference is a very handy tool for this and certainly also for following projects. You have now created your own module and enabled the .script file (hence the GUI or panel later on) to access functions and methods written in the .py file.\nSummary Modules are defined by the contents within their definition folder. A module consists of of a .script file containing the panel configuration and a .py file containing functions that are accessed via the panel and provide functionalities (interacting with the parameters of modules in the macros internal network). A macro module\u0026rsquo;s panel can access parameters of its internal modules. The panel is layouted using MDL. ","tags":["Beginner","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685404800","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/matplotlib/2dplotting/","title":"Example 2: 2D Plotting","summary":"Example 2: 2D Plotting Introduction In this tutorial, we will equip the macro module we created in the previous tutorial with a responsive and interactable panel to plot grayscale distributions of single slices as well as defined sequences of slices in 2D.\nSteps to Do Open the module definition folder of your macro module and the related .script file in MATE. Then, activate the preview as shown below:\n","content":"Example 2: 2D Plotting Introduction In this tutorial, we will equip the macro module we created in the previous tutorial with a responsive and interactable panel to plot grayscale distributions of single slices as well as defined sequences of slices in 2D.\nSteps to Do Open the module definition folder of your macro module and the related .script file in MATE. Then, activate the preview as shown below:\nDrag the small preview window to the bottom right corner of your window where it does not bother you. We will now be adding contents to be displayed there.\nAdding the following code to your .script file will open a panel window if the macro module is clicked. This new panel window contains a Matplotlib canvas where the plots will be displayed later on as well as two prepared boxes that we will add functions to in the next step.\nBaseNetwork.script\nWindow { Category { Horizontal { Vertical { expandY = True expandX = False Box { title= \u0026#34;Single Slice\u0026#34; } Box { title = \u0026#34;Sequence\u0026#34; } Empty { expandY = True } } Box { MatplotlibCanvas { expandY = True expandX = True name = canvas useToolBar = True } expandY = True expandX = True } } } } Letting a box expand on the x- or y-axis or adding an empty object do so contributes to the panel looking a certain way and helps the positioning of the elements. You can also try to vary the positioning by adding or removing \u0026ldquo;expand\u0026rdquo; statements or moving boxes from a vertical to a horizontal alignment. Hover over the boxes in the preview to explore the concept.\nInfo:\u0026nbsp; You can click and hold onto a box to move it within the preview. Your code will automatically be changed according to the new positioning. Now, we need to identify which module parameters we want to be able to access from the panel of our macro:\nTo plot a slice or a defined sequence of slices, we need to be able to set a start and an end. Go back into your MeVisLab workspace, right-click your BaseNetwork module and choose \u0026ldquo;Show Internal Network\u0026rdquo;.\nThe `SubImage` module provides the option to set sequences of slices. The starting and ending slices of the sequence can be set in the module panel. Info:\u0026nbsp; To find out what the parameters are called, what type of values they contain and receive, and what they refer to, you can right-click on them within the panel. We now know that we will need SubImage.z and SubImage.sz to define the start and end of a sequence. But there are a few other module parameters that must be set beforehand to make sure the data we extract to plot later is compareable and correct.\nTo do so, we will be defining a \u0026ldquo;setDefaults\u0026rdquo; function for our module. Open the .py file and add the code below.\nBaseNetwork.py\ndef setDefaults(): ctx.field(\u0026#34;SubImage.fullSize\u0026#34;).touch() ctx.field(\u0026#34;SubImage.autoApply\u0026#34;).value = True ctx.field(\u0026#34;Histogram.updateMode\u0026#34;).value = \u0026#34;AutoUpdate\u0026#34; ctx.field(\u0026#34;Histogram.xRange\u0026#34;).value = \u0026#34;Dynamic Min/Max\u0026#34; ctx.field(\u0026#34;Histogram.useZeroAsBinCenter\u0026#34;).value = False ctx.field(\u0026#34;Histogram.binSize\u0026#34;).value = 5.0 ctx.field(\u0026#34;Histogram.backgroundValue\u0026#34;).value = False ctx.field(\u0026#34;Histogram.curveType\u0026#34;).value = \u0026#34;Area\u0026#34; ctx.field(\u0026#34;Histogram.useStepFunction\u0026#34;).value = True ctx.field(\u0026#34;Histogram.curveStyle\u0026#34;).value = 7 As it is also incredibly important that the values of the parameters we are referencing are regularly updated, we will be setting some global values containing those values.\nBaseNetwork.py\nstartSlice = None endSlice = None bins = None def updateSlices(): global startSlice, endSlice, bins startSlice = int(ctx.field(\u0026#34;SubImage.z\u0026#34;).value) endSlice = int(ctx.field(\u0026#34;SubImage.sz\u0026#34;).value) bins = ctx.field(\u0026#34;Histogram.binSize\u0026#34;).value Make sure that the variable declarations as \u0026ldquo;None\u0026rdquo; are put above the \u0026ldquo;setDefaults\u0026rdquo; function and add the execution of the \u0026ldquo;updateSlices()\u0026rdquo; function into the \u0026ldquo;setDefaults\u0026rdquo; function, like so:\nBaseNetwork.py\ndef setDefaults(): ctx.field(\u0026#34;Histogram.xRange\u0026#34;).value = \u0026#34;Dynamic Min/Max\u0026#34; ctx.field(\u0026#34;Histogram.useZeroAsBinCenter\u0026#34;).value = False ctx.field(\u0026#34;Histogram.binSize\u0026#34;).value = 5.0 ctx.field(\u0026#34;Histogram.backgroundValue\u0026#34;).value = False ctx.field(\u0026#34;Histogram.curveType\u0026#34;).value = \u0026#34;Area\u0026#34; ctx.field(\u0026#34;Histogram.useStepFunction\u0026#34;).value = True ctx.field(\u0026#34;Histogram.curveStyle\u0026#34;).value = 7 ctx.field(\u0026#34;SubImage.fullSize\u0026#34;).touch() ctx.field(\u0026#34;SubImage.autoApply\u0026#34;).value = True ctx.field(\u0026#34;Histogram.updateMode\u0026#34;).value = \u0026#34;AutoUpdate\u0026#34; updateSlices() Now we are ensuring that the \u0026ldquo;setDefaults\u0026rdquo; function and therefore also the \u0026ldquo;updateSlices\u0026rdquo; function are executed every time the panel is opened by setting \u0026ldquo;setDefaults\u0026rdquo; as a wakeup command.\nBaseNetwork.script\nCommands { source = $(LOCAL)/BaseNetwork.py wakeupCommand = \u0026#34;setDefaults\u0026#34; } And we add field listeners, so that the field values that we are working with are updated every time they are changed.\nBaseNetwork.script\nCommands { source = $(LOCAL)/BaseNetwork.py wakeupCommand = \u0026#34;setDefaults\u0026#34; FieldListener { listenField = \u0026#34;SubImage.sz\u0026#34; listenField = \u0026#34;SubImage.z\u0026#34; listenField = \u0026#34;Histogram.binSize\u0026#34; command = \u0026#34;updateSlices\u0026#34; } } To see if all of this is working, we need to embed fields into our panel. Put this inside of the box titled \u0026ldquo;Single Slice\u0026rdquo;:\nBaseNetwork.script\nField \u0026#34;SubImage.sz\u0026#34; { title = \u0026#34;Plot slice\u0026#34; } Button { title = \u0026#34;in 2D\u0026#34; command = \u0026#34;singleSlice2D\u0026#34; } Button { title = \u0026#34;in 3D\u0026#34; command = \u0026#34;click3D\u0026#34; } Empty {} And then add this to your box titled \u0026ldquo;Sequence\u0026rdquo;:\nBaseNetwork.script\nField \u0026#34;SubImage.z\u0026#34; { title = \u0026#34;From slice\u0026#34; } Field \u0026#34;SubImage.sz\u0026#34; { title = \u0026#34;To slice\u0026#34; } Button { title = \u0026#34;Plot 2D\u0026#34; command = \u0026#34;click2D\u0026#34; } Button { title = \u0026#34;Plot 3D\u0026#34; command = \u0026#34;click3D\u0026#34; } Lastly, put this under your two boxes, but above the empty element in the vertical alignment: BaseNetwork.script\nField \u0026#34;Histogram.binSize\u0026#34; { title = \u0026#34;Bin size\u0026#34; } If you followed all of the listed steps, your panel preview should look like this and display all the current parameter values. We can now work on the functions that visualize the data as plots on the Matplotlib canvas. You will have noticed how all of the buttons in the .script file have a command. Whenever that button is clicked, its designated command is executed. However, for any of the functions referenced via \u0026ldquo;command\u0026rdquo; to work, we need one that ensures that the plots are shown on the integrated Matplotlib canvas. We will start with that one.\nBaseNetwork.py\ndef clearFigure(): control = ctx.control(\u0026#34;canvas\u0026#34;).object() control.figure().clear() Now that this is prepared and ready, we can add the functions to extract the data:\nBaseNetwork.py\ndef getX(): x = ctx.field(\u0026#34;Histogram.outputHistogramCurve\u0026#34;).object().getXValues() stringx = \u0026#34;,\u0026#34;.join([str(i) for i in x]) xValues = stringx.split(\u0026#34;,\u0026#34;) return [float(s) for s in xValues] def getY(): y = ctx.field(\u0026#34;Histogram.outputHistogramCurve\u0026#34;).object().getYValues() stringy = \u0026#34;,\u0026#34;.join([str(i) for i in y]) yValues = stringy.split(\u0026#34;,\u0026#34;) return [float(s) for s in yValues] And lastly, enable the plotting of a single slice as well as a sequence in 2D through our panel by adding the code below.\nBaseNetwork.py\ndef singleSlice2D(): lastSlice = endSlice ctx.field(\u0026#34;SubImage.z\u0026#34;).value = endSlice click2D() ctx.field(\u0026#34;SubImage.z\u0026#34;).value = lastSlice def plotSequence(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() values = [i for i in range(startSlice, endSlice + 1)] if len(values) \u0026lt;= 4: # adapt the height of the subplot to the number of plots sub = 100 * len(values) + 11 for i in values: subplot = figure.add_subplot(sub) sub += 1 ctx.field(\u0026#34;SubImage.z\u0026#34;).value = i ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = i subplot.bar(getX(), getY(), bins, color=\u0026#39;r\u0026#39;, label=f\u0026#39;Slice {i}\u0026#39;) subplot.legend([f\u0026#39;Slice {i}\u0026#39;]) else: subplot = figure.add_subplot() for i in values: ctx.field(\u0026#34;SubImage.z\u0026#34;).value = i ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = i subplot.plot(getX(), getY(), bins) subplot.legend([f\u0026#39;Slice {i}\u0026#39; for i in values]) ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] figure.canvas.draw() def click2D(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() if startSlice == endSlice: subplot = figure.add_subplot(111) subplot.bar(getX(), getY(), bins, color=\u0026#39;b\u0026#39;, label=f\u0026#34;Slice {endSlice}\u0026#34;) subplot.legend() subplot.plot() figure.canvas.draw() else: plotSequence() You should now be able to reproduce results like these:\n2D plot of slice 28 Smaller sequences are displayed as multiple single slice plots. Sequence in 2D Info:\u0026nbsp; Notice how the bin size affects the plots appearance. You can download the .py file below if you want. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Python file here. Summary Functions are connected to fields of the panel via commands. The panel preview in MATE can be used to change positioning of panel components without touching the code. An \u0026ldquo;expand\u0026rdquo; statement can help the positioning of components in the panel. ","tags":["Advanced","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685491200","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/matplotlib/slicecomparison/","title":"Example 3: Slice Comparison","summary":"Example 3: Slice Comparison Introduction We will adapt the previously created macro module to be able to overlay two defined slices to compare their grayscale distributions.\nThe module we are adapting has been set up in the Example 1: Module Setup tutorial. The panel and two-dimensional plotting functionality has been added in [Example 2: 2D Plotting] (tutorials/thirdparty/matplotlib/2dplotting). Steps to Do At first, we will extend the panel: Open your BaseNetwork macro module within an empty MeVisLab workspace and select the .script file from its related files.\n","content":"Example 3: Slice Comparison Introduction We will adapt the previously created macro module to be able to overlay two defined slices to compare their grayscale distributions.\nThe module we are adapting has been set up in the Example 1: Module Setup tutorial. The panel and two-dimensional plotting functionality has been added in [Example 2: 2D Plotting] (tutorials/thirdparty/matplotlib/2dplotting). Steps to Do At first, we will extend the panel: Open your BaseNetwork macro module within an empty MeVisLab workspace and select the .script file from its related files.\nAdd the following code into your .script file between the \u0026ldquo;Single Slice\u0026rdquo; and the \u0026ldquo;Sequence\u0026rdquo; box.\nBaseNetwork.script\nBox { title = \u0026#34;Comparison\u0026#34; Field \u0026#34;SubImage.z\u0026#34; { title = \u0026#34;Compare slice\u0026#34; } Field \u0026#34;SubImage.sz\u0026#34; { title = \u0026#34;With slice\u0026#34; } Button { title = \u0026#34;Plot\u0026#34; command = \u0026#34;comparison\u0026#34; } } Your panel should now be changed to look like this:\nWe will now add the \u0026ldquo;comparison\u0026rdquo; function, to give the \u0026ldquo;Plot\u0026rdquo; button in our \u0026ldquo;Comparison\u0026rdquo; box a purpose. To do so, switch to your module\u0026rsquo;s .py file and choose a cosy place for the following piece of code:\nBaseNetwork.py\ndef comparison(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() values = [startSlice, endSlice] ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = values[0] y1 = getY() x1 = getX() ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[1] ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = values[1] y2 = getY() x2 = getX() subplot = figure.add_subplot(211) subplot.bar(x1, y1, bins, color=\u0026#39;r\u0026#39;) subplot.bar(x2, y2, bins, color=\u0026#39;b\u0026#39;) subplot.legend([f\u0026#39;Slice {i}\u0026#39; for i in values]) subplot.plot() subplot = figure.add_subplot(212) subplot.bar(x2, y2, bins, color=\u0026#39;b\u0026#39;) subplot.bar(x1, y1, bins, color=\u0026#39;r\u0026#39;) subplot.legend([f\u0026#39;Slice {i}\u0026#39; for i in values]) figure.canvas.draw() ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] You should now be able to reproduce results like these:\nSummary Grayscale distributions of two slices can be layered to compare them and make deviations noticeable. ","tags":["Beginner","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685491200","url":"https://mevislab.github.io/examples/pull/133/tutorials/thirdparty/matplotlib/3dplotting/","title":"Example 4: 3D Plotting","summary":"Example 4: 3D Plotting Introduction In this tutorial, we will equip the macro module we created in the Example 1: Module Setup and later on adapted by enabling it to plot grayscale distributions of single slices and sequences in 2D in Example 2: 2D Plotting with a three-dimensional plotting functionality.\nSteps to Do The fields and commands needed have already been prepared in the second tutorial. We will just have to modify our .py file a little bit to make them usable. Integrate the following code into your .py file and import numpy.\n","content":"Example 4: 3D Plotting Introduction In this tutorial, we will equip the macro module we created in the Example 1: Module Setup and later on adapted by enabling it to plot grayscale distributions of single slices and sequences in 2D in Example 2: 2D Plotting with a three-dimensional plotting functionality.\nSteps to Do The fields and commands needed have already been prepared in the second tutorial. We will just have to modify our .py file a little bit to make them usable. Integrate the following code into your .py file and import numpy.\nBaseNetwork.py\ndef click3D(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() values = [i for i in range(startSlice, endSlice + 1)] if startSlice == endSlice: subplot = figure.add_subplot(111, projection=\u0026#39;3d\u0026#39;) subplot.bar3d(x=getX(), y=startSlice, z=0, dx=1, dy=1, dz=getY()) subplot.set_yticks(np.arange(startSlice, endSlice)) subplot.set_title(f\u0026#39;Slice {startSlice}\u0026#39;) figure.canvas.draw() else: clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() subplot = figure.add_subplot(111, projection=\u0026#39;3d\u0026#39;) for i in values: ctx.field(\u0026#34;SubImage.z\u0026#34;).value = i ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = i subplot.bar3d(x=getX(), y=i, z=0, dx=1, dy=1, dz=getY()) subplot.set_yticks(values) subplot.set_title(f\u0026#39;Sequence from {values[0]} to {endSlice}\u0026#39;) ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] figure.canvas.draw() After saving, you should be able to reproduce results like these:\nWarning:\u0026nbsp; You cannot zoom into 3D plots on a Matplotlib canvas. Try changing the viewing angle instead. You can download the .py file below if you want. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Python file here. ","tags":["Advanced","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1701216000","url":"https://mevislab.github.io/examples/pull/133/tutorials/shorts/","title":"Tips and Tricks","summary":"MeVisLab Tips and Tricks This chapter shows some features and functionalities that are helpful but do not provide its own tutorial.\nKeyboard Shortcuts Using Snippets Scripting Assistant User Scripts Show status of module in- and output Module suggestion of module in- and output Keyboard Shortcuts This is a collection of useful keyboard shortcuts in MeVisLab.\nShortcut Functionality CTRL + 1 Automatically arrange selection of modules in the current network CTRL + 2 Open most recent network file CTRL + 3 Run most recent test case (extremely useful for developers) CTRL + A then CTRL + 1 Layout network CTRL + A then TAB Layout *.script* file (in MATE) CTRL + D Duplicate currently selected module (including all field values) CTRL and Left Mouse Button or Middle Mouse Button Show internal network SPACE Show hidden outputs of the currently selected module CTRL + ALT + T Start test center CTRL + K Restart MeVisLab with current network(s) CTRL + R Run script file with the same name of your network file if available in the same directory. ALT Double-click on a module Open automatic panel of the module. Using Snippets \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Sometimes you have to create the same network over and over again \u0026ndash; for example, to quickly preview DICOM files. Generally, you will at least add one module to load and another module to display your images. Sometimes you may also want to view the DICOM header data. A network you possibly generate whenever opening DICOM files will be the following:\n","content":"MeVisLab Tips and Tricks This chapter shows some features and functionalities that are helpful but do not provide its own tutorial.\nKeyboard Shortcuts Using Snippets Scripting Assistant User Scripts Show status of module in- and output Module suggestion of module in- and output Keyboard Shortcuts This is a collection of useful keyboard shortcuts in MeVisLab.\nShortcut Functionality CTRL + 1 Automatically arrange selection of modules in the current network CTRL + 2 Open most recent network file CTRL + 3 Run most recent test case (extremely useful for developers) CTRL + A then CTRL + 1 Layout network CTRL + A then TAB Layout *.script* file (in MATE) CTRL + D Duplicate currently selected module (including all field values) CTRL and Left Mouse Button or Middle Mouse Button Show internal network SPACE Show hidden outputs of the currently selected module CTRL + ALT + T Start test center CTRL + K Restart MeVisLab with current network(s) CTRL + R Run script file with the same name of your network file if available in the same directory. ALT Double-click on a module Open automatic panel of the module. Using Snippets \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Sometimes you have to create the same network over and over again \u0026ndash; for example, to quickly preview DICOM files. Generally, you will at least add one module to load and another module to display your images. Sometimes you may also want to view the DICOM header data. A network you possibly generate whenever opening DICOM files will be the following:\nOpen DICOM files Create a snippet of your commonly used networks by adding the snippets list from the main menu. Open [ View \u0026rarr; Views \u0026rarr; Snippets List ]. A new panel is shown. Select all modules of your network and double-click New\u0026hellip; in your Snippets List.\nEnter a name for your snippet like DICOM Viewer and click Add.\nA new snippet will be shown in your Snippets List. You can drag and drop the snippet to your workspace and the modules are reused, including all defined field values.\nSnippets List Scripting Assistant \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. If you are new to Python or don\u0026rsquo;t have experiences in accessing fields in MeVisLab via Python scripting, the Scripting Assistant might help you.\nOpen [ View \u0026rarr; Views \u0026rarr; Scripting Assistant ]. A new panel is shown.\nIf you now interact with a network, module, or macro module, your user interactions are converted into Python calls. You can see the calls in the panel of the Scripting Assistant and copy and paste them for your Python script.\nScripting Assistant User Scripts User scripts allow you to call any Python code from the main menu entry [ Scripting ]. MeVisLab already comes with some user scripts you can try. You can also view the sources for example code via right-click on the menu entry under [ Scripting ].\nThis example shows you how to change the color of the MeVisLab IDE to a dark mode.\nRight-click menu entry [ Scripting \u0026rarr; Utilities \u0026rarr; Close Unselected Panels ] and select Edit User Script. The Python file opens in MATE. Right-click on the tab in the editor and select Show Enclosing Folder.\nThe opened directory contains all available user scripts. Add a new file MyScripts.def and open the file in MATE.\nEnter the following: MyScripts.def\nUserIDEActions { Action \u0026#34;Set Dark Theme\u0026#34; { name = changeTheme userScript = $(LOCAL)/changeTheme.py statusTip = \u0026#34;Change theme to dark mode\u0026#34; accel = \u0026#34;ctrl+F9\u0026#34; } } UserIDEMenus { SubMenu \u0026#34;Theme\u0026#34; { ActionReference = changeTheme } } We define an action Set Dark Theme, which is added to the submenu Theme in the MeVisLab IDE menu item [ Scripting ]. The action is named changeTheme and a reference to a Python script is added as $(LOCAL)/changeTheme.py. We also defined a keyboard shortcut ctrl\u0026#43;F9 .\nChange to MeVisLab IDE and select menu item [ Extras \u0026rarr; Reload Module Database (Clear Cache) ]. Open the menu item [ Scripting ]. You can see the new submenu [ Theme \u0026rarr; Set Dark Theme ]. If you select this entry, you get an error in MeVisLab console: Could not locate user script: \u0026hellip;/changeTheme.py\nWe did not yet create the Python file containing the code of your script.\nOpen the directory where your MyScripts.def file is located and create a new Python file changeTheme.py. Open the file in MATE and enter the following:\nchangeTheme.py\nfrom PythonQt.QtGui import QApplication, QColor, QPalette fgColor = QColor(\u0026#34;#888888\u0026#34;) bgColor = QColor(\u0026#34;#333333\u0026#34;) palette = QApplication.palette() palette.setColor(QPalette.Window, bgColor) palette.setColor(QPalette.Background, bgColor) palette.setColor(QPalette.Base, bgColor) palette.setColor(QPalette.Button, bgColor) palette.setColor(QPalette.WindowText, fgColor) palette.setColor(QPalette.Text, fgColor) QApplication.setPalette(palette) This script defines the color of the MeVisLab user interface elements. You can define other colors and more items; this is just an example of what you can do with user scripts.\nSwitch back to the MeVisLab IDE and select the menu item [ Extras \u0026rarr; Reload Module Database (Clear Cache) ] again. The colors of the MeVisLab IDE change as defined in our Python script. This change persists until you restart MeVisLab and can always be repeated by selecting the menu entry or pressing the keyboard shortcut ctrl\u0026#43;F9 .\nShow Status of Module Input and Output Especially in large networks it is useful to see the state of the input and output connectors of a module. By default, the module connectors do not show if data is available. Below image shows a DicomImport module and a View2D module where no data is loaded.\nNo status on connector In the MeVisLab preferences dialog, you can see a checkbox Show ML image state. By default, the setting is Off.\nShow ML image state After enabling Show ML image state, your network changes and the input and output connectors appear red in the case no data is available at the output.\nNo data on connector After loading a valid DICOM directory, the connectors providing a valid ML image appear green. The previously red outputs are beige again, showing there is data available.\nNo data on connector Module Suggestion of Module Input and Output \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. MeVisLab provides a functionality to suggest frequently used modules for the selected output in your network.\nEspecially for new users learning MeVisLab, it makes sense to enable the module suggestion via menu item [ Scripting \u0026rarr; Module Suggest \u0026rarr; Module Suggest (toggle) ].\nIf you now select an input or output, MeVisLab shows the modules that have been frequently used for this connector in our example networks.\nModule suggestion You can toggle through the suggestions via keyboard shortcut , or shift\u0026#43;, .\n","tags":["Basic","Tutorial"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/basic_mechanisms/","title":"Basic Mechanisms","summary":"Basic Mechanism Examples: The following examples are available:\n[1] Contour Filter [2] Creating a Simple Application [3] Panel for the Contour Filter [4] Python Scripting","content":"Basic Mechanism Examples: The following examples are available:\n[1] Contour Filter [2] Creating a Simple Application [3] Panel for the Contour Filter [4] Python Scripting ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/","title":"Data Objects","summary":"Data Object Examples: The following examples are available:\n[1] 2D and 3D Visualization of Contours [2] Annotation of Images [3] Apply Transformations to a 3D WEM Object Via Mouse Interactions [4] Contour Interpolation [5] Contours and Ghosting [6] Creation of Contours [7] Creation of WEMs [8] Distance Between Markers [9] Drawing Curves [10] Interactively Moving WEM [11] Processing and Modifying of WEMs [12] WEM - Primitive Value Lists","content":"Data Object Examples: The following examples are available:\n[1] 2D and 3D Visualization of Contours [2] Annotation of Images [3] Apply Transformations to a 3D WEM Object Via Mouse Interactions [4] Contour Interpolation [5] Contours and Ghosting [6] Creation of Contours [7] Creation of WEMs [8] Distance Between Markers [9] Drawing Curves [10] Interactively Moving WEM [11] Processing and Modifying of WEMs [12] WEM - Primitive Value Lists ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/thirdparty/example2/","title":"Face Detection in OpenCV","summary":"Third-party Example 2: Face Detection in OpenCV This Python file shows how to access the webcam and detect faces in the video stream via OpenCV.\nDownload You can download the Python files here\n","content":"Third-party Example 2: Face Detection in OpenCV This Python file shows how to access the webcam and detect faces in the video stream via OpenCV.\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/image_processing/","title":"Image Processing","summary":"Image Processing Examples: The following examples are available:\n[1] Arithmetic operations on two images [2] Clip Planes [3] Masking Images [4] Region Growing (Segmentation) [5] Subtract 3D Objects","content":"Image Processing Examples: The following examples are available:\n[1] Arithmetic operations on two images [2] Clip Planes [3] Masking Images [4] Region Growing (Segmentation) [5] Subtract 3D Objects ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/testing/example3/","title":"Iterative Tests in MeVisLab With Screenshots","summary":"Testing Example 3: Iterative Tests in MeVisLab With Screenshots In this example you will learn how to write iterative tests in MeVisLab. In addition to that, we create a screenshot of a viewer and add the image to the test report.\nDownload n.a.\n","content":"Testing Example 3: Iterative Tests in MeVisLab With Screenshots In this example you will learn how to write iterative tests in MeVisLab. In addition to that, we create a screenshot of a viewer and add the image to the test report.\nDownload n.a.\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/open_inventor/","title":"Open Inventor","summary":"Open Inventor Examples: The following examples are available:\n[1] Camera Interaction With Collision Detection [2] Camera Interactions in Open Inventor [3] Mouse Interactions in an Open Inventor Scene [4] Open Inventor objects","content":"Open Inventor Examples: The following examples are available:\n[1] Camera Interaction With Collision Detection [2] Camera Interactions in Open Inventor [3] Mouse Interactions in an Open Inventor Scene [4] Open Inventor objects ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/thirdparty/example1/","title":"OpenCV Webcam Access","summary":"Third-party Example 1: OpenCV Webcam Access This Python file shows how to access the webcam via OpenCV and use the video via PythonImage module in MeVisLab.\nDownload You can download the Python files here\n","content":"Third-party Example 1: OpenCV Webcam Access This Python file shows how to access the webcam via OpenCV and use the video via PythonImage module in MeVisLab.\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/testing/example2/","title":"Profiling in MeVisLab","summary":"Testing Example 2: Profiling in MeVisLab This example shows how to use the Profiling View in MeVisLab.\nDownload n.a.\n","content":"Testing Example 2: Profiling in MeVisLab This example shows how to use the Profiling View in MeVisLab.\nDownload n.a.\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/thirdparty/pytorch1/","title":"PyTorch Segmentation","summary":"Third-party Example 5: Segmentation in Webcam Stream by using PyTorch This macro module segments a person shown in a webcam stream by using a pretrained network from PyTorch (torchvision).\nDownload You can download the Python files here\n","content":"Third-party Example 5: Segmentation in Webcam Stream by using PyTorch This macro module segments a person shown in a webcam stream by using a pretrained network from PyTorch (torchvision).\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/testing/","title":"Testing Examples","summary":"Testing Examples: The following examples are available:\n[1] Iterative Tests in MeVisLab With Screenshots [2] Profiling in MeVisLab [3] Writing a Simple Test Case in MeVisLab","content":"Testing Examples: The following examples are available:\n[1] Iterative Tests in MeVisLab With Screenshots [2] Profiling in MeVisLab [3] Writing a Simple Test Case in MeVisLab ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/thirdparty/","title":"Third-party Examples","summary":"Third-party Examples: The following examples are available:\n[1] Face Detection in OpenCV [2] OpenCV Webcam Access [3] PyTorch Segmentation","content":"Third-party Examples: The following examples are available:\n[1] Face Detection in OpenCV [2] OpenCV Webcam Access [3] PyTorch Segmentation ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/howto/","title":"Using Provided Examples","summary":"Structure Each tutorial chapter was used as an umbrella theme to structure related examples that are linked in a list. After clicking any of the linked examples, you will be forwarded to a short description of the feature and have the option to download the resource that produces your desired effect.\nThe provided files are usually either .mlab files or .zip archives. You will find a short tutorial on how to add those files into your MeVisLab application to work with them below.\n","content":"Structure Each tutorial chapter was used as an umbrella theme to structure related examples that are linked in a list. After clicking any of the linked examples, you will be forwarded to a short description of the feature and have the option to download the resource that produces your desired effect.\nThe provided files are usually either .mlab files or .zip archives. You will find a short tutorial on how to add those files into your MeVisLab application to work with them below.\nMeVisLab (.mlab) Files MeVisLab files are networks stored as .mlab files. Info:\u0026nbsp; Double-clicking the left mouse button within your MeVisLab workspace works as a shortcut to open files. Files can also be opened using the menu option [ File \u0026rarr; Open ].\nArchive (.zip) Files Archives mostly contain macro modules. To use those macro modules, you will need to know how to handle user packages.\nCheck:\u0026nbsp; See Example 2.1: Package creation for more information on packages in MeVisLab. The contents can be extracted into the directory of your package. Make sure to keep the directory\u0026rsquo;s structure for the examples to be loaded and displayed correctly.\nThe typical directory structure of a MeVisLab package looks like this: Package directory structure The package TutorialSummary within the package group MeVis is shown above. A package typically contains at least a Projects directory, which is where the macro modules are located. When extracting the contents of a .zip file, the Projects folder of your package should be the target directory.\nSometimes we even provide test cases. Extract them into the TestCases directory. Package directory structure Notice:\u0026nbsp; Feel free to create certain directories if they do not exist yet, but make sure to name them conforming the directory structure shown above. Continuing on your MeVisLab workspace: You might need to reload the module cache after adding macro modules out of .zip archives for them to be displayed and ready to be used. To do so, open [ Extras \u0026rarr; Reload Module Database (Clear Cache) ].\nPython (.py) or Script (.script) Files In the rare case that a .py or .script file is provided, make sure to firstly follow the tutorials related to macro modules and test cases.\nWarning:\u0026nbsp; The integration of Python scripts might not add a lot of value for someone that lacks the knowledge conveyed by the tutorials. ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/visualization/","title":"Visualization Examples","summary":"Visualization Examples: The following examples are available:\n[1] Creating a Magnifier [2] Display Images Converted to Open Inventor Scene Objects [3] Image Overlays [4] Synchronous View of Two Images [5] Volume Rendering and Interactions [6] Volume Rendering vs. Path Tracing","content":"Visualization Examples: The following examples are available:\n[1] Creating a Magnifier [2] Display Images Converted to Open Inventor Scene Objects [3] Image Overlays [4] Synchronous View of Two Images [5] Volume Rendering and Interactions [6] Volume Rendering vs. Path Tracing ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/pull/133/examples/testing/example1/","title":"Writing a Simple Test Case in MeVisLab","summary":"Testing Example 1: Writing a Simple Test Case in MeVisLab This example shows how to write and execute test cases in MeVisLab. The Python files can be downloaded below.\nDownload You can download the Python files here\n","content":"Testing Example 1: Writing a Simple Test Case in MeVisLab This example shows how to write and execute test cases in MeVisLab. The Python files can be downloaded below.\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276093","url":"https://mevislab.github.io/examples/pull/133/about/about/","title":"Overview","summary":"Symbols We embedded three symbols, referencing additional info, tasks, and warnings: Info:\u0026nbsp; Provides additional links or info on the current topic. Check:\u0026nbsp; Points out a related task. Warning:\u0026nbsp; Hints common mistakes or steps you should consider beforehand. ","content":"Symbols We embedded three symbols, referencing additional info, tasks, and warnings: Info:\u0026nbsp; Provides additional links or info on the current topic. Check:\u0026nbsp; Points out a related task. Warning:\u0026nbsp; Hints common mistakes or steps you should consider beforehand. Keyboard Shortcuts Keyboard shortcuts are incorporated like this: CTRL + ALT + 2 .\nNetworks The networks shown and used in the tutorials can be found in the Examples section of this page. They are usually embedded like this: \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Symbols","Glossary","Overview"],"section":"about"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/contours/example3/","title":"2D and 3D Visualization of Contours","summary":"Contour Example 3: 2D and 3D Visualization of Contours This example shows how to display CSOs in 2D as an overlay and additionally how the CSOs are displayed in 3D.\nSummary Images are loaded by using a LocalImage module and displayed in a 2D viewer. A SoCSOLiveWireEditor is added to draw contours on the images. The CSOSliceInterpolator generates additional contours between the manually created CSOs by using linear interpolation.\nThe module VoxelizeCSO is used to create a three-dimensional voxel mask of the contours that can be used as an overlay on the images in a View2D panel. The SoView2DOverlay module defines the color and opacity of the overlay.\n","content":"Contour Example 3: 2D and 3D Visualization of Contours This example shows how to display CSOs in 2D as an overlay and additionally how the CSOs are displayed in 3D.\nSummary Images are loaded by using a LocalImage module and displayed in a 2D viewer. A SoCSOLiveWireEditor is added to draw contours on the images. The CSOSliceInterpolator generates additional contours between the manually created CSOs by using linear interpolation.\nThe module VoxelizeCSO is used to create a three-dimensional voxel mask of the contours that can be used as an overlay on the images in a View2D panel. The SoView2DOverlay module defines the color and opacity of the overlay.\nLastly, the panel of the View3D module is used to visualize the voxel mask in 3D.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/contours/example4/","title":"Annotation of Images","summary":"Contour Example 4: Annotation of Images This example shows how to add annotations to an image.\nSummary In this example, the network of Contour Example 3 is extended, so that the volume of the 3D mask generated by the VoxelizeCSO module is calculated. The CalculateVolume module counts the number of voxels in the given mask and returns the correct volume in ml. The calculated volume will be used for a custom SoView2DAnnotation displayed in the View2D.\n","content":"Contour Example 4: Annotation of Images This example shows how to add annotations to an image.\nSummary In this example, the network of Contour Example 3 is extended, so that the volume of the 3D mask generated by the VoxelizeCSO module is calculated. The CalculateVolume module counts the number of voxels in the given mask and returns the correct volume in ml. The calculated volume will be used for a custom SoView2DAnnotation displayed in the View2D.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/surface_objects/example3/","title":"Apply Transformations to a 3D WEM Object Via Mouse Interactions","summary":"Surface Example 3: Interactions With WEM Scale, Rotate, and Move a WEM in a Scene In this example, we are using a SoTransformerDragger module to apply transformations on a 3D WEM object via mouse interactions. Download You can download the example network here\nInteractively Modify WEMs In this example, we are using a SoWEMBulgeEditor module to modify a WEM using the mouse. Download You can download the example network here\n","content":"Surface Example 3: Interactions With WEM Scale, Rotate, and Move a WEM in a Scene In this example, we are using a SoTransformerDragger module to apply transformations on a 3D WEM object via mouse interactions. Download You can download the example network here\nInteractively Modify WEMs In this example, we are using a SoWEMBulgeEditor module to modify a WEM using the mouse. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/image_processing/example1/","title":"Arithmetic operations on two images","summary":"Image Processing Example 1: Arithmetic Operations on Two Images In this example, we apply scalar functions on two images like Add, Multiply, Subtract, etc.\nSummary We are loading two images by using the LocalImage module and show them in a SynchroView2D. In addition to that, both images are used for arithmetic processing in the module Arithmetic2.\nDownload You can download the example network here\n","content":"Image Processing Example 1: Arithmetic Operations on Two Images In this example, we apply scalar functions on two images like Add, Multiply, Subtract, etc.\nSummary We are loading two images by using the LocalImage module and show them in a SynchroView2D. In addition to that, both images are used for arithmetic processing in the module Arithmetic2.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/open_inventor/example4/","title":"Camera Interaction With Collision Detection","summary":"Open Inventor Example 4: Camera Interaction With Collision Detection This example shows how to implement a camera flight using keyboard shortcuts. Collisions with anatomical structures are detected and the flight stops. In addition to that, the camera object and direction is rendered in another viewer.\nThis example has been taken from the MeVisLab forum.\nSummary A local macro flightControl allows you to navigate with the camera through the scene.\n","content":"Open Inventor Example 4: Camera Interaction With Collision Detection This example shows how to implement a camera flight using keyboard shortcuts. Collisions with anatomical structures are detected and the flight stops. In addition to that, the camera object and direction is rendered in another viewer.\nThis example has been taken from the MeVisLab forum.\nSummary A local macro flightControl allows you to navigate with the camera through the scene.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/open_inventor/example3/","title":"Camera Interactions in Open Inventor","summary":"Open Inventor Example 3: Camera Interactions in Open Inventor This example shows different options for using a camera and different viewers in Open Inventor.\nSummary We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nDownload You can download the example network here\n","content":"Open Inventor Example 3: Camera Interactions in Open Inventor This example shows different options for using a camera and different viewers in Open Inventor.\nSummary We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/image_processing/example5/","title":"Clip Planes","summary":"Image Processing Example 5: Clip Planes In this example, we are using the currently visible slice from a 2D view as a clip plane in 3D.\nSummary We are loading images by using the LocalImage module and render them as a two-dimensional image stack SoRenderArea. The displayed slice is used to create a 3D plane/clip plane in a SoExaminerViewer.\nDownload You can download the example network here\n","content":"Image Processing Example 5: Clip Planes In this example, we are using the currently visible slice from a 2D view as a clip plane in 3D.\nSummary We are loading images by using the LocalImage module and render them as a two-dimensional image stack SoRenderArea. The displayed slice is used to create a 3D plane/clip plane in a SoExaminerViewer.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/basic_mechanisms/contour_filter/","title":"Contour Filter","summary":"Example 1: Contour Filter This example shows how to create a contour filter.\nSummary Images are loaded via ImageLoad module and visualized unchanged in a View2D module View2D1. Additionally, the images are modified by a local macro module Filter and shown in another View2D viewer View2D.\nIn order to display the same slice (unchanged and changed), the module SyncFloat is used to synchronize the field value startSlice in both viewers. The SyncFloat module duplicates the value Float1 to the field Float2 if it differs by Epsilon.\n","content":"Example 1: Contour Filter This example shows how to create a contour filter.\nSummary Images are loaded via ImageLoad module and visualized unchanged in a View2D module View2D1. Additionally, the images are modified by a local macro module Filter and shown in another View2D viewer View2D.\nIn order to display the same slice (unchanged and changed), the module SyncFloat is used to synchronize the field value startSlice in both viewers. The SyncFloat module duplicates the value Float1 to the field Float2 if it differs by Epsilon.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/contours/example2/","title":"Contour Interpolation","summary":"Contour Example 2: Contour Interpolation This example shows how to interpolate CSOs across slices.\nSummary In this example, semiautomatic countours are created using the SoCSOLiveWireEditor module and their visualization is modified using the SoCSOVisualizationSettings module.\nAdditional contours between the manually created ones are generated by the CSOSliceInterpolator and added to the CSOManager. Different groups of contours are created for the left and right lobe of the lung and colored respectively.\n","content":"Contour Example 2: Contour Interpolation This example shows how to interpolate CSOs across slices.\nSummary In this example, semiautomatic countours are created using the SoCSOLiveWireEditor module and their visualization is modified using the SoCSOVisualizationSettings module.\nAdditional contours between the manually created ones are generated by the CSOSliceInterpolator and added to the CSOManager. Different groups of contours are created for the left and right lobe of the lung and colored respectively.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/contours/example5/","title":"Contours and Ghosting","summary":"Contour Example 5: Contours and Ghosting This image shows how to automatically create CSOs based on isovalues. In addition, the visualization of CSOs on previous and subsequent slices is shown.\nSummary In this example, the CSOIsoGenerator is used to generate contours based on a given isovalue out of the image. Contours are generated in the image where the given isovalue is close to the one configured. These contours are stored in the CSOManager and ghosting is activated in the SoCSOVisualizationSettings.\n","content":"Contour Example 5: Contours and Ghosting This image shows how to automatically create CSOs based on isovalues. In addition, the visualization of CSOs on previous and subsequent slices is shown.\nSummary In this example, the CSOIsoGenerator is used to generate contours based on a given isovalue out of the image. Contours are generated in the image where the given isovalue is close to the one configured. These contours are stored in the CSOManager and ghosting is activated in the SoCSOVisualizationSettings.\n\u0026ldquo;Ghosting\u0026rdquo; means not only showing contours available on the currently visible slice but also contours on the neighboring slices with increasing transparency.\nThe contours are also displayed in a three-dimensionsl SoExaminerViewer by using the SoCSO3DRenderer.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/visualization/example2/","title":"Creating a Magnifier","summary":"Visualization Example 2: Creating a Magnifier This example shows how to create a magnifier. Using the module SubImage, a fraction of the original image can be extracted and enlarged. Download You can download the example network here\n","content":"Visualization Example 2: Creating a Magnifier This example shows how to create a magnifier. Using the module SubImage, a fraction of the original image can be extracted and enlarged. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/basic_mechanisms/viewer_application/","title":"Creating a Simple Application","summary":"Example 3: Creating a Simple Application In this example, you will learn how to create a simple prototype application in MeVisLab including a user interface (UI) with 2D and 3D viewers.\nDownload You can download the example network here\n","content":"Example 3: Creating a Simple Application In this example, you will learn how to create a simple prototype application in MeVisLab including a user interface (UI) with 2D and 3D viewers.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/contours/example1/","title":"Creation of Contours","summary":"Contour Example 1: Creation of Contours Contours are stored as Contour Segmentation Objects (CSOs) in MeVisLab. This example highlights ways of creating CSOs using modules of the SoCSOEditor group.\nInfo:\u0026nbsp; You may want to look at the glossary entry on CSOs. The SoCSOEditor module group contains several modules, some of which are listed right below:\n","content":"Contour Example 1: Creation of Contours Contours are stored as Contour Segmentation Objects (CSOs) in MeVisLab. This example highlights ways of creating CSOs using modules of the SoCSOEditor group.\nInfo:\u0026nbsp; You may want to look at the glossary entry on CSOs. The SoCSOEditor module group contains several modules, some of which are listed right below:\nSoCSOPointEditor SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor SoCSODistancePolylineEditor SoCSOEllipseEditor SoCSORectangleEditor SoCSOSplineEditor SoCSOPolygonEditor SoCSOIsoEditor SoCSOLiveWireEditor Info:\u0026nbsp; Whenever Contour Segmentation Objects are created, they are temporarily stored by and can be managed with the CSOManager. In this example, contours are created and colors and styles of these CSOs are customized by using the SoCSOVisualizationSettings module.\nSummary Contours are stored as their own abstract data type called Contour Segmentation Objects (often abbreviated to CSO). The SoCSO\\*Editor module group contains several useful modules to create, interact with or modify CSOs. Created CSOs are temporarily stored and can be managed using the CSOManager. Download The example network can be downloaded here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/surface_objects/example1/","title":"Creation of WEMs","summary":"Surface Example 1: Creation of WEMs This example shows how to create WEMs out of voxel images and CSOs. Download You can download the example network here\n","content":"Surface Example 1: Creation of WEMs This example shows how to create WEMs out of voxel images and CSOs. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/visualization/example4/","title":"Display Images Converted to Open Inventor Scene Objects","summary":"Visualization Example 4: Display Images Converted to Open Inventor Scene Objects This example shows how to convert images to Open Inventor scene objects using the module SoView2D and modules based on SoView2D. Download You can download the example network here\n","content":"Visualization Example 4: Display Images Converted to Open Inventor Scene Objects This example shows how to convert images to Open Inventor scene objects using the module SoView2D and modules based on SoView2D. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/markers/example1/","title":"Distance Between Markers","summary":"Marker Example 1: Distance Between Markers This examples shows how to create markers in a viewer and measure their distance. Download You can download the example network here\n","content":"Marker Example 1: Distance Between Markers This examples shows how to create markers in a viewer and measure their distance. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/curves/example1/","title":"Drawing Curves","summary":"Curves Example: Drawing Curves This examples shows how to create and render curves. Download You can download the example network here\n","content":"Curves Example: Drawing Curves This examples shows how to create and render curves. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/visualization/example3/","title":"Image Overlays","summary":"Visualization Example 3: Image Overlays This example shows the creation of an overlay. Using the module SoView2DOverlay, an overlay can be blended over a 2D image. Download You can download the example network here\n","content":"Visualization Example 3: Image Overlays This example shows the creation of an overlay. Using the module SoView2DOverlay, an overlay can be blended over a 2D image. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/surface_objects/example4/","title":"Interactively Moving WEM","summary":"Surface Example 4: Interactively Moving WEM This example shows how to use dragger modules to modify objects in a 3D viewer. Download You can download the example network here\n","content":"Surface Example 4: Interactively Moving WEM This example shows how to use dragger modules to modify objects in a 3D viewer. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/image_processing/example2/","title":"Masking Images","summary":"Image Processing Example 2: Masking Images In this example, we create a simple mask on an image, so that background voxels are not affected by changes of the window/level values.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is shown in the right viewer of the SynchroView2D but with a Threshold-based Mask.\nDownload You can download the example network here\n","content":"Image Processing Example 2: Masking Images In this example, we create a simple mask on an image, so that background voxels are not affected by changes of the window/level values.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is shown in the right viewer of the SynchroView2D but with a Threshold-based Mask.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/open_inventor/example2/","title":"Mouse Interactions in an Open Inventor Scene","summary":"Open Inventor Example 2: Mouse Interactions in an Open Inventor Scene This example shows how to implement object interactions.\nSummary A SoExaminerViewer is used to render a SoCube object. The SoMouseGrabber is used to identify mouse interactions in the viewer and to resize the cube.\nDownload You can download the example network here\n","content":"Open Inventor Example 2: Mouse Interactions in an Open Inventor Scene This example shows how to implement object interactions.\nSummary A SoExaminerViewer is used to render a SoCube object. The SoMouseGrabber is used to identify mouse interactions in the viewer and to resize the cube.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/open_inventor/example1/","title":"Open Inventor objects","summary":"Open Inventor Example 1: Open Inventor Objects In this example, a simple Open Inventor scene is created. The Open Inventor scene shows three objects of different color and shape.\nSummary A SoExaminerViewer is used to render Open Inventor scenes in 3D. The SoBackground module defines the background of the whole scene.\nThree 3D objects are created (SoCone, SoSphere, and SoCube) having a defined SoMaterial module for setting the DiffuseColor of the object. The cube and the cone are also transformed by a SoTransform module, so that they are located next to the centered sphere.\n","content":"Open Inventor Example 1: Open Inventor Objects In this example, a simple Open Inventor scene is created. The Open Inventor scene shows three objects of different color and shape.\nSummary A SoExaminerViewer is used to render Open Inventor scenes in 3D. The SoBackground module defines the background of the whole scene.\nThree 3D objects are created (SoCone, SoSphere, and SoCube) having a defined SoMaterial module for setting the DiffuseColor of the object. The cube and the cone are also transformed by a SoTransform module, so that they are located next to the centered sphere.\nIn the end, all three objects including their materials and transformations are added to the SoExaminerViewer by a SoGroup.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/basic_mechanisms/macro_modules_and_module_interaction/example1/","title":"Panel for the Contour Filter","summary":"Example 1: Panel for the Contour Filter This example contains an entire package structure. Inside, you can find the example contour filter for which a panel was created.\nSummary A new macro module Filter has been created. Initially, macro modules do not provide an own panel containing user interface elements such as buttons. The Automatic Panel is shown on double-clicking the module providing the name of the module.\nIn this example we update the .script file of the Filter module to display the kernel selection field of the Convolution module within its network.\n","content":"Example 1: Panel for the Contour Filter This example contains an entire package structure. Inside, you can find the example contour filter for which a panel was created.\nSummary A new macro module Filter has been created. Initially, macro modules do not provide an own panel containing user interface elements such as buttons. The Automatic Panel is shown on double-clicking the module providing the name of the module.\nIn this example we update the .script file of the Filter module to display the kernel selection field of the Convolution module within its network.\nInfo:\u0026nbsp; Changes applied to fields in the macro module\u0026rsquo;s panel are applied to their internal network as well. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/surface_objects/example2/","title":"Processing and Modifying of WEMs","summary":"Surface Example 2: Processing and Modifying of WEMs This example shows how to process and modify WEMs using the modules WEMModify, WEMSmooth, and WEMSurfaceDistance. Download You can download the example network here\n","content":"Surface Example 2: Processing and Modifying of WEMs This example shows how to process and modify WEMs using the modules WEMModify, WEMSmooth, and WEMSurfaceDistance. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/basic_mechanisms/macro_modules_and_module_interaction/example2/","title":"Python Scripting","summary":"Example 2: Python Scripting This example shows how to create module interactions via Python scripting.\nSummary A new macro module IsoCSOs is created providing two viewers in its internal network, View2D and SoExaminerViewer. Both viewers are included in the panel of the module.\nTo showcase how Python functions can be implemented in MeVisLab and called from within a module, additional buttons to browse directories and create contours via the CSOIsoGenerator are added. Lastly, a field listener is implemented that reacts to field changes by colorizing contours when the user hovers over them with the mouse.\n","content":"Example 2: Python Scripting This example shows how to create module interactions via Python scripting.\nSummary A new macro module IsoCSOs is created providing two viewers in its internal network, View2D and SoExaminerViewer. Both viewers are included in the panel of the module.\nTo showcase how Python functions can be implemented in MeVisLab and called from within a module, additional buttons to browse directories and create contours via the CSOIsoGenerator are added. Lastly, a field listener is implemented that reacts to field changes by colorizing contours when the user hovers over them with the mouse.\nDownload The files need to be added to a package. You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/image_processing/example3/","title":"Region Growing (Segmentation)","summary":"Image Processing Example 3: Region Growing (Segmentation) In this example, we create a simple mask on an image by using the RegionGrowing module.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is used as input for the RegionGrowing module. The starting point for the algorithm is a list of markers created by the SoView2DMarkerEditor. As the RegionGrowing may leave gaps, an additional CloseGap module is added. The resulting segmentation mask is shown as an overlay on the original image via SoView2DOverlay.\n","content":"Image Processing Example 3: Region Growing (Segmentation) In this example, we create a simple mask on an image by using the RegionGrowing module.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is used as input for the RegionGrowing module. The starting point for the algorithm is a list of markers created by the SoView2DMarkerEditor. As the RegionGrowing may leave gaps, an additional CloseGap module is added. The resulting segmentation mask is shown as an overlay on the original image via SoView2DOverlay.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/image_processing/example4/","title":"Subtract 3D Objects","summary":"Image Processing Example 4: Subtract 3D Objects In this example, we subtract a sphere from another WEM.\nSummary We are loading images by using the LocalImage module and render them as a 3D scene in a SoExaminerViewer. We also add a sphere that is then subtracted from the original surface.\nDownload You can download the example network here\n","content":"Image Processing Example 4: Subtract 3D Objects In this example, we subtract a sphere from another WEM.\nSummary We are loading images by using the LocalImage module and render them as a 3D scene in a SoExaminerViewer. We also add a sphere that is then subtracted from the original surface.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/visualization/example1/","title":"Synchronous View of Two Images","summary":"Visualization Example 1: Synchronous View of Two Images This simple example shows how to load an image and apply a basic Convolution filter to the image. The image with and without filter is shown in a viewer and scrolling is synchronized, so that the same slice is shown for both images.\nDownload You can download the example network here\n","content":"Visualization Example 1: Synchronous View of Two Images This simple example shows how to load an image and apply a basic Convolution filter to the image. The image with and without filter is shown in a viewer and scrolling is synchronized, so that the same slice is shown for both images.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/visualization/example5/","title":"Volume Rendering and Interactions","summary":"Visualization Example 5: Volume Rendering and Interactions This example shows the volume rendering of a scan. The texture of the volume is edited and animations are implemented. Download You can download the example network here\n","content":"Visualization Example 5: Volume Rendering and Interactions This example shows the volume rendering of a scan. The texture of the volume is edited and animations are implemented. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/visualization/example6/","title":"Volume Rendering vs. Path Tracing","summary":"Visualization Example 6.1: Volume Rendering vs. Path Tracing This example shows a comparison between Volume Rendering and Path Tracing. The same scene is rendered and the camera interactions in both viewers are synchronized. Download You can download the example network here\n","content":"Visualization Example 6.1: Volume Rendering vs. Path Tracing This example shows a comparison between Volume Rendering and Path Tracing. The same scene is rendered and the camera interactions in both viewers are synchronized. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/pull/133/examples/data_objects/surface_objects/example5/","title":"WEM - Primitive Value Lists","summary":"Surface Example 5: WEM - Primitive Value Lists This example shows how to use Primitive Value Lists (PVLs). With the help of PVLs, the distance between the surfaces of WEMs is color-coded. Download You can download the example network here\n","content":"Surface Example 5: WEM - Primitive Value Lists This example shows how to use Primitive Value Lists (PVLs). With the help of PVLs, the distance between the surfaces of WEMs is color-coded. Download You can download the example network here\n","tags":[],"section":"examples"}]