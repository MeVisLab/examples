[{"date":"1655276193","url":"https://mevislab.github.io/examples/introduction/introduction/","title":"Introduction to MeVisLab","summary":"Tutorial Introduction Welcome to MeVisLab!\nMore than 20 years of experience and the continuous implementation of adaptations made MeVisLab one of the most powerful development platforms for medical image processing. Several applications and their prototypes are based on and could be realized because of MeVisLab, including software assistants for neuro-imaging, dynamic image analysis, surgery planning, and cardiovascular analysis.\nMeVisLab is a development environment for rapid prototyping and product development of medical and industrial imaging applications.","content":"Tutorial Introduction Welcome to MeVisLab!\nMore than 20 years of experience and the continuous implementation of adaptations made MeVisLab one of the most powerful development platforms for medical image processing. Several applications and their prototypes are based on and could be realized because of MeVisLab, including software assistants for neuro-imaging, dynamic image analysis, surgery planning, and cardiovascular analysis.\nMeVisLab is a development environment for rapid prototyping and product development of medical and industrial imaging applications. It includes a Software Development Kit (SDK) and an ApplicationBuilder for deploying your applications to end-customers.\nIn turn, the MeVisLab SDK consists of an Integrated Development Environment (IDE) for visual programming and the advanced text editor MATE for Python scripting, providing code completion, debugging, profiling and automated test development as well as execution.\nYou can re-use thousands of pre-defined Modules for image processing (2D up to 6D images) and visualization, combine them or even build your own.\nA quick introduction on available modules and example networks will be given in the following tutorials.\nStructure and usage of provided tutorials This tutorial is a hands-on training. You will learn about basic mechanics and features of MeVisLab.\nStarting with this introduction, we will be leading you through all relevant aspects of the user interface, commonly used functionalities and provide you with all the basic knowledge you need to build your own web applications.\nAdditional information is accessible through embedded links, forwarding you to a related glossary entry or tutorial and shortcuts, advice and hints will be highlighted as shown here.\nThe tutorials are divided into chapters by their topic and each chapter contains at least one example for you to try. You find them at the end of the tutorial or, also sorted by chapters, under the menu entry Examples. The examples under the designated menu entry are more suitable if you already have a little experience and rather search for inspiration than for explanations.\nStarting MeVisLab for the first time Right after installation of MeVisLab, you will find some new icons on your Desktop (if selected during setup).\nMeVisLab Desktop Icons (Windows) Use the top middle icon to start the MeVisLab IDE. You can also start the integrated text editor MATE or the ToolRunner. For this tutorial, you will generally require the IDE.\nWarning:\u0026nbsp; Maybe postpone the usage of the QuickStart icons as they can cause created packages not to be loaded. MeVisLab IDE User Interface First, start the MeVisLab IDE. After the Welcome Screen, the standard user interface opens.\nMeVisLab IDE User Interface Workspace By default, MeVisLab starts with an empty workspace.\nThis is where you will be developing and editing networks. Networks essentially form the base of all processing and visualization pipelines, so the workspace is, where the visual programming is done.\nViews Area The standard Views Area contains the Output Inspector and Module Inspector. With the help of the Output Inspector, you can visualize the modules output.\nInfo:\u0026nbsp; Further information on each module, e. g. about module parameters, can be found using the Module Inspector. Debug Output Debugging information can be found using the Debug Output.\nThe MeVisLab IDE and its layout are completely configurable. You can rearrange the items and add new views via [ Main Menu \u0026rarr; View \u0026rarr; Views ].\nFiletypes used in, for and with MeVisLab Extension Description .mlab Network file, includes all information about the networks modules, their settings, their connections, and module groups. Networks developed using the MeVisLab SDK are stored as .mlab files and can only be opened having a valid SDK license. .def Module definition file, necessary for a module to be added to the common MeVisLab module database. May also include all MDL script parts (if they are not sourced out to the .script file). .script MDL script file, typically includes the user interface definition of panels. See Chapter GUI Development for an example on GUI programming. .mlimage MeVisLab internal image format for 6D images saved with all DICOM tags, lossless compression, and in all data types. .mhelp File with descriptions of all fields and possible use-cases of a module, edit- and creatable by using MATE. See Help files for details. .py Python file, used for scripting in macro modules. See Python scripting for an example on macro programming. .dcm DCM part of the imported DICOM file, see Importing DICOM Data. Module types Info:\u0026nbsp; Modules are the basic entities the MeVisLab concept is built upon. They provide the functionalities to process, display and interact with images. The three existing module types (ML, Open Inventor and macro module) can be distinguished by their colors:\nType Appearance Characteristics ML module (blue) ML module Page-based, demand-driven processing of voxels. Open Inventor module (green) Open Inventor module Visual scene graphs (3D). Usually starting with So (for Scene object) as a naming convention. Macro module (brown) Macro module Combination of other module types, allowing the implementation of hierarchies and scripted interaction. Invalid modules If a module is invalid, it is displayed in bright red. This might happen if the module itself is not available for your system.\nAppearance Explanation Invalid module Invalid module Macro State Invalid Macro containing an invalid module As you can see, the number of warning and error messages that are being printed to the debug console are listed in the upper right corner of the module. This is intentional, as it enables the developer to quickly find the module causing the errors.\nCheck:\u0026nbsp; Once the debug console is cleared, the warning and error indicators next to the module are also cleared. Informational messages are indicated in a similar matter on the same spot, but in a subtle grey color.\nModule interactions through the Context Menu Each module has a context menu, providing the following options:\nContext Menu of a module Show Internal Network: Macro modules provide an entry to open the internal network. You can see what happens inside a macro module. The internal network may also contain other macro modules. Show Window: If a module does not provide an User Interface, you will see the automatic panel, showing the module\u0026rsquo;s name. Modules may additionally have one or more windows which can be opened. You can also open the Scripting Console of a module to integrate Python. Instance Name: You can edit or copy the instance name. Renaming can be useful if the same module appears more than once in one network and/or if you want to access and distinguish the modules in your Python script. Help: The menu entry Help provides access to the Module Help pages and to an example network where the module is used. This example network often helps to understand which additional modules can be added to create your desired effect. Extras: Automated tests written for the specific module can be executed here. You can also run this module in a separate process. Reload Definition: In case you are currently working on a module, you may need to reload the definition so that your changes are applied on the module (for example attached Python scripts). Related Files: Related files allows a quick access to the modules *.script or *.py files. The files are automatically opened in MATE for editing. Show Enclosing Folder: This entry opens the directory where your module is stored. Grouping: Multiple modules can be clustered and the groups can be named. This adds clarity to the structure of your network. In addition to that, grouped modules can be converted to local- or global macro modules easily. Input and Output Connectors As the creation of a network requires connected modules, each module has input and output connectors, located on their top and bottom side. Data is transmitted from the output connector on the top side of one module to the input connector on another module\u0026rsquo;s bottom side.\nOnce again, three types can be distinguished:\nAppearance Shape Definition Triangle - ML Image triangle ML images Circle - Inventor Scene half-circle Inventor scene Square - Base Object square Base objects: Pointers to data structures Info:\u0026nbsp; A connection can be established by dragging one module close to the other. Some modules even contain hidden connectors in addition to the ones displayed on the module\u0026rsquo;s surface. Click on the workspace and press SPACE to see the hidden connectors as well as the internal networks of each module. You can now also use the hidden connectors for building connections.\nFor more information about connectors and different types of connections click here . If you want to know more about establishing, interrupting, moving and replacing connections, have a look at this. Parameter Connections Besides through a module\u0026rsquo;s input and output connectors, connections can also be established between parameters in the module\u0026rsquo;s panel.\nCheck:\u0026nbsp; An exemplary use-case for a parameter connection is synchronization. Have a look here. Macro modules Info:\u0026nbsp; The creation of macros is furtherly explained in Tutorial Chapter I - Example 2.2 Adding modules to your workspace There are several ways to add a module to your current network:\nvia the menu bar entry [ Modules ]. via [ Quick Search ]. via the View Module Search. via the View Module Browser. via copy and paste from another network. by scripting, see the Scripting Reference Both the menu entry[ Modules ] and the Module Browser display all available modules. The modules are sorted hierarchically by topic and name, as defined in the file Genre.def.\nTherefore, both places are a good starting point when in need of a specific function, like an ImageLoad module.\nModules Menu and Module Browser The advantage of the Module Browser is that you can right-click the entries, open the context menu and, for example, open the help (in your default Internet browser) or the module files (in MATE, the in-built text editor).\nCheck:\u0026nbsp; For a module to be listed, it has to be available in the SDK or in your self-defined packages. A detailed tutorial on how to create packages, can be found here. If in doubt or missing something, check out the loaded packages in the preferences. Usually the quickest way to add modules to a network is the quick search in the menu bar. It offers you the possibility to search for modules by module name. By default, the search will also be extended to keywords and substrings and is case-insensitive. To change these settings, click the magnifier button for the search options.\nQuick Search Options Info:\u0026nbsp; Any time you enter something in the MeVisLab GUI while not being in a dialog window, your entry will be put into the quick search automatically. Use the \u0026uarr; ArrowUp and \u0026darr; ArrowDown keys on your keyboard to move to one of the listed modules. The module\u0026rsquo;s decription will appear next to it, allowing you to decide if this is the right module for your use-case.\nQuick Search Results Tip:\u0026nbsp; For a more complex search, use the Module Search View. ","tags":["Tutorial","Introduction","Glossary","Modules","ML Module","Filetypes","UI","Workspace","Search"],"section":"introduction"},{"date":"1655276093","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/","title":"Chapter I: Basic Mechanisms of MeVisLab","summary":"Basic Mechanics of MeVisLab (Example: Building a Contour Filter) In this chapter you will learn the basic mechanisms of the MeVisLab IDE. You will learn how to re-use existing modules to load and view data and you will build your first processing pipeline.\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Extra Infos:\u0026nbsp; Additional information on the basics of MeVisLab are explained here Loading Data First, we need to load the data we would like to work on, e.","content":"Basic Mechanics of MeVisLab (Example: Building a Contour Filter) In this chapter you will learn the basic mechanisms of the MeVisLab IDE. You will learn how to re-use existing modules to load and view data and you will build your first processing pipeline.\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Extra Infos:\u0026nbsp; Additional information on the basics of MeVisLab are explained here Loading Data First, we need to load the data we would like to work on, e.g. a CT scan. In MeVisLab, modules are used to perform their associated specific task, they are the basic entities you will be working with. Each module has a different functionality for processing, visualization and interaction. Connecting modules enables the development of complex processing pipelines. You will get to know different types of modules throughout the course of this tutorial.\nStarting off, we will add the module ImageLoad to our network to load our data. The module can be found by typing its name into the search bar on the top-right corner and is added to your network by clicking it.\nSearch for ImageLoad Next, we select and load the data we\u0026rsquo;d like to process. Double-click the module ImageLoad to open its panel. You can browse through your folders to select the data you\u0026rsquo;d like to open. Example data can be found in the MeVisLab DemoData directory $(InstallDir)/Packages/MeVisLab/Resources/DemoData located in the MeVisLab installation path. Select a file, for example an MRI scan of a shoulder Shoulder_Fracture.tif. The image is loaded immediately and basic information of the loaded image can be seen in the Panel.\nExtra Infos:\u0026nbsp; There also are modules to load multiple other formats of data. These are the most common ones:\nDicomImport to load DICOM Images LocalImage to load any image format For a more detailed description on loading DICOM images, look here\nThe Output-Inspector and the Module Inspector To inspect and visualize the loaded data, we can use the Output Inspector located in the Views area. You can already interact with the image using the mouse wheel and mouse buttons / . To preview the image, click on the triangle on the top side of the module ImageLoad, which offers the module\u0026rsquo;s output. All module outputs can be found at the top side of the respective module.\nYou can now inspect your image in 2D:\nOutput Inspector Your image does not look like this? One reason might be that the slice of the image you are looking at has no information. Click on the Output Inspector and scroll through the slices (This process is called \u0026ldquo;Slicing\u0026rdquo;) by using the mouse wheel . Still not seeing anything? Then try to adjust the contrast of the given image by keeping the right mouse button pressed while moving the mouse.\nYou are not restricted to 2D. The Output Inspector offers a 3D View of most loaded images. Try to click on the 3D-tab located in the Output Inspector. The 3D display of the image can be rotated by left-clicking on the image and moving the courser around. The little cube in the lower right corner of the viewer shows the orientation of the image.\nNotation:\u0026nbsp; A = anterior, front P = posterior, back R = right side L = left side H = head F = feet Below the Output Inspector, you\u0026rsquo;ll find the Module Inspector. The Module Inspector displays properties and parameters of the selected module. Parameters are stored in so called Fields. Using the Module Inspector you can examine different fields of your ImageLoad module. The module has, for example, the fields filename (the path, the loaded image is stored in), as well as sizeX, sizeY and sizeZ (the size of the loaded image).\nModule Inspector Viewer Instead of using the Output Inspector to inspect images, we\u0026rsquo;d suggest to add another viewer to the network. Search for the module View2D and add it to your workspace. Most modules have different connector options. Data is generally transmitted from the top side of a module to another modules bottom side.\nThe module View2D has one input connector for voxel images (triangle-shaped) and three other possible input connectors (Shaped like half-circles) on the bottom. The half-circle-shaped input connectors will be explained later on. Generally, module outputs can be connected to module inputs with the same symbol and thus transmit information and data between those modules.\n2D Viewer You can now display the loaded image in the newly added viewer module by connecting the output of the module ImageLoad to the input connector of the module View2D. Follow these steps to do so:\nClick the output connector of ImageLoad.\nKeep the left mouse button pressed while dragging the connection to the input connector of View2D (white line).\nCheck if the connection is well-defined (green line).\nRelease the mouse button on the input connector of your View2D-module to establish the connection.\nEstablish connection Although the connection is established, no image rendering has started yet. To initialize rendering, open the View2D panel by double-clicking on the module. Similar to the Output Inspector, you can scroll through the slices and set different levels of contrast. The amount of displayed annotations is altered by pressing A on the keyboard (annotation-mode).\nView2D Panel By dragging the connection away from either the input or the output connector, the connection is interrupted.\nConnections between compatible outputs and inputs are established automatically if two modules get close enough to each other.\nExtra Infos:\u0026nbsp; Connecting, Disconnecting, Moving and Replacing Connections is furtherly explained here Image Processing An average kernel will be used to smooth the image out as our next step will be to actually process our image. Add the Convolutionmodule to your workspace and disconnect the View2Dmodule from the ImageLoadmodule by clicking on the connection and pressing DEL . Now, you can build new connections from the module ImageLoad to the module Convolution and the Convolutionmodule to View2D.\nConvolution Module Open the panel of the Convolution module by double-clicking it. The panel allows configuration of the module. You can adjust parameters or select a kernel. We will be using the 3x3 Average Kernel for now.\nSelect a Kernel The module View2D is now displaying the smoothed image.\nTo compare the processed and unprocessed image, click on the output connector of the module ImageLoad to display the original image in the Output Inspector. The Output Inspectors greatest advantage is, that it\u0026rsquo;s able to display the output of any connector in the process chain (as long as an interpretable format is used). Simply click the connector or connection to find out more about the module output.\nYou can also inspect changes between processed (output connector) and unprocessed (input connector) images by adding a second or even third viewer to your network.\u0026ldquo;Layers\u0026rdquo; of applied changes can be inspected next to each other using more than one viewer and placing as well as connecting them accordingly. We will be using a second View2D-module. Notice how the second Viewer is numbered for you to be able to distinguish them better. It might be important to know at this point, that numerous connections can be established from one output-connector but an input-connector can only receive one stream of data. Please connect the module ImageLoad to the second viewer to display the images twice. You can now scroll through the slices of both viewers and inspect the images.\nMultiple Viewers Parameter Connection for Synchronization You\u0026rsquo;re now able to scroll through the slices of the image in two separate windows. To examine the effect of the filter even better, we will now synchronize both viewers.\nWe already know data connections between module inputs and outputs. Besides module connections, it is also possible to connect the fields within the panels of the modules via parameter connection. The values of connected fields are synchronized, which means that the changing value of one field will be adapted to all other connected fields.\nIn order to practise establishing parameter connections, add the SyncFloat module to your workspace.\nSyncFloat Module We will be synchronizing the startSlice fields of our viewers, to be able to directly compare the effect our processing module has on the slices: Right-click the viewer View2D to open its context menu and select [ Show Window \u0026rarr; Automatic Panel ].\nAutomatic Panel View2D Doing so shows all parameter fields of the module View2D.\nSearch for the field startSlice. The field indicates which slice is currently shown in the viewer. If you scroll through the slices of an image the value of startSlice changes.\nNow, double-click the module SyncFloat to open its panel.\nClick on the label startSlice in the automatic panel of the module View2D, keep the button pressed and drag the connection to the label Float1 in the panel of the module SyncFloat.\nSynchronize StartSlice The connection is drawn as a thin grey arrow between both modules with the arrowhead pointing to the module that receives the field value as input. The value of the field starSlice is now transmitted to the field Float1. Changing StartSlice automatically changes Float1, but not the other way round.\nParameter Connection StartSlice We will now establish a connection from the module SyncFloat to the second viewer, Viewer2D1. In order to do that, open the automatic panel View2D1. Draw a connection from the label Float2 of the panel of the module SyncFloat to the label startSlice in the automatic panel of the module View2D1. Lastly, implement a connection between the parameter fields startSlice of both viewers. Draw the connection from View2D1 to View2D.\nSynchronize both directions As a result, scrolling through the slices with the mouse wheel in one of the viewers synchronizes the rendered slice in the second viewer. In this case, you can inspect the differences between smoothed and unsmoothed data on every single slice.\nYour final Network It is also possible to use the pre-defined module SynchroView2D to accomplish a similar result.(SynchroView2D\u0026rsquo;s usage is futherly described in this chapter ).\nGrouping modules A contour filter can be created based on our previously created network. To finalize the filter, add the modules Arithmetic2 and Morphology to your workspace and connect the modules as shown below. Double-click the module Arithmetic2 to open its panel. Change the field Function of the module Arithmetic2 to use the function subtract in the panel of the module. The contour filter is done now. You can inspect each processing step using the Output Inspector by clicking on the input and output connectors of the respective modules. The final results can be displayed using the viewer modules. If necessary, adjust the contrast by pressing the right arrow key and moving the cursor.\nGrouping modules If you\u0026rsquo;d like to know more about specific modules, search for help. You can do this by right-clicking the module and select help, which offers an example network and further information about the selected module in particular.\nModule Help To be able to better distinguish the image processing pipeline, you can encapsulate it in a group: Select the three modules, for example by dragging a selection rectangle around them. Then right-click the selection to open the context menu and select [ Add to New Group ].\nAdd to new group Enter a name for the new group, for example Filter. The new group is created and displayed as a green rectangle. The group allows for quick interactions with all its modules.\nYour Filter Group Your network got very complex and you lost track? No problem. Let MeVisLab arrange your modules automatically via [ Mein Menu \u0026rarr; Edit \u0026rarr; Auto Arrange Selection ] (or via keyboard shortcut CTRL + 1 ).\nNow, it is time to save your first network. Open the tab [ File \u0026rarr; Save ] to save the network in an .mlab file.\nExtra Infos:\u0026nbsp; More information on module groups can be found here Macro modules You have probably already noticed how the modules differ in color. Each color represents another type of module:\nThe blue modules are called ML modules, they process voxel images. Green modules are OpenInventor modules, they enable visual 3D scene graphs. The brown modules are called macro modules. Macro modules encapsulate a whole network in a single module. To condense our filter into one single module, we will now be creating a macro module out of it. To do that, right-click on the group title and select Convert To Local Macro. Name your new macro module and finish. You just created a local macro module. Local macro modules can only be used in the network they were created in.\nConvert to local macro Your first local macro Right-click the macro module and select Show Internal Network to inspect and change the internal network. You can change the properties of the new macro module by changing the properties in the internal network. You can, for example, right-click the module Convolution and change the kernel. These changes will be preserved.\nInternal Network of your local macro \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Extra Infos:\u0026nbsp; Module handling is explained here\nMore information on macro modules can be found here\nSummary MeVisLab provides pre-defined modules you can re-use and connect for building more or less complex networks. Each module\u0026rsquo;s output can be previewed using the Output Inspector. Each module provides example networks to explain their usage. Parameters of each module can be changed in the Module Inspector or Automatic Panel of the module. Parameter connections can be established to synchronize the values of these parameters. Modules can be clustered. Clustered modules can be encapsulated into local or global macro modules. Macro modules encapsulate networks. Internal networks can be shown and modified. Any changes of the internal network are applied to the macro module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Macro","Macro modules","Local Macro"],"section":"tutorials"},{"date":"1655276093","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/dataimport/","title":"Example 1: Data import in MeVisLab","summary":"Example 1: Data Import in MeVisLab MeVisLab provides several pre-defined modules to import data for processing in your networks.\nExtra Infos:\u0026nbsp; The easiest way to load data in MeVisLab is to drop the file onto the MeVisLab workspace. MeVisLab will try to find a module that is capable of loading your file automatically. These chapters explain the data formats and modules related to this example:\nImages DICOM Data Segmentations / 2D Contours 3D Data / Meshes Extra Infos:\u0026nbsp; Detailed explanations on loading images onto your MeVisLab workspace can be found here Images A good option to load images is the ImageLoad module.","content":"Example 1: Data Import in MeVisLab MeVisLab provides several pre-defined modules to import data for processing in your networks.\nExtra Infos:\u0026nbsp; The easiest way to load data in MeVisLab is to drop the file onto the MeVisLab workspace. MeVisLab will try to find a module that is capable of loading your file automatically. These chapters explain the data formats and modules related to this example:\nImages DICOM Data Segmentations / 2D Contours 3D Data / Meshes Extra Infos:\u0026nbsp; Detailed explanations on loading images onto your MeVisLab workspace can be found here Images A good option to load images is the ImageLoad module. ImageLoad Module The ImageLoad module can import the following formats:\nDICOM TIFF DICOM/TIFF RAW LUMISYS PNM Analyze PNG JPEG MLImageFileFormat Basic information of the imported images are available on the Panel which opens via double-click.\nDICOM data Extra Infos:\u0026nbsp; Additional information about Digital Imaging and Communications in Medicine (DICOM) can be found at Wikipedia Even if the above explained ImageLoad is able to import DICOM data, a much better way is to use one of the specialized modules for DICOM images such as DicomImport.\nThe DicomImport module allows to define a directory containing DICOM files to import as well as a list of files which can be dropped to the UI and imported. After import, the volumes are shown in a patient tree providing the following patient, study, series and volume information (depending on the availability in the DICOM file(s)):\nPATIENT LEVEL Patient Name (0010,0010) - Patient Birthdate (0010,0030) STUDY LEVEL Study Date (0008,0020) - Study Description (0008,1030) SERIES/VOLUME LEVEL Modality (0008,0060) - Series Description (0008,103e) - Rows (0028,0010) - Columns (0028,0011) - number of slices in volume - number of time points in volume DicomImport Module Configuration The DicomImport module generates volumes based on the Dicom Processor Library (DPL) which allows to define sorting and partitioning options.\nDicomImport Sort Part Configuration DicomTree information In order to get all DICOM tags from your currently imported and selected volume, you can connect the DicomImport module to a DicomTagBrowser.\nDicomTagBrowser Module Segmentations / 2D Contours 2-dimensional contours in MeVisLab are handled via CSOs (Contour Segmentation Objects).\nExtra Infos:\u0026nbsp; Tutorials for CSOs are available here\nDetailed explanations about CSOs can be found here\nThe CSO library provides data structures and modules for an interactive or automatic generation of contours in voxel images. Furthermore, these contours can be analyzed, maintained, grouped, and converted into a voxel image or a set of markers.\nCSOs can be created by the existing SoCSO*Editor modules. The following modules are available:\nSoCSOPointEditor SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor SoCSODistancePolylineEditor SoCSOEllipseEditor SoCSORectangleEditor SoCSOIsoEditor SoCSOSplineEditor SoCSOPolygonEditor SoCSOLiveWireEditor For saving and loading existing CSOs, the modules CSOSave and CSOLoad can be used.\n3D data / meshes Winged Edge Mesh (WEM) 3-dimensional meshes in MeVisLab are handled via WEMs (Winged Edge Mesh).\nThe module WEMLoad loads different 3D mesh file formats like:\nObject File Format (*.off *.geom) Wavefront (*.obj) Polygon File Format (*.ply) Standard Tessellation Language (*.stl) VRML (*.wrl) Winged Edge Mesh (*.wem) WEMLoad Module WEMs can be rendered via Open Inventor by using the modules SoExaminerViewer or SoRenderArea and SoCameraInteraction.\nBefore visualizing a WEM, it needs to be converted to a Scene Object via SoWEMRenderer.\nSoWEMRenderer Module Extra Infos:\u0026nbsp; Tutorials for WEMs are available here. Loading arbitrary 3D files The SoSceneLoader module is able to load external 3D formats. MeVisLab uses the integrated assimp ThirdParty library which is able to import most common 3D file types. The currently integrated assimp version can be found here Extra Infos:\u0026nbsp; Supported file formats of the assimp library are documented on their website. SoSceneLoader Module The SoSceneLoader module generates a 3D scene from your loaded files which can be rendered via SoExaminerViewer or SoRenderArea and SoCameraInteraction Extra Infos:\u0026nbsp; Example usage is explained in the tutorials for Open Inventor. ","tags":["Beginner","Tutorial","Data Import","DICOM"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/coordinatesystems/coordinatesystems/","title":"Example 1.1: MeVisLab Coordinate Systems","summary":"Example 1.1: MeVisLab Coordinate Systems Three coordinate systems exist next to each other:\nWorld coordinates Voxel coordinates Device coordinates World coordinate systems in MeVisLab are always right handed.\nThe blue rectangle shows the same region in the three coordinate systems.\nCoordinate Systems in MeVisLab World coordinates World coordinates are:\nGlobal: Combine several objects in a view Isotropic: All directions are equivalent Orthogonal: Coordinate axes are orthogonal to each other The origin of the world coordinate system can be anywhere and is not clearly defined.","content":"Example 1.1: MeVisLab Coordinate Systems Three coordinate systems exist next to each other:\nWorld coordinates Voxel coordinates Device coordinates World coordinate systems in MeVisLab are always right handed.\nThe blue rectangle shows the same region in the three coordinate systems.\nCoordinate Systems in MeVisLab World coordinates World coordinates are:\nGlobal: Combine several objects in a view Isotropic: All directions are equivalent Orthogonal: Coordinate axes are orthogonal to each other The origin of the world coordinate system can be anywhere and is not clearly defined. Origins of the other coordinate systems can always be mapped to the world coordinate system. In case of DICOM images, this mapping is defined by DICOM tags.\nWorld coordinates in MeVisLab You can show the world coordinates in MeVisLab by using the following example network:\nWorld Coordinates in MeVisLab The ConstantImage module generates an artificial image with a certain size, data type and a constant fill value. The origin of the image is at the origin of the world coordinate system, therefore the SoCoordinateSystem module shows the world coordinate system.\nConstantImage Info Placing an object into the Open Inventor Scene of the SoExaminerViewer, in this case a SoCube with width, height and depth of 10, places the object to the origin of the world coordinate system.\nSoCube in world coordinate system Translations You can move an object in your scene, for example by using a SoTranslation module. Update your network and add the module before your cube. Defining a translation vector 50, 0, 0 moves your cube by 50 in x-direction based on the origin of the world coordinate system.\nSoTranslation Transformations More complex transformations can be done by using the SoTransform module. You can not only translate an existing object, but also rotate, scale and apply many other transformations.\nSoTransform \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Voxel coordinates Voxel coordinates are:\nRelative to an image Continuous from [0..x,0..y,0..z], voxel center at 0.5 Direct relation to voxel location in memory Voxel coordinates in MeVisLab You can show the voxel coordinates in MeVisLab by using the following example network:\nVoxel Coordinates Load the file Liver1_CT_venous.small.tif .The Info module shows detailed information about the image loaded by the LocalImage. Opening the SoExaminerViewer shows the voxel coordinate system of the loaded image. You may have to change the LUT in SoGVRVolumeRenderer so that the image looks better.\nVoxel coordinates of the loaded image The Advanced tab of the Info module shows the world coordinates of the image. In this case, the origin of the voxel coordinate system is located at -186.993, -173.993, -249.993.\nIn addition to that, you can see a scaling which has been done on the image. The voxel sizes are shown in the diagonal values of the matrix as 3.985792, 3.985792, 3.985798.\nWorld coordinates of the loaded image You can change the scaling to 1 by adding a Resample3D module to the network, Set the voxel size to 1, 1, 1 and inspect the Info module.\nResample3D Image Info after Resampling The voxel size is now 1.\nYou can add this network to the world coordinate system network developed above and see both coordinate systems.\nWorld coordinates of the loaded image Opening the SoExaminerViewer shows the world coordinate system in white and the voxel coordinate system in yellow.\nWorld and Voxel coordinates On the yellow axis, we can see that the coordinate systems are located as already seen in the Info module Advanced tab. On the x-axis, the voxel coordinate origin is translated by -186.993 and on the y-axis by -173.993.\nYou can also add a SoVertexProperty and a SoLineSet module and configure a line from the origin of the world coordinate system 0, 0, 0 to the origin of the voxel coordinate system as defined by the image -186.993, -173.993, -249.993.\nSoVertexProperty \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Device coordinates Device coordinates are:\n2D coordinates in OpenGL viewport Measured in pixel Have their origin (0,0) in the top left corner of the device (with x-coordinates increasing to the right and y-coordinates increasing downwards) The viewport is the rectangle in pixels on your screen you want to render to. Affine transformations map abstract coordinates from your scene to physical pixels on your device.\nAll triangular vertices go through a projection matrix and end in a normalized range from -1 to 1 representing your field of view. To find which pixels the triangles actually cover on screen, those coordinates get linearly remapped from [−1, 1] to the range of the viewport rectangle in pixels. Technically that kind of mapping is called an affine transformation.\n","tags":["Beginner","Tutorial","Data Import","DICOM","Coordinate Systems"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/coordinatesystems/coordinatesystems2/","title":"Example 1.2: DICOM Coordinate Systems","summary":"Example 1.2: DICOM Coordinate Systems General Coordinate systems in DICOM are basically the same as world coordinates in MeVisLab (except for the 0.5 voxel offset). World coordinates also refer to the patient axes. They are:\nBased on the patient\u0026rsquo;s main body axes (transverse, coronal, sagittal) Measured as 1 coordinate unit = 1 millimeter Right-handed Not standardized regarding their origin World Coordinates in Context of the Human Body The DICOM (Digital Imaging and Communications in Medicine) standard defines a data format that groups information into data sets.","content":"Example 1.2: DICOM Coordinate Systems General Coordinate systems in DICOM are basically the same as world coordinates in MeVisLab (except for the 0.5 voxel offset). World coordinates also refer to the patient axes. They are:\nBased on the patient\u0026rsquo;s main body axes (transverse, coronal, sagittal) Measured as 1 coordinate unit = 1 millimeter Right-handed Not standardized regarding their origin World Coordinates in Context of the Human Body The DICOM (Digital Imaging and Communications in Medicine) standard defines a data format that groups information into data sets. This way, the image data is always kept together with all meta information like patient ID, study time, series time, acquisition data etc. The image slice is represented by another tag with pixel information.\nDICOM tags have unique numbers, encoded as two 16 bit numbers, usually shown in hexadecimal notation as two four-digit numbers (xxxx,xxxx). These numbers are the data group number and the data element number.\nInfo:\u0026nbsp; Although DICOM is a standard, often the data that is received / recorded does not follow the standard. Wrongly used tags or missing mandatory tags may cause problems in data processing. Some typical modules for DICOM handling:\nDirectDicomImport is a module for DICOM import that generates 3D or 4D images (as ML images) from a list of DICOM files which can directly be used by other modules. It has a lot of options to control the import process, which can, e.g., determine which slices are combined into an image stack. DicomImport is a new module for DICOM import. The new implementation does not yet provide all known functionalities from DirectDicomImport, most of them will be added in future releases. Its main advantage is that the import process is faster and happens asynchronously. You can view the the DICOM tags of a DICOM image or a processed ML image with the module DicomTagBrowser. You can view and cut out frame-specific tags with the module DicomFrameSelect. You can modify DICOM tags with the module DicomTagModify. You can also create a new DICOM header for an image file with the ImageSave module, tab Options, Save DICOM header file only. Saving of loaded DICOM data to the filesystem or sending to a PACS (Picture Archiving and Communication System) is possible with the DicomTool macro module. Basic support for querying and receiving DICOM data from a PACS is available via the DicomQuery and DicomReceiver modules. Info:\u0026nbsp; For handling and manipulating DICOM data in C++, the DICOM toolkit DCMTK (DICOM@offis) is recommended. Parts of this toolkit are also used in MeVisLab.\nAnother option for Python is pydicom.\nOrthogonal views The module OrthoView2D provides a 2D view displaying the input image in three orthogonal viewing directions. By default, the view is configured as Cube where the transverse view is placed in the top right segment, sagittal in bottom left and coronal in bottom right segment. Use the left mouse button to set a position in the data set. This position will be displayed in all available views and is available as field worldPosition.\nOrthoView2D As already learned in the previous example 1.1: MeVisLab Coordinate Systems, world and voxel positions are based on different coordinate systems. Selecting the top left corner of any of your views will not show a world position of 0, 0, 0. You can move the mouse cursor to the voxel position 0, 0, 0 as seen in the image information of the viewers in brackets (x, y, z). The field worldPosition then shows the location of the image in world coordinate system (see Info module).\nOrthoView2D Voxel- and World Position Another option is to use the module OrthoReformat3 which transforms the input image (by rotating and/or flipping) into the three main views commonly used:\nOutput 0: Sagittal view Output 1: Coronal view Output 2: Transverse view OrthoReformat3 The general View2D always uses the original view from the image data without reconstructing another view. In case of ProbandT1, this is the sagittal view.\n","tags":["Beginner","Tutorial","Data Import","DICOM","Coordinate Systems"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/","title":"Example 2: Macro modules and Module Interaction","summary":"Example 2: Macro modules Macro modules and Module Interactions via User Interface and Python Scripting MeVisLab provides different types of modules, which can be distinguished by their color. The brown modules are called Macro modules. Macro modules condense a whole network into one module. You can open the internal network by pressing the middle mouse button or via right mouse click and select [ Help \u0026rarr; Show Internal Network ]. Macro modules provide the possibility to create customized user interfaces and Python interactions.","content":"Example 2: Macro modules Macro modules and Module Interactions via User Interface and Python Scripting MeVisLab provides different types of modules, which can be distinguished by their color. The brown modules are called Macro modules. Macro modules condense a whole network into one module. You can open the internal network by pressing the middle mouse button or via right mouse click and select [ Help \u0026rarr; Show Internal Network ]. Macro modules provide the possibility to create customized user interfaces and Python interactions.\nIn Chapter I - Basic Mechanics we built a contour filter and condensed all the modules into one local Macro module. Until now, the local Macro module containing the contour filter can only be used in the current network. In the following chapters, we like to make the Macro module commonly available throughout projects and equip this Macro module with panels and help pages. Commonly available macro modules are called global macros and can be found in MeVisLab [ Module Search ]. Global macros and projects are stored in packages. A package structure makes it easy to exchange projects and different functionalities between people.\n","tags":["Beginner","Tutorial","Macro","Macro modules"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/package/","title":"Example 2.1: Package Creation","summary":"Example 2.1: Package creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Packages are the way MeVisLab organizes different development projects.\nMacro modules and projects are stored in packages. If you like to create a global macro module, you need a package in which this macro module can be stored in. In this chapter, we will create our own package. We start our package creation by creating a package group, because every package needs to be stored in a package group.","content":"Example 2.1: Package creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Packages are the way MeVisLab organizes different development projects.\nMacro modules and projects are stored in packages. If you like to create a global macro module, you need a package in which this macro module can be stored in. In this chapter, we will create our own package. We start our package creation by creating a package group, because every package needs to be stored in a package group. You can find detailed information about packages and package groups here and in the package documentation .\nSteps to do To create packages and package groups, we will use the Project Wizard. Open the Project Wizard via [ File \u0026rarr; Run Project Wizard ... ]. Then, select [ Package \u0026rarr; New Package ] and Run Wizard.\nThe Project Wizard Next you need to:\nFind a name for your package group, for example your company name or in our example the name MyPackageGroup.\nFind a name for your package, in our example we call it General.\nSelect the path your package group is supposed to be stored in (If you like to add a package to an existing package group, select its name and chose the path the package group is stored in)\nIf you now create the package, you can find a folder structure in the desired directory. The folder of your package group contains the folder of your package. We have now successfully created a package in which we can store our global macro module.\nPackage creation Summary Packages are needed to store global macro modules and projects. Package groups contain packages. Packages and package groups can be created using the Project Wizard. Detailed information about packages can be found in the package documentation . ","tags":["Beginner","Tutorial","Package"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/globalmacromodules/","title":"Example 2.2: Creation of global macro modules","summary":"Example 2.2: Global macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter you will learn how to create global macro modules. There are many ways to do this. You can convert local macros into global macro modules or you can directly create global macro modules using the Project Wizard. In contrast to local macro modules, global macro modules are commonly available throughout projects and can be found via module search and under [ Modules ].","content":"Example 2.2: Global macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter you will learn how to create global macro modules. There are many ways to do this. You can convert local macros into global macro modules or you can directly create global macro modules using the Project Wizard. In contrast to local macro modules, global macro modules are commonly available throughout projects and can be found via module search and under [ Modules ].\nSteps to do Transform a local macro module into a global macro module To transform our local macro module Filter from Chapter I into a global macro module, right-click the macro module to open the context menu and select [ Extras \u0026rarr; Convert To Global Module... ]\nConvert local macro to global macro Define module properties Choose a unique module name\nState the module author\nSelect the genre of the module. For this, browse through the module genres to select the appropriate genre. In our case, as our macro module contains a contour filter, we will choose the genre Filters.\nThe Genre defines the location where your module will be shown in MeVisLab [ Modules ] menu.\nTick the box Add reference to example network to directly create the template for an example network for your macro module.\nSelect the package you like to store the module in. We choose the package we created before. Your module is saved in an .mlab format and can be found in \\MyPackageGroup\\General\\Modules\\Macros\\MyProject.\nInfo:\u0026nbsp; Make sure to chose Directory Structure as self-contained. This makes sure that all files of your module are stored in a single directory.\nAlso keep in mind that Python files are only created automatically if selected in the Project Wizard. Converting a local macro to a global macro does NOT create a Python file automatically.\nCreate global macro module Use the Project Wizard to create global macro modules Instead of converting a local macro module into a global macro module, you can also use the Project Wizard to create new macro modules. Open the Project Wizard via [ File \u0026rarr; Run Project Wizard ... ]. Then, select [ Modules (Scripting) \u0026rarr; Macro module ] and Run Wizard.\nDefine module properties Choose a unique module name\nState the module author\nSelect the genre of the module. For this, browse through the module genres to select the appropriate genre. In our case, as our macro module contains a contour filter, we will choose the genre Filters.\nTick the box Add reference to example network to directly create the template for an example network for your macro module.\nSelect the package you like to store the module in. We choose the package we created before. Your module is saved in an .mlab format and can be found in \\MyPackageGroup\\General\\Modules\\Macros\\MyProject.\nInfo:\u0026nbsp; Make sure to chose Directory Structure as self-contained. This makes sure that all files of your module are stored in a single directory. Press Next \u0026gt; to edit further properties. You have the opportunity to directly define the internal network of the macro module, for example by copying an existing network. In this case, we could copy the network of the local macro module Filter we already created. In addition, you have the opportunity to directly create a Python file. Python scripting can be used for the implementation of module interactions and other module functionalities. More information about Python scripting can be found here.\nProjectWizard1 ProjectWizard2 Structure of global macro modules After creating your global macro module, you can find the created project MyProject in your package. This project contains your macro module Filter. For the macro module exist three files:\nFilter.def: Module definition file Filter.mlab: Network file which contains the internal network of your macro module Filter.script: MDL script file, which defines in- and outputs of your macro module as well as fields. This file defines the module panel, as well as references to python scripts. In addition, two folders may be created:\nmhelp: contains the help files of all modules of this project network: contains the example networks of all modules of this project Structure of global macro modules How to find global macro modules All available modules are categorized and can be found via [ Modules ] in the respective genre. After creating a global macro, the new module can be found via [ Modules \u0026rarr; Filters ]. In addition, you can now find your macro module via module search.\nFind module in menu Hint:\u0026nbsp; If you do not find your new global macro module, try to reload the module database. Reload module database Summary Via right-click [ Extras \u0026rarr; Convert To Global Module... ] global macro modules can be created out of local macro modules You can use the Project Wizard to create new macro modules You need to have a package structure to store your global macro module Global macro modules are available throughout projects and can be found via Module Search and under menu item [ Modules ]. ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/helpfiles/","title":"Example 2.3: Creation of module help","summary":"Example 2.3: Creation of module help Generating help of a macro module is part of the video about macro modules from Example 2: Creation of global macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter, you will learn how to create a help page and an example network. For hands-on training, we will use the macro module Filter, which was created in the previous chapter.\nDepending on the way the macro module was created the default help page and example network might or might not exist.","content":"Example 2.3: Creation of module help Generating help of a macro module is part of the video about macro modules from Example 2: Creation of global macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this chapter, you will learn how to create a help page and an example network. For hands-on training, we will use the macro module Filter, which was created in the previous chapter.\nDepending on the way the macro module was created the default help page and example network might or might not exist. In case they exist, the help page only contains information about module in- and outputs as well as module fields. The example network only contains the macro module itself. Both, the help page and the example network, can be created and edited after module creation.\nSteps to do Creation of help files using MeVisLab MATE We will start by creating a help file using the build in text editor MeVisLab MATE (MeVisLab Advanced Text Editor). If you open the context menu of your global macro module and select [ Help ], it might be, that no help page is given. We will start to create a help file by selecting [ Help \u0026rarr; Create Help ]. If a help page already exists, select [ Help \u0026rarr; Edit Help ].\nCreation of module help MeVisLab MATE opens. An *.mhelp file (Filter.mhelp) is created automatically and is stored in the folder your macro module Filter is stored in. You can find the folder structure in MATE on the left side. Editing the text field, you can edit the help file.\nEdit module help file via MATE When creating the help file of a module, all important information of the module down to the field specifications are extracted and created automatically. Thus, the basic module information is always available in the module help. Additional documentation should be added by the module author. On the left side, you can find the outline of the help file. Each section can be edited. In this example, we added the purpose of the module to the help file.\nEdit module help file via MATE MATE offers the possibility to format the text. By using the button M, module names can be formatted in such a way that links to the respective help file of the modules are created.\nEdit module help file via MATE After finishing your documentation, you can click Generate Help or F7 and your final help file is generated.\nExtra Infos:\u0026nbsp; More information on MeVisLab MATE can be found here\nThe Module Help Editor is explained here\nThe result can be seen when opening the help file via context menu in MeVisLab IDE (or by pressing F1 ).\nHelp file of the module Watch out:\u0026nbsp; Depending on the way the macro module was created, more or less features are automatically given in the help file and the example network. All missing features can be added manually. Creation of an example network To add an example network to your module, you need to add a reference to the respective *.mlab file to the module definition file (.def). Open the file Filter.def. You can find the line exampleNetwork = \u0026ldquo;$(LOCAL)/networks/FilterExample.mlab\u0026rdquo;, which defines the reference to the *.mlab file containing the example network. Per default the name of the example network is ModulenameExample.mlab. An *.mlab file containing only the module Filter is created insight the folder networks.\nIt is possible that the reference to the example network or the file FilterExample.mlab is missing. One reason could be, that its creation was not selected when creating the macro module. In this case, add the reference and the file manually.\nReference to Example Network To create the example network, open the file FilterExample.mlab in MeVisLab and create an appropriate example.\nExample Network Summary MeVisLab MATE is a build-in text editor which can be used to create module help files, module panels or to create module interactionss via Python scripting. You can create help files via the module context menu using MeVisLab Mate. You can add an example network to your macro module via the .def file ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro","Help"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/guidesign/","title":"Example 2.4: GUI development","summary":"Example 2.4: Building a Panel Layout: Interactions with macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This chapter will give you an introduction into the creation of module panels and user interfaces. For the implementation you will need to use the MeVisLab Definition Language (MDL) .\nExtra Infos:\u0026nbsp; More information about GUI design in MeVisLab can be found here Creating a panel for the macro module flilter Creation of a module panel In Example 2.","content":"Example 2.4: Building a Panel Layout: Interactions with macro modules \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This chapter will give you an introduction into the creation of module panels and user interfaces. For the implementation you will need to use the MeVisLab Definition Language (MDL) .\nExtra Infos:\u0026nbsp; More information about GUI design in MeVisLab can be found here Creating a panel for the macro module flilter Creation of a module panel In Example 2.2 we created the global macro module Filter. By now, this module does not have a proper panel. When double-clicking the module, the Automatic Panel is shown.\nThe Automatic Panel contains fields, as well as module in and outputs. In this case, no fields exists except the instanceName. Accordingly, there is no possibility to interact with the module. Only the input and the output of the module are given.\nAutomatic Panel To add and edit a panel, open the context menu and select [ Related Files \u0026rarr; Filter.script ]. The text editor MATE opens. You can see the file Filter.script, which you can edit to define a custom User Interface for the Module.\nModule script file Module interface Per default, the *.script file contains the interface of the module. In the interface section (everything insight the curled brackets behind the name Interface) you can define the module inputs, the module outputs and also all module fields (or Parameters).\nFilter.script\nInterface { Inputs { Field input0 { internalName = Convolution.input0 } } Outputs { Field output0 { internalName = Arithmetic2.output0 } } } Module inputs and outputs To create an input/output, you need to define a Field in the respective input/output environment. Each input/output gets a name (here input0/output0) which you can use to reference this field. The module input maps to an input of the internal network. You need to define this mapping. In this case the input of the macro module Filter maps to the input of the module Convolution of the internal network (internalName = Convolution.input0). Similarly, you need to define which output of the internal network maps to the output of the macro module Filter. In this example, the output of the internal module Arithmethic2 maps to the output of our macro module Filter (internalName = Arithmetic2.output0).\nCreating an input/output causes:\nInput/output connectors are added to the module. You can find placeholders for the input and output in the internal network (see image). Input/output fields are added to the automatic panel. A description of the input/output fields is automatically added to the module help file, when opening the *.mhelp file after input/output creation. Helpfile creation is explained in Example 2.3. Internal Network of your macro module Module fields In the environment Parameters you can define fields of your macro module. These fields may map to existing fields of the internal network (internalName = \u0026hellip; ), but they do not need to and can also be completely new. You can reference these fields when creating a panel, to allow interactions with these fields. All fields appear in the Automatic Panel.\nModule panel layout To create your own User Interface, we need to create a Window . A window is one of the layout elements which exist in MDL. These layout elements are called controls . The curled brackets define the window environment, in which you can define properties of the window and insert further controls like a Box .\nInitially, we call the window MyWindowTitle, which can be used to reference this window.\nDouble-clicking on your module now opens your first self developed User Interface.\nFilter.script\nInterface { Inputs { Field input0 { internalName = Convolution.input0 } } Outputs { Field output0 { internalName = Arithmetic2.output0 } } Parameters { } } Window MyWindowName { title = MyWindowTitle Box MyBox { } } Module Panel You can define different properties of your control. For a window, you can for example define a title, or whether the window should be shown in full screen (fullscreen = True).\nThese properties are called tags and are individually different for each control. Which tags exist for the control window can be found here . The control box has different tags. You can for example define a title for the box, but you can not define whether to present the box in full screen.\nIf you like to add more than one control to your window, for example one box and one label, you can specify their design like in the following examples:\nFilter.script\nWindow MyWindowName { title = MyWindowTitle w = 100 h = 50 Vertical { Box MyBox { title = \u0026#34;Title of my Box\u0026#34; } Label MyLabel { title = \u0026#34;This is a label below the box\u0026#34; } } } Vertical layout of Box and Text Filter.script\nWindow MyWindowName { title = MyWindowTitle w = 100 h = 50 Horizontal { Box MyBox { title = \u0026#34;Title of my Box\u0026#34; } Label MyLabel { title = \u0026#34;This is a label below the box\u0026#34; } } } Horizontal layout of Box and Text There are much more controls, which can be used. For example a CheckBox, a Table, a Grid, a Button, ... . To find out more, take a look into the MDL Reference .\nModule interactions Until now, we learned how to create the layout of a panel. As a next step, we like to get an overview over interactions.\nExtra Infos:\u0026nbsp; You can add the module GUIExample to your workspace and play around with is. Access to existing fields of the internal network To interact with fields of the internal network in your User Interface, we need to access these fields. To access the field of the internal module Convolution, which defines the kernel, we need to use the internal network name. To find the internal field name, open the internal network of the macro module Filter (click on the module using the middle mouse button ).\nThen, open the panel of the module Convolution and right-click the field title Use of the box Predefined Kernel and select Copy Name. You now copied the internal network name of the field to your clipboard. The name is made up of ModuleName.FieldName, in this case Convolution.predefKernel.\nConvolution Module In the panel of the module Convolution, you can change this variable Kernel via a drop-down menu. In MDL a drop-down menu is called a ComboBox . We can take over the field predefKernel, its drop-down menu and all its properties by creating a new field in our panel and reference to the internal field Convolution.predefKernel, which already exist in the internal network.\nChanges of the properties of this field can be done in the curled brackets using tags (here, we changed the title).\nFilter.script\nWindow MyWindowName { title = MyWindowTitle Field Convolution.predefKernel { title = Kernel } } Selecting the kernel As an alternative, you can define the field kernel in the Parameters environment, and reference the defined field by its name. The result in the panel is the same. You can see a difference in the Automatic Panel. All fields, which are defined in the interface in the Parameters environment appear in the Automatic Panel. Fields of the internal network, which are used but not declared in the section Parameters of the module interface do not appear in the Automatic Panel.\nFilter.script\nInterface { Inputs { Field input0 { internalName = Convolution.input0 } } Outputs { Field output0 { internalName = Arithmetic2.output0 } } Parameters { Field kernel { internalName = Convolution.predefKernel title = Kernel: } } } Window MyWindowName { title = MyWindowTitle Field kernel {} } Commands We can not only use existing functionalities, but also add new interactions via Python scripting.\nIn below example we added a wakeupCommand to the Window and a simple command to the Button.\nFilter.script\nWindow MyWindowName { title = MyWindowTitle wakeupCommand = myWindowCommand Button MyButton { command = myButtonAction } } The wakeupCommand defines a Python function which is executed as soon as the Window is opened. The Button command is executed when the user clicks on the Button.\nBoth commands reference a Python function which is executed whenever both actions (open the Window or click the Button) are executed.\nIf you like to learn more about Python scripting, take a look at Example 2.5.\nWe need to define the Python script, which contains our Python functions. In order to do this, add a Command section outside your window and define the tag source.\nExample: Filter.script\nCommands { source = $(LOCAL)/Filter.py } Infos:\u0026nbsp; The section Source should already be available and generated automatically in case you enable the Wizard to add a Python file to your module. You can right-click on the command (myWindowCommand or myButtonAction) in your *.script file and select [ Create Python Funtion...... ]. The text editor MATE opens automatically and generates an initial Python function for you. You can simply add a logging function or implement complex logic here.\nExample: Filter.py\ndef myWindowCommand: MLAB.log(\u0026#34;Window opened\u0026#34;) def myButtonAction: MLAB.log(\u0026#34;Button clicked\u0026#34;) Available examples MeVisLab provides a lot of example modules for GUI development. All of these examples provides the *.script file for UI development and the *.py file containing the Python script.\nLayouting examples TestVerticalLayout Module TestHorizontalLayout Module TestTableLayout Module TestGridLayout Module TestSplitterLayout Module TestBoxLayout Module TestTabViewLayout Module Other examples TestHyperText Module TestListBox Module TestListView Module TestIconView Module TestPopupMenu Module TestViewers Module TestEventFilter Module TestStyles Module TestButtonGroups Module TestImageMap Module Please use the Module Search with the prefix Test for more examples.\nSummary User interfaces and several module panels can be created for each macro module. You can create a panel, define inputs and outputs as well as interactions, in your *.script file in MATE by using the MeVisLab Definition Language (MDL) . Module interactions can be implemented using commands, which are linked to Python functions. You can implement field listeners, which trigger actions after a field value changes. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro","User Interface","GUI"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/pythonscripting/","title":"Example 2.5: Interactions via Python scripting","summary":"Example 2.5: Module Interactions Using Python Scripting Introduction This chapter will give you an overview over Python scripting in MeVisLab. Here, no introduction into Python will be given. However, basic knowledge in Python is helpful. Instead, we will show how to integrate and use Python in the MeVisLab SDK.\nIn fact, nearly everything in MeVisLab can be done via Python scripting: You can add modules to your network, or remove modules, you can dynamically establish and remove connections and so on.","content":"Example 2.5: Module Interactions Using Python Scripting Introduction This chapter will give you an overview over Python scripting in MeVisLab. Here, no introduction into Python will be given. However, basic knowledge in Python is helpful. Instead, we will show how to integrate and use Python in the MeVisLab SDK.\nIn fact, nearly everything in MeVisLab can be done via Python scripting: You can add modules to your network, or remove modules, you can dynamically establish and remove connections and so on. But, much more important: You can access module inputs and outputs, as well as module fields to process their parameters and data. You can equip user interfaces and panel with custom functionalities. Python can be used to implement module interactions. When you open a panel or you press a button in a panel, the executed actions are implemented via Python scripting.\nBasics To see how to access modules, fields, and so on, open the Scripting Console Via [ Scripting \u0026rarr; Show Scripting Console ].\nInternal field names You can find the internal name of one module field in the respective network. Open a panel, for example the Automatic Panel and right-click the field\u0026rsquo;s title to open the field\u0026rsquo;s context menu. Now, you can select Copy Name, to copy the internal name of the field. This name can be used to access the field via scripting.\nScripting context When entering ctx to the console, you can see the context you are working with. In the context of the Scripting Console, you have access to your workspace, meaning the whole network, its modules and the module fields.\nScripting context Editing the workspace In the Scripting Console, you can add and connect modules using the following commands:\nctx.addModule(\u0026quot;\u0026lt; ModuleName \u0026gt;\u0026quot;) : Add the desired module to your workspace. ctx.field(\u0026quot; \u0026lt; ModuleName.FieldName\u0026gt; \u0026quot;) : Access a field of a module. ctx.field(\u0026quot; \u0026lt; ModuleInput \u0026gt; \u0026quot;).connectFrom(\u0026quot; \u0026lt; ModuleOutput \u0026gt; \u0026quot;) : Draw a connection from one module\u0026rsquo;s output to another module\u0026rsquo;s input. In this case we added the modules DicomImport and View2D to the workspace and connected both modules.\nAdd and connect modules via scripting It is also possible to add notes to your workspace.\nAdd a note to your workspace Access modules and module fields You can access modules via ctx.module(\u0026quot; \u0026lt; ModuleName \u0026gt; \u0026quot;). From this object, you can access module fields, module inputs and outputs and everything in context of this module.\nYou can also directly access a module field via ctx.field(\u0026quot; \u0026lt; ModuleName.FieldName \u0026gt; \u0026quot;). Different methods can be called on this object. Take a look at the Scripting Reference to find out which methods can be called for which object or class. You can for example access the value of the respective field.\nAccess modules and module fields Python Scripting Reference Here , you can find the Scripting Reference. In the Scripting Reference you can find information about different Python classes used in MeVisLab and their methods.\nWhere and how to use Python scripting Scripting View Under [ View \u0026rarr; Views \u0026rarr; Scripting ] you can find the View Scripting. The view offers a standard Python console, without any meaningful network or module context. This means only general Python functionalities can be tested and used. Access to modules or your network is not possible.\nScripting Console You can open the Scripting Console via [ Scripting \u0026rarr; Show Scripting Console ]. In the context of your workspace, you can access your network and modules.\nScripting console of modules Every module offers a scripting console. Open the context menu of a module and select [ Show Window \u0026rarr; Scripting Console ]. You can work in the context (ctx.) of this module.\nModule RunPythonScript The module RunPythonScript allows to execute Python scripts from within a MeVisLab network. You can draw parameter connection from modules to RunPythonScript and back, to process parameter fields using Python scripting. An example for the usage of RunPythonScript can be found here.\nModule interactions via Python scripting You can reference to a Python function inside a *.script file of a macro module. With this, you can for example execute a Python function, whenever you open a panel, define the action which is executed when pressing a button or specify the command triggered by a field listener. An example for module interactions via Python scripting is given in the same example.\nTips and tricks Scripting Assistant Under [ View \u0026rarr; Views \u0026rarr; Scripting Assistant ] you can find the view Scripting Assistant. In this view, the actions you execute in the workspace are translated into Python script.\nFor example: Open the Scripting Assistant. Add the module WEMInitialize to your workspace. You can select a Model, for example the cube. In addition, you can change the Translation and press Apply. All these actions can be seen in the Scripting Assistant, translated into Python code. Therefore, the Scripting Assistant is a powerful tool to help you to script you actions.\nScripting Assistant Examples See the following examples for Python Scripting:\nThe module RunPythonScript Module interactions via Python scripting Summary Python can be used to access, create and process networks, modules, fields and panels. You can use Python via different scripting consoles. You can also define custom module interactions by referencing to Python functions from the *.script file ","tags":["Beginner","Tutorial","Macro","Macro modules","Global Macro","Python","Scripting"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/scriptingexample1/","title":"Example 2.5.1: The module RunPythonScript","summary":"Example 2.5.1: The module RunPythonScript \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The module RunPythonScript allows to execute Python scripts from within a MeVisLab network. You can draw parameter connection from modules to RunPythonScript and back, to process parameter fields using Python scripting.\nSteps to do Develop your network In this example, we like to dynamically change the color of a cube in an Open Inventor scene. For that, add and connect the following modules as shown.","content":"Example 2.5.1: The module RunPythonScript \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The module RunPythonScript allows to execute Python scripts from within a MeVisLab network. You can draw parameter connection from modules to RunPythonScript and back, to process parameter fields using Python scripting.\nSteps to do Develop your network In this example, we like to dynamically change the color of a cube in an Open Inventor scene. For that, add and connect the following modules as shown.\nRunPythonScript Scripting using the moduule RunPythonScript Open the panel of RunPythonScript. There is an option to display input and output fields. For that, tick the box Fields on the top left side of the panel.\nYou can also name these fields individually, by ticking the box Edit field titles. Call the first input field TimeCounter and draw a parameter connection from the field Value of the panel of TimeCounter to the input field TimeCounter of the module RunPythonScript. We can name the first output field DiffuseColor and draw a parameter connection from this field to the field Diffuse Color in the panel of the module SoMaterial.\nTimeCounter The module TimeCounter counts in a defined Frequency. We like to randomly change the color of the cube in the frequency the TimeCounter counts. Add this code:\nIsoCSOs.py\nimport random red = TimeCounter * random.randrange(0,52)/255 green = TimeCounter * random.randrange(0,52)/255 blue = TimeCounter * random.randrange(0,52)/255 updateOutputValue(\u0026#34;DiffuseColor\u0026#34;, str(red) + \u0026#34; \u0026#34; + str(green) + \u0026#34; \u0026#34; + str(blue)) To update the output field DiffuseColor, it is important to use the methods updateOutputValue(name, value) or setOutputValue(name, value) instead of simply assigning a value to the output field.\nYou can now see a color change in the viewer SoExaminerViewer every time the TimeCounter counts.\nTriggered color change Summary The module RunPythonScript can be used to process module fields in your network using Python scripting. Use the methods updateOutputValue(name, value) or setOutputValue(name, value) to update output fields of RunPythonScript. ","tags":["Beginner","Tutorial","Python","Scripting","RunPythonScript"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/scriptingexample2/","title":"Example 2.5.2: Module interactions via Python scripting","summary":"Example 2.5.2: Module interactions via Python scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to add Python scripting to your User Interface. The network used in Chapter V will be used for creating the macro module.\nSteps to do Creating the macro module First, we condense the example network into a macro module and then we create a panel for that module. To create a macro module use the Project Wizard, which you find under [ File \u0026rarr; Run Project Wizard ].","content":"Example 2.5.2: Module interactions via Python scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to add Python scripting to your User Interface. The network used in Chapter V will be used for creating the macro module.\nSteps to do Creating the macro module First, we condense the example network into a macro module and then we create a panel for that module. To create a macro module use the Project Wizard, which you find under [ File \u0026rarr; Run Project Wizard ]. Select Macro module and press Run.\nNow, you have to edit:\nName: The name of your module Package: Select the package you like to save the macro module in. Directory Structure: Change to Self-contained Project: Select you project name Press Next and edit the following:\nCopy existing network: Select the example network Check the box: Add Python file Now, create your macro module and reload MeVisLab. You can find your module via search in MeVisLab.\nCreating macro module Enable Python scripting To design a panel and create a user interface for the macro module, open the *.script file. You can see, that a Command environment exist, which defines the python file as source for all commands.\nOpen the script file Script file Creating a panel with tabs and viewers At first, we create a Window with two Tabs . One Main tab, in which both viewers of the network are represented and one tab for Settings. For generating tabs, we can use the control TabView , with its items TabViewItem . The control TabView enables to add a command, which is executed when opening the tab. For adding the viewers to the panel, we use the Control Viewer.\nIsoCSOs.script\nWindow { TabView { TabViewItem Main { Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 } Viewer SoExaminerViewer.self { type = SoExaminerViewer pw = 400 ph = 400 } } } TabViewItem Settings { } } } Panel with Tabs and Viewers Edit viewer settings in the panel You may want to change the design setting of the right viewer. This is still possible via the internal network of the macro module. Open the internal network either via the context menu or using the middle mouse button and click on the module. After that, open the Automatic Panel of the module SoExaminerViewer via context menu [ Show Windows \u0026rarr; Automatic Panel ] and change the field decoration to False. Keep in mind, as we did not create CSOs by now, the right viewer stays black.\nChange viewer settings Changed viewer settings Selection of images Next, we like to add the option to browse through the folders and select the image, we like to create CSOs from. This functionality is already given in the internal network in the module LocalImage. We can copy this functionality from LocalImage and add this option to the panel above both viewers. But, how should we know, which field name we reference to? To find this out, open the internal network of your macro module. Now you are able to open the panel of the module LocalImage. Right-click the desired field: In this case, right-click the label Name:. Select Copy Name, to copy the internal name of this field.\nCopy the field name Now, you can add this field as a new field to your window by pasting the name. All field settings are taken over from the internal field from the module LocalImage.\nIsoCSOs.script\nWindow { TabView { TabViewItem Main { Vertical { Field LocalImage.name {} Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 } Viewer SoExaminerViewer.self { type = SoExaminerViewer pw = 400 ph = 400 } } } } TabViewItem Settings { } } } Add name field Add buttons to your panel As a next step, we like to add a Browse...-Button, like in the module LocalImage, and also a button to create the CSOs.\nTo create the Browse...-Button:\nCreate a button containing the command fileDialog. Right-click the command to create the respective function in the Python file. Edit the function in the Python file, to enable the file dialog (similar function as in LocalImage.py). To create the Iso Generator Button:\nWe like to copy the field of the Update-Button from the internal module IsoCSOGenerator, but not its layout so:\nCreate a new Field in the interface, called IsoGenerator, which contains the internal field Update from the module IsoCSOGenerator. Create a new Button in your Window which uses the field IsoGenerator. After these steps, you can use the Iso Generator button to create CSOs.\nIsoCSOs.script\nInterface { Inputs {} Outputs {} Parameters { Field IsoGenerator { internalName = CSOIsoGenerator.apply } } } Commands { source = $(LOCAL)/IsoCSOs.py } Window { TabView { TabViewItem Main { Vertical { Horizontal { Field LocalImage.name {} Button { title = \u0026#34;Browse...\u0026#34; command = fileDialog } Button IsoGenerator { title = \u0026#34;Iso Generator\u0026#34; } } Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 } Viewer SoExaminerViewer.self { type = SoExaminerViewer pw = 400 ph = 400 } } } } TabViewItem Settings { } } } IsoCSOs.py\nfrom mevis import * def fileDialog(): exp = ctx.expandFilename(ctx.field(\u0026#34;LocalImage.name\u0026#34;).stringValue()) filename = MLABFileDialog.getOpenFileName(exp, \u0026#34;\u0026#34;, \u0026#34;Open file\u0026#34;) if filename: ctx.field(\u0026#34;LocalImage.name\u0026#34;).value = ctx.unexpandFilename(filename) Automatically generate CSOs based on Iso value Colorizing CSOs We like to colorize the CSO we hover over with our mouse in the 2D viewer. Additionally, when clicking a CSO with the left mouse key , this CSO shall be colorized in the 3D viewer. This functionality can be implemented via Python scripting (even though MeVisLab has a build-in function to do that). We can do this in the following way:\nEnable the View Scripting Assistant, which translates actions into Python code.\nScripting Assistant Enable a functionality which allows us to notice the id of the CSO we are currently hovering over with our mouse. For this open the internal network of our macro module. We will use the module SoView2DCSOExtensibleEditor. Open its panel and select the tab Advanced. You can check a box to enable Update CSO id under mouse. If you now hover over a CSO, you can see its id in the panel. We can save the internal network to save this functionality, but we can also solve our problem via scripting. The Scripting Assistant translated our action into code, which we can use.\nEnabling CSO id identification We like to activate this functionality when opening the panel of our macro module IsoCSOs. Thus, we add a starting command to the control Window. We can call this command for example enableFunctionalities.\nIn the *.script file:\nIsoCSOs.script\nWindow { windowActivatedCommand = enableFunctionalities TabView { TabViewItem Main { ... } } } In the Python file, we define the function enableFunctionalities. We see our action as Python code in the Scripting Assistant. Just copy the code into our Python function.\nIsoCSOs.py\ndef enableFunctionalities(): ctx.field(\u0026#34;SoView2DCSOExtensibleEditor.updateCSOIdUnderMouseCursor\u0026#34;).value = True Implement a field listener. This field listener will detect when you hover over a CSO and the CSO id changes. Triggered by a CSO id change, a colorization function will be executed, which will colorize the selected CSO. In the *.script file:\nIsoCSOs.script\nCommands { source = $(LOCAL)/IsoCSOs.py FieldListener SoView2DCSOExtensibleEditor.csoIdUnderMouseCursor { command = colorizeCSO } } In the Python file:\nIsoCSOs.py\n# global variables listCSOs = [] idxCSO = -1 def colorizeCSO(): if ctx.field(\u0026#34;CSOManager.numCSOs\u0026#34;) == 0: pass else: global listCSOs global idxCSO if listCSOs == []: listCSOs = ctx.field(\u0026#34;CSOManager.outCSOList\u0026#34;).object() # COLORIZATION OF CSO # Changing back color of previously selected CSO to default value if idxCSO \u0026gt;= 0: oldCSO = listCSOs.getCSOAt(idxCSO) oldCSO.setPathPointColor((1.0, 1.0, 0.0)) # Color change of CSO oldCSO.setPathPointWidth(1) # Line width change # Changing color and width of selected CSO idxCSO = ctx.field(\u0026#34;SoView2DCSOExtensibleEditor.csoIdUnderMouseCursor\u0026#34;).value - 1 # -1 because CSOs are indexed starting at 1 if idxCSO \u0026gt;= 0: currentCSO = listCSOs.getCSOAt(idxCSO) currentCSO.setPathPointColor((1.0, 0.0, 1.0)) currentCSO.setPathPointWidth(5) Reload your module ( F5 ) and open the panel. After generating CSOs, the CSO under your mouse is marked. Clicking this CSO enables the marking in the 3D viewer. If you like, you can add some settings to your Settings page. For example\nIsoCSOs.script\nTabViewItem Settings { Field CSOIsoGenerator.isoValue {} Field SoCSOVisualizationSettings.ghostingDepthInVoxel {} } Colored selection Summary The control Tabview creates tabs in panels. The control Viewer allows to add viewers to your panel. The control Button creates a button executing a Python function when pressed. The tag WindowActivationCommand of the control Window triggers Python functions executed when opening the panel. Field listeners can be used to activate Python functions triggered by a change of defined parameter fields. Use the view Scripting Assistant can be used to translate actions into Python code. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Macro","Macro modules","Global Macro","Python","Scripting"],"section":"tutorials"},{"date":"1655276324","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/viewerexample/","title":"Example 3: Creating a simple application","summary":"Example 3: Creating a simple application Introduction In the previous examples, you already learned how to create macro modules, user interfaces and how to interact with your UI via Python scripting.\nIn this example, you will learn how to create a simple Prototype application in MeVisLab including a User Interface with 2D and 3D viewer. You will learn how to implement field listeners and react on events.\nSteps to do Create your network Start with an empty network and add the Module ImageLoad to your workspace.","content":"Example 3: Creating a simple application Introduction In the previous examples, you already learned how to create macro modules, user interfaces and how to interact with your UI via Python scripting.\nIn this example, you will learn how to create a simple Prototype application in MeVisLab including a User Interface with 2D and 3D viewer. You will learn how to implement field listeners and react on events.\nSteps to do Create your network Start with an empty network and add the Module ImageLoad to your workspace. Then add a View2D and View3D to your workspace and connect the modules as seen below.\nLoading and viewing images Load an image Now double-click on the ImageLoad module and open any image. You can use the included file ./MeVisLab/Resources/DemoData/MRI_Head.dcm.\nOpening your viewers should now show the images in 2D and 3D.\nShow images in 2D and 3D Save your network Now, save your network as *.mlab file and remember the location.\nCreate a macro module Open the Project Wizard via [ File \u0026rarr; Run Project Wizard ] and run Wizard for a macro module. Name your module MyViewerApplication, enter your details and click Next \u0026gt;.\nModule Properties On the next screen, make sure to add a Python file and use the existing network you previously saved. Click Next \u0026gt;.\nMacro module Properties You can leave all fields empty for now and just click Create.\nModule Field Interface MeVisLab reloads its internal database and you can open a new Tab. Search for your newly created module, in our case it was MyViewerApplication.\nMyViewerApplication In case you double-click your module now, you will see the Automatic Panel only showing the name of your module, because we did not add any own Window until now.\nDevelop your User Interface Before adding your own UI, open internal network of your macro module via right-click and [ Show Internal Network ]. Open the panel of your ImageLoad module and set filename to an empty string (clear). This is necessary for later.\nNow, right-click on your MyViewerApplication and select [ Related Files \u0026rarr; MyViewerApplication.script ]\nMATE opens showing your script file. You already learned how to create simple UI elements in Example 2.4. Now we will create a little more complex UI including your View2D and View3D.\nFirst we need a new Field in your Parameters section. Name the field filepath and set internalName to ImageLoad.filename.\nMyViewerApplication.script\nInterface { Inputs {} Outputs {} Parameters { Field filepath { internalName = ImageLoad.filename } } } We now re-use the filepath field from the ImageLoad module for our interface. Add a Window and a Vertical to the bottom of your *.script file. Add the just created parameter field filepath inside your Vertical as seen below.\nMyViewerApplication.script\nInterface { Inputs {} Outputs {} Parameters { Field filepath { internalName = ImageLoad.filename } } } Commands { source = $(LOCAL)/MyViewerApplication.py } Window { Vertical { Field filepath {} } } If you now double-click on your module, you can see your just created filepath field.\nFilepath field in UI Next, we will add your 2D and 3D Viewers and a Button to your Window. Change your *.script file as seen below:\nMyViewerApplication.script\nWindow { Vertical { Horizontal { Field filepath {} Button { title = \u0026#34;Reset\u0026#34; } } Horizontal { Viewer View2D.self { type = SoRenderArea pw = 400 ph = 400 expandX = yes expandY = yes } Viewer View3D.self { pw = 400 ph = 400 expandX = yes expandY = yes } } } } We have a vertical Layout having 2 items placed horizontally next to each other. The new Button gets the title Reset but does nothing, yet because we did not add a Python function to a command.\nAdditionally we added the View2D and the View3D to our Window and defined the height, width and the expandX/Y property to yes. This leads our viewers to resize together with our Window.\nExtra Infos:\u0026nbsp; Additional information about the View2D and View3D options can be found in the MeVisLab MDL Reference You can now play around with your module in MeVisLab SDK. Open the Window and select a file. You can see the 2 viewers showing the 2D and 3D images. You can interact with your viewers the same way as in your MeVisLab network. All functionalities are taken from the modules and transferred to your user interface.\n2D and 3D viewers in our application Develop a python function for your Button Next we want to reset the filepath to an empty string on clicking our Reset button. Add the reset command to your Button. MyViewerApplication.script\n... Button { title = \u0026#34;Reset\u0026#34; command = reset } ... Right-click on reset and select [ Create Python function \u0026#39;reset\u0026#39; ]. MATE opens the Python file of your module and automatically adds the function definition. Set the filename of the ImageLoad module to an empty string.\nMyViewerApplication.py\nfrom mevis import * def reset(): ctx.field(\u0026#34;filepath\u0026#34;).value = \u0026#34;\u0026#34; Clicking on Reset in your module now clears the filename field and the viewers do not show any images anymore.\nField listeners A field listener watches a given field in your network and reacts on any changes of the field value. You can define Python functions to execute in case a change has been detected.\nIn order to define such a listener, you need to add it to the Commands section in your *.script file.\nExample: MyViewerApplication.script\nCommands { source = $(LOCAL)/MyViewerApplication.py FieldListener View2D.startSlice { command = printCurrentSliceNumber } } In the above example, we react on changes of the field startSlice of the module View2D. Whenever the field value (currently displayed slice) changes, the Python function printCurrentSliceNumber is executed.\nIn your Python file Filter.py you can now add the following:\nFilter.py\ndef printCurrentSliceNumber(field): MLAB.log(field.value) Scrolling through slices in the View2D module now logs a message containing the slice number currently visible to the MeVisLab Debug Output.\nSummary You can add any Viewers to your application UI by reusing them in MDL. Parameter Fields using the internalName of an existing field in your network allows re-using this UI element in your own UI. Changes in your UI are applied to the field in the module. Field Listeners allow reacting on changes of a field value in Python. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Macro","Macro modules","Global Macro","Python","Scripting"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/tutorials/basicmechanisms/macromodules/pythonpip/","title":"Example 4: Installing additional Python packages using the PythonPip module","summary":"Example 4: Installing additional Python packages using the PythonPip module Introduction MeVisLab already comes with a lot of integrated third party software tools ready to use. Nevertheless it might be necessary to install additional Python packages for your specific needs. This example will walk you through the process of adding packages through usage of/using the PythonPip module.\nThe PythonPip module allows to work with the Python package manager pip. It can be used to install Python packages into the site-packages of the MeVisLab Python installation.","content":"Example 4: Installing additional Python packages using the PythonPip module Introduction MeVisLab already comes with a lot of integrated third party software tools ready to use. Nevertheless it might be necessary to install additional Python packages for your specific needs. This example will walk you through the process of adding packages through usage of/using the PythonPip module.\nThe PythonPip module allows to work with the Python package manager pip. It can be used to install Python packages into the site-packages of the MeVisLab Python installation.\nIt technically provides the full Python package ecosystem, though you will have to keep some things in mind to avoid your newly added packages to interfere with the existing ones that MeVisLab operates on:\nPackages can contain C-Extensions (since we use the same MSVC compiler resp. same GCC settings as Python 3 itself), but you can only install packages that do not interfere with packages or DLLs that are already part of MeVisLab. This means that installing packages with C-Extensions might work in many circumstances, but is not guaranteed to work All installed packages with C-Extensions are release only, so you can only import them in a release MeVisLab (under Windows) Attention:\u0026nbsp; Under Windows: Existing packages (e. g./ like NumPy) can only be upgraded if they haven\u0026rsquo;t already been loaded by MeVisLab\u0026rsquo;s Python. So please make sure to start with a fresh MeVisLab Packages that you should not upgrade or install (because they have been adapted for MeVisLab):\nvtk cv2 (OpenCV) PySide2 / PyQt (we have our own PythonQt binding and our own Qt DLLs) matplotlib These are some of the most important packages that have been adapted for MeVisLab. If you seem to have a problem upgrading another one that is not listed here, make sure to ask in the MeVisLab forum or directly contact our developers via EMail.\nWorking with the PythonPip module on your MeVisLab workspace The module PythonPip can be found via module search. It provides a user interface showing the currently installed Python packages including version and MeVisLab package it has been installed to.\nPythonPip interface Select the package to install the Python package into, write the name of the package and click install.\nIn case you want to install a specific version, you can also use ==1.2.0\nAttention:\u0026nbsp; We strongly recommend to install the packages into a MeVisLab user package. This has many advantages:\nUser packages can be updated without administrator privileges User packages remain available after uninstalling and/or updating MeVisLab It is possible to install multiple versions of the Python packages into different user packages. Possible conflicts will be shown in the PythonPip module panel. The only disadvantage: Python commands will not be recognized outside of MeVisLab by default.\nThirdparty information and *.mli files are updated automatically.\nUsing the commandline Another option is using the commandline tool provided by MeVisLab. Under Windows, you need to change to directory Packages\\MeVis\\ThirdParty\\Python first.\ncommandline\nMeVisPython -m pip ... Attention:\u0026nbsp; The commandline option does not provide the possibility to install into a specified user package. Thirdparty information and *.mli files are not adapted automatically with the commandline tool. In Example 1: Installing PyTorch using the PythonPip module we are installing PyTorch to use it in MeVisLab scripting.\nSummary The PythonPip module allows to install additional Python packages to adapt MeVisLab to a certain extent ","tags":["Advanced","Tutorial","Python","PythonPip","pip"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/openinventor/","title":"Chapter II: Open Inventor","summary":"Open Inventor modules Introduction In total, there are three types of modules:\nblue ML modules brown macro modules green Open Inventor modules The names of Open Inventor modules start with the prefix So\\* (for Scene Objects). Open Inventor modules process and render 3D scene objects and enable image interactions. Scene objects are transmitted using the semi-circle shaped input and output connectors. With the help of these modules, Open Inventor scenes can be implemented.","content":"Open Inventor modules Introduction In total, there are three types of modules:\nblue ML modules brown macro modules green Open Inventor modules The names of Open Inventor modules start with the prefix So\\* (for Scene Objects). Open Inventor modules process and render 3D scene objects and enable image interactions. Scene objects are transmitted using the semi-circle shaped input and output connectors. With the help of these modules, Open Inventor scenes can be implemented.\nAn exemplary Open Inventor scene will be implemented in the following paragraph.\nOpen Inventor Scenes and Execution of Scene Graphs Inventor scenes are organized in structures called scene graphs. A scene graph is made up of nodes, which represent 3D objects to be drawn, properties of the 3D objects, nodes that combine other nodes and are used for hierarchical grouping, and others (cameras, lights, etc.). These nodes are accordingly called shape nodes, property nodes, group nodes and so on. Each node contains one or more pieces of information stored in fields. For example, the Sphere node contains only its radius, stored in its radius field. Open Inventor modules function as Inventor nodes, so they may have input connectors to add Inventor child nodes (modules) and output connectors to link themselves to Inventor parent nodes (modules).\nExecution order in Open Inventor scenes:\u0026nbsp; The model below depicts the order in which the modules are executed. The red arrow indicates the traversal order, from top to bottom and from left to right. The modules are numbered accordingly, from 1 to 8. Knowing about the traversal order can be crucial to achieve a certain ouput. Traversing through a network of Open Inventor modules SoGroup and SoSeparator The SoGroup and SoSeparator modules can be used as containers for child nodes. They both allow multiple inputs and combine the results in one single output as seen above. Nevertheless, there is a big difference in handling the traversal state of the scene graph.\nSoGroup vs. SoSeparator In the network above, we render four SoCone objects. The left side uses the SoSeparator modules, the right side uses the SoGroup ones. There is a SoMaterial module defining one of the left cone objects to be yellow. As you can see, the SoMaterial module is only applied to that cone, the other left cone remains in its default grey color, because the SoSeparator module isolates the separator\u0026rsquo;s children from the rest of the scene graph.\nOn the right side, we are using SoGroup ( SoGroup module reference ). The material of the cone is set to red. As the SoGroup module does not alter the traversal state in any way, the second cone in this group is also red.\nCheck:\u0026nbsp; Be aware of some Open Inventor modules altering the traversal order. If your scene turns out to differ from your expected result, check whether incorporated SoSeparator modules are the cause. Details on these can be found in the SoSeparator module reference \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. More Information about Open Inventor and Scene Graphs can be found here , in the Open Inventor Overview or the Open Inventor Reference.\n","tags":["Beginner","Tutorial","Open Inventor","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/openinventor/openinventorobjects/","title":"Example 1: Open Inventor Objects","summary":"Example 1: Open Inventor Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to construct an Open Inventor scene in which we display three 3D objects of different color and shape.\nSteps to do Generating Open Inventor Objects First, add the modules SoExaminerViewer and SoCone to the workspace and connect both modules as shown. The module SoCone creates a cone shaped object, which can be displayed in the Viewer SoExaminerViewer.","content":"Example 1: Open Inventor Objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to construct an Open Inventor scene in which we display three 3D objects of different color and shape.\nSteps to do Generating Open Inventor Objects First, add the modules SoExaminerViewer and SoCone to the workspace and connect both modules as shown. The module SoCone creates a cone shaped object, which can be displayed in the Viewer SoExaminerViewer.\nSoExaminerViewer We like to change the color of the cone. In order to do so, add the module SoMaterial to the workspace and connect the module as shown below. When creating an Open Inventor scene (by creating networks of Open Inventor modules), the sequence of module connections, in this case the sequence of the inputs to the module SoExaminerViewer determines the functionality of the network.\nOpen Inventor modules are executed like scene graphs. This means, modules are executed from top to bottom and from left to right. Here, it is important to connect the module SoMaterial to an input on the left side of the connection between SoCone and SoExaminerViewer. With this, we first select features like a color and these features are then assigned to all objects, which were executed afterwards. Now, open the panel of the module SoMaterial and select any Diffuse Color you like. Here, we choose green.\nColors and Material in Open Inventor We like to add a second object to the scene.\nIn order to do that, add the module SoSphere to the workspace. Connect this module to SoExaminerViewer. When connecting SoSphere to an input on the right side of the connection between the viewer and the module SoMaterial, the sphere is also colored in green. One problem now is, that currently both objects are displayed at the same position.\nAdding a SoSphere They display both objects at different positions, add the modules SoSeparator and SoTransform to the scene and connect both modules shown on the following picture. Open the panel of SoTransform and implement a translation in x-direction to shift the object. Now you can examine two things:\nThe sphere loses its green color The cone is shifted to the side Transformation The module SoTransform is responsible for shifting objects, in this case the cone, to the side. The module SoSeparator ensures that only the cone is shifted and also only the cone is colored in green. It separates this features from the rest of the scene.\nWe like to add a third object, a cube, and shift it to the other side of the sphere. Add the modules SoCube and SoTransform to the workspace and connect both modules as shown below. To shift the cube to the other side of the sphere, open the panel of SoTransform and adjust the Translation in x direction. The sphere is not affected by the translation, as the connection from SoTransform1 to SoExaminerViewer is established on the right side of the connection between SoSphere and SoExaminerViewer.\nAdding a SoCube Again, we use the module SoMaterial to select a color for the cone and the sphere.\nMultiple Materials For easier handling we group an object together with its features by using the module SoGroup. This does not separate features, which is the reason for the cube to be colorized. All modules that are derived from SoGroup offer a basically infinite number of input connectors (a new connector is added for every new connection).\nSoGroup If we do not want to colorize the cube, we have to exchange the module SoGroup by another SoSeparator module.\nSoSeparator The implementation of all objects can be grouped together.\nGrouping In addition to the objects, a background can be added to the scene using the module SoBackground.\nSoBackground Summary Scene objects are represented by nodes. Size and position is defined by transformation nodes. A rendering node represents the root of the scene graph. Nodes are rendered in the order of traversal. Nodes on the same level are traversed from left to right. All modules that are derived from SoGroup offer a basically infinite number of input connectors (a new connector is added for every new connection). \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Open Inventor","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/openinventor/mouseinteractions/","title":"Example 2: Mouse interactions in Open Inventor","summary":"Example 2: Mouse interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we implement some image or object interactions. We will create a 3D scene, in which we display a cube and change its size using the mouse. We also get to know another viewer, the module SoExaminerViewer. This viewer is important. It enables the rendering of Open Inventor scenes and allows interactions with the Open Inventor scenes.","content":"Example 2: Mouse interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we implement some image or object interactions. We will create a 3D scene, in which we display a cube and change its size using the mouse. We also get to know another viewer, the module SoExaminerViewer. This viewer is important. It enables the rendering of Open Inventor scenes and allows interactions with the Open Inventor scenes.\nSteps to do Develop your network For implementing the example, build the following network. We already know the module SoCube, which builds a 3D scene object forming a cube. In addition to that, add the module SoMouseGrabber. Connect the modules as shown below.\nExtra Infos:\u0026nbsp; Additional information about the SoMouseGrabber can be found here: SoMouseGrabber SoMouseGrabber Configure mouse interactions Now, open the panels of the module SoMouseGrabber and the module SoExaminerViewer, which displays a cube. In the Viewer, press the right key of your mouse and move the mouse around. This action can be seen in the panel of the module SoMouseGrabber.\nAttention:\u0026nbsp; Make sure to configure SoMouseGrabber fields as seen below. SoMouseGrabber You can see:\nButton 3, the right mouse button , is tagged as being pressed Changes of the mouse coordinates are displayed in the box Output. Mouse Interactions Resize cube via mouse interactions We like to use the detected mouse-movements to change the size of our cube. In order to that, open the panel of SoCube. Build parameter connections from the mouse coordinates to the width and depth of the cube.\nChange Cube size by Mouse Events If you now press the right mouse key inside the Viewer and move the mouse around, the size of the cube changes.\nExercises Change location of the cube via Mouse Interactions by using the Module SoTransform Add more objects to the scene and interact with them Summary The module SoExaminerViewer enables the rendering of Open Inventor scenes and allows interactions with the Open Inventor scenes. Mouse interactions can be applied to the objects in the scene. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Open Inventor","3D","Mouse Interactions"],"section":"tutorials"},{"date":"1679443200","url":"https://mevislab.github.io/examples/tutorials/openinventor/camerainteraction/","title":"Example 3: Camera Interactions in Open Inventor","summary":"Example 3: Camera Interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are learning the basic principles of camera interactions in Open Inventor. We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nThe SoRenderArea module The module SoRenderArea is a simple renderer for Open Inventor scenes. It offers functionality to record movies and to create snapshots, but does not include an own camera or light.","content":"Example 3: Camera Interactions in Open Inventor \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are learning the basic principles of camera interactions in Open Inventor. We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nThe SoRenderArea module The module SoRenderArea is a simple renderer for Open Inventor scenes. It offers functionality to record movies and to create snapshots, but does not include an own camera or light.\nAdd a SoBackground, a SoMaterial and a SoOrientationModel module to your workspace and connect them to a SoGroup. Add a SoRenderArea to the SoGroup and open the viewer.\nSoRenderArea without camera and lights You can not interact with your scene and the rendered content is very dark. Open the SoOrientationModel and change Model to Skeleton to see that a little better. You can also change the material by using the panel of the SoMaterial module.\nAdd a SoCameraInteraction module and connect it between the SoGroup and the SoRenderArea.\nSoRenderArea with SoCameraInteraction The SoCameraInteraction does not only allow you to change the camera position in your scene but also adds light. The module automatically adds a headlight you can switch off in the fields of the module.\nHeadlight_TRUE Headlight_FALSE The SoCameraInteraction can also be extended by a SoPerspectiveCamera or a SoOrthographicCamera. Add a SoSwitch to your SoGroup and connect a SoPerspectiveCamera and a SoOrthographicCamera.\nSoPerspectiveCamera and SoOrthographicCamera You can now switch between both cameras, but you can not interact with them in the viewer. Select the SoCameraInteraction and toggle detectCamera. Now the default camera of the SoCameraInteraction is replaced by the camera selected in the SoSwitch.\nWhenever you change the camera in the switch, you need to detect the new camera in the SoCameraInteraction.\nSoPerspectiveCamera SoOrthographicCamera A SoPerspectiveCamera camera defines a perspective projection from a viewpoint.\nThe viewing volume for a perspective camera is a truncated pyramid. By default, the camera is located at (0,0,1) and looks along the negative z-axis; the Position and Orientation fields can be used to change these values. The Height Angle field defines the total vertical angle of the viewing volume; this and the Aspect Ratio field determine the horizontal angle.\nA SoOrthographicCamera camera defines a parallel projection from a viewpoint.\nThis camera does not diminish objects with distance, as an SoPerspectiveCamera does. The viewing volume for an orthographic camera is a cuboid (a box).\nBy default, the camera is located at (0,0,1) and looks along the negative z-axis; the Position and Orientation fields can be used to change these values. The Height field defines the total height of the viewing volume; this and the Aspect Ratio field determine its width.\nAdd a SoCameraWidget and connect it to your SoGroup.\nSoCameraWidget This module shows a simple widget on an Inventor viewer that can be used to rotate, pan, or zoom the scene. You can configure the Main Interaction in the panel of the SoCameraWidget.\nYou can also add more than one widget to show multiple widgets in the same scene, see example network of the SoCameraWidget module.\nThe SoExaminerViewer module The SoExaminerViewer makes some things much easier, because a camera and a light are already integrated.\nAdd a SoExaminerViewer to your workspace and connect it to the SoBackground, the SoMaterial and the SoOrientationModel modules.\nSoExaminerViewer The difference to the SoRenderArea can be seen immediately. You can interact with your scene and a light is available initially.\nThe module also allows you to switch between perspective and orthographic camera by changing the field cameraType.\nSoExaminerViewer_Perspective SoExaminerViewer_Orthographic The module also provides UI elements to interact.\nSummary MeVisLab provides multiple options for adding a camera to a scene The SoExaminerViewer already has an integrated camera and light, the SoRenderArea requires additional modules. You can use perspective and orthographic cameras \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Open Inventor","3D","Camera","Perspective Camera","Orthographic Camera"],"section":"tutorials"},{"date":"1714726353","url":"https://mevislab.github.io/examples/tutorials/openinventor/posteffectsinopeninventor/","title":"Example 4: Post Effects in Open Inventor","summary":"Example 4: Post Effects in Open Inventor Introduction Up to this point, we practiced constructing Open Inventor scenes and placed three-dimensional Open Inventor objects of different colors and shapes within them. In this tutorial, we will go over the steps to add shadows to our 3D-objects, make them glow and vary their opacity to make them transparent. We will also incorporate WEMs from multi-frame DICOMs and render them as scene objects to see how different post effects can be used on them.","content":"Example 4: Post Effects in Open Inventor Introduction Up to this point, we practiced constructing Open Inventor scenes and placed three-dimensional Open Inventor objects of different colors and shapes within them. In this tutorial, we will go over the steps to add shadows to our 3D-objects, make them glow and vary their opacity to make them transparent. We will also incorporate WEMs from multi-frame DICOMs and render them as scene objects to see how different post effects can be used on them.\nSteps to follow From DICOM to scene object To incorporate DICOMs into your Open Inventor Scene, they have to be rendered as Open Inventor objects, which can be done by converting them into WEMs first. Begin by adding the modules LocalImage, WEMIsoSurface and SoWEMRenderer to your workspace. Open the panel of the LocalImage module, browse your files and choose a DICOM with multiple frames as input data. Connect the LocalImage module\u0026rsquo;s output connector to WEMIsoSurface module\u0026rsquo;s input connector to create a WEM of the study\u0026rsquo;s surface. Then connect the WEMIsoSurface module\u0026rsquo;s output connector to the SoWEMRenderer module\u0026rsquo;s input connector to render a scene object, that can be displayed by adding a SoExaminerViewer module to the workspace and connecting the SoWEMRenderer module\u0026rsquo;s output connector to its input connector.\nCheck:\u0026nbsp; We don\u0026rsquo;t recommend using single frame DICOMs for this example as a certain depth is required to interact with the scene objects as intended. Also make sure that the pixel data of the DICOM file you choose contains all slices of the study, as it might be difficult to arrange scene objects of individual slices to resemble the originally captured study. How to create a scene object out of a multi-frame DICOM Info:\u0026nbsp; Consider adding a View2D and an Info module to your LocalImage module\u0026rsquo;s output connector to be able to compare the rendered object with the original image and adapt the ISO values to minimize noise. PostEffectShader To apply shading to our DICOM scene object, add a SoShaderPipeline and a SoShaderPipelineCellShading module to our network and connect their output connectors to a SoToggle module\u0026rsquo;s input connector. Then connect the SoToggle module\u0026rsquo;s output connector to the SoExaminerViewer, but on the left side of the connection to the SoWEMRenderer module. This way, shading can be toggled and is applied to all scene objects connected to the right of the SoToggle module\u0026rsquo;s connection.\nShading toggled off Shading toggled on Tidying your workspace and preparing the next steps Now add a SoPostEffectBackground module to your workspace and connect its output connector to the SoExaminerViewer module\u0026rsquo;s input connector. Group the modules SoToggle, SoShaderPipeline and SoShaderPipelineCellShading together and name the group \u0026ldquo;Toggle Shading\u0026rdquo;. Then, group the modules SoWEMRenderer, WEMIsoSurface and LocalImage together and name the group \u0026ldquo;DICOM Object\u0026rdquo;.\nInfo:\u0026nbsp; Structuring the workspace by grouping modules based on their functionality helps to stay focused and keeps everything tidy. Use a SoPostEffectMainGeometry module to connect both of the groups you just created to the SoExaminerViewer module. Lastly, add a SoPostEffectRenderer module to your workspace and connect its output connector to the SoExaminerViewer module\u0026rsquo;s input connector.\nGrouped modules You can now change your Open Inventor scene\u0026rsquo;s background color.\nPostEffectEdges Add the module SoPostEffectEdges to your workspace and connect its output connector with the SoExaminerViewer module\u0026rsquo;s input connector. Then open its panel and choose a color. You can try different modes, sampling distances and thresholds:\nColored edges Varying settings of colored edges Varying settings of colored edges PostEffectGeometry To include geometrical objects in your Open Inventor scene, add two SoSeparator modules to the workspace and connect them to the input connector of SoPostEffectMainGeometry. Then add a SoMaterial, SoTransform and SoSphere or SoCube module to each SoSeparator and adjust their size (using the panel of the SoSphere or SoCube module) and placement within the scene (using the panel of the SoTransform module) as you like.\nCheck:\u0026nbsp; You\u0026rsquo;ll observe that the transparency setting in the SoMaterial module does not apply to the geometrical objects. Add a SoPostEffectTransparentGeometry module to your workspace, connect its output connector to the SoExaminerViewer module\u0026rsquo;s input connector and its input connectors to the SoSeparator module\u0026rsquo;s output connector to create transparent geometrical objects in your scene. Workspace PostEffectGlow To put a soft glow on the geometrical scene objects, the module SoPostEffectGlow can be added to the workspace.\nApplied SoPostEffectGlow Summary Multi-frame DICOM images can be rendered to be scene objects by converting them into WEMs first Open Inventor scenes can be augmented by adding PostEffects to scene objects \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Open Inventor","Post Effects"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/visualization/","title":"Chapter III: Visualization","summary":"Visualization in MeVisLab Introduction Images and data objects can be rendered in 2D and 3D and interacted with in several ways using a set of tools available through MeVisLab. In this chapter in particular, we will focus on simple image interaction with two- and three-dimensional visualizations.\nInfo:\u0026nbsp; Not only pixel- and voxel-based data, but also scene objects and 3D scenes can be visualized. See our tutorial on OpenInventorModules for further information.","content":"Visualization in MeVisLab Introduction Images and data objects can be rendered in 2D and 3D and interacted with in several ways using a set of tools available through MeVisLab. In this chapter in particular, we will focus on simple image interaction with two- and three-dimensional visualizations.\nInfo:\u0026nbsp; Not only pixel- and voxel-based data, but also scene objects and 3D scenes can be visualized. See our tutorial on OpenInventorModules for further information. View2D and View3D An easy way to display data and images in 2D and 3D is by using the Modules View2D and View3D. What can be done with these viewers?\nView2D and View3D View2D Scroll through the slices using the mouse wheel and/or middle mouse button .\nChange the contrast of the image by clicking the right mouse button and move the mouse\nZoom in and out by pressing CTRL and middle mouse button Toggle between multiple timepoints (if available) via \u0026larr; ArrowLeft and \u0026rarr; ArrowRight More features can be found on the help page.\nInfo:\u0026nbsp; The View2DExtensions module provides additional ways to interact with an image. View2DExtensions View3D Zoom in and out using the mouse wheel Drag the 3D objects using the middle mouse button Change the contrast of the image by clicking the right mouse button and moving the mouse\nRotate the object by pressing the left mouse button and moving the object around. The present orientation is displayed by a cube in the bottom right corner.\nMore features, like recording movies, can be found on the help page.\nToggle between multiple timepoints (if available) via \u0026larr; ArrowLeft and \u0026rarr; ArrowRight Info:\u0026nbsp; More Information on Image Processing in MeVisLab can be found here ","tags":["Beginner","Tutorial","Visualization","2D","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample1/","title":"Example 1: Synchronous view of two images","summary":"Example 1: Synchronous view of two images Introduction In this example we like to use the module SynchroView2D to be able to inspect two different images simultaneously.\nThe module SynchroView2D provides two 2D viewers that are synchronized.\nAs in Tutorial Chapter 1 - Basic Mechanics of MeVisLab, the processed and the unprocessed image can be displayed simultaneously. Scrolling through one image automatically changes the slices of both viewers, so slices with the same slice number are shown in both images.","content":"Example 1: Synchronous view of two images Introduction In this example we like to use the module SynchroView2D to be able to inspect two different images simultaneously.\nThe module SynchroView2D provides two 2D viewers that are synchronized.\nAs in Tutorial Chapter 1 - Basic Mechanics of MeVisLab, the processed and the unprocessed image can be displayed simultaneously. Scrolling through one image automatically changes the slices of both viewers, so slices with the same slice number are shown in both images.\nThe difference is that we are now using an already existing module named SynchroView2D.\nExtra Infos:\u0026nbsp; The SynchroView2D module is explained here Steps to do Develop your network Start the example by adding the module LocalImage to your workspace to load the example image Tumor1_Head_t1.small.tif. Next, add and connect the following modules as shown.\nSynchroView2D Viewer Summary Multiple images can be synchronized by the SynchroView2D module \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample2/","title":"Example 2: Creating a magnifier","summary":"Example 2: Creating a magnifier \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Medical images are typically displayed in three different viewing directions (see image): coronal, axial and sagittal.\nUsing the Viewer OrthoView2D you are able to decide, which viewing direction you like to use. In addition to that, you have the opportunity to display all three orthogonal viewing directions simultaneously. Here, we like to display an image of the head in all three viewing directions and mark positions in the image.","content":"Example 2: Creating a magnifier \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Medical images are typically displayed in three different viewing directions (see image): coronal, axial and sagittal.\nUsing the Viewer OrthoView2D you are able to decide, which viewing direction you like to use. In addition to that, you have the opportunity to display all three orthogonal viewing directions simultaneously. Here, we like to display an image of the head in all three viewing directions and mark positions in the image.\nBody Planes Steps to do Develop your network In this example, use the module LocalImage to load the example image MRI_Head.tif. Now, connect the module OrthoView2D with the loaded image. The image is displayed in three orthogonal viewing directions. The yellow marker displays the same voxel in all three images. You can scroll through the slices in all three viewing directions.\nExtra Infos:\u0026nbsp; In case your image is black, change the Window and Center values by moving the mouse with right mouse button\npressed.\nOrthoView2D SoView2DPosition Next, we add the module SoView2DPosition (an Open Inventor module).\nThe module enables the selection of an image position via mouse-click . The last-clicked location in the Viewer is marked in white. If you now scroll through the slices, both, the last-clicked location and the current image location are shown.\nSoView2DPosition SoView2DRectangle Instead of points, we like to mark areas. In order to do that, replace the module SoView2DPosition with the module SoView2DRectangle. The module allows to add a rectangle to the image. Left-click on the image and draw a rectangle. In the OthoView2D, the rectangle is displayed in every viewing direction.\nSoView2DRectangle Using a rectangle to build a magnifier We like to use the module SoView2DRectangle to create a magnifier. In order to do that add the following modules to your workspace and connect them as shown below. We need to connect the module SoView2DRectangle to a hidden input connector of the module SynchroView2D. To be able to do this, click on your workspace and afterwards press SPACE . You can see, that SynchroView2D possesses Open Inventor input connectors. You can connect your module SoView2DRectangle to one of these connectors.\nHidden Inputs of SynchroView2D Connect Hidden Inputs of SynchroView2D In addition to that, add two types of the module DecomposeVector3 to your network. In MeVisLab exist different data types, for example vectors, or single variable, which contain the data type float or integer. This module can be used to convert field values of type vector (in this case a vector consisting of three entries) into three single coordinates. You will see in the next step, why this module can be useful.\nDecomposeVector3 We like to use the module SubImage to select a section of a slice, which is then displayed in the Viewer. The idea is to display a magnified section of one slice next to the whole slice in the module SynchroView2D. In order to do that, we need to tell the module SubImage which section to display in the Viewer. The section is selected using the module SoView2DRectangle. As a last step, we need to transmit the coordinates of the chosen rectangle to the module SubImage. To do that, we will build some parameter connections.\nSubImage Now, open the panels of the modules SoView2DRectangle and DecomposeVector3 and DecomposeVector31.\nWe here rename the DecomposeVector3 modules (press F2 to do that), for a better overview.\nIn the panel of the module Rectangle in the box Position you can see the position of the rectangle given in two 3D vectors.\nWe like to use the modules DecomposeVector3 to extract the single x, y and z values of the vector. For that, create a parameter connection from the field Start Wold Pos to the vector of the module we named StartWorldPos_Rectangle and create a connection from the field End World Pos to the vector of module EndWorldPos_Rectangle. The decomposed coordinates can be now used for further parameter connections.\nParameter Connections Open the panel of the module SubImage. Select the Mode World Start \u0026amp; End (Image Axis Aligned). Enable the function Auto apply.\nExtra Infos:\u0026nbsp; Make sure to also check Auto-correct for negative subimage extents so that you can draw rectangles from left to right and from right to left. World Coordinates Now, create parameter connections from the fields X, Y, Z of the module StartWorldPos_Rectangle to the field Start X, Start Y, Start Z in the panel of the module SubImage. Similarly, connect the parameter fields X, Y, Z of the module EndWorldPos_Rectangle to the field End X, End Y, End Z in the panel of the module SubImage.\nAnother Parameter Connection With this, you finished your magnifier. Open the Viewer and draw a rectangle on one slice, to see the result.\nFinal Magnifier with SubImage Exercises Invert the image inside your magnified SubImage without changing the original image. You can use Arithmetic* modules for inverting.\nSummary The module OrthoView2D provides coronal, axial and sagittal views of an image. The SubImage module allows to define a region of an input image to be treated as a separate image. Single x, y and z coordinates can be transferred to a 3-dimensional vector and vice versa by using ComposeVector3 and DecomposeVector3 Some modules provide hidden in- and outputs which can be shown via SPACE \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D","Magnifier"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample3/","title":"Example 3: Image Overlays","summary":"Example 3: How to blend images over each other \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will show you how to blend a 2D image over another one. With the help of the module SoView2DOverlay we will create an overlay, which allows us to highlight all bones in the scan.\nSteps to do Develop your network Start this example by adding the shown modules, connecting the modules to form a network and loading the example image Bone.","content":"Example 3: How to blend images over each other \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will show you how to blend a 2D image over another one. With the help of the module SoView2DOverlay we will create an overlay, which allows us to highlight all bones in the scan.\nSteps to do Develop your network Start this example by adding the shown modules, connecting the modules to form a network and loading the example image Bone.tiff.\nOpen the panel of the module Threshold and configure the module as shown below.\nExtra Infos:\u0026nbsp; The Threshold module is explained here The module Threshold compares the contrast of each voxel of the image with a customized threshold. In this case: If the contrast of the chosen voxel is lower than the threshold, the voxel contrast is replaced by the minimum contrast of the image. If the contrast of the chosen voxel is higher than the threshold, the voxel contrast is replaced by the maximum contrast of the image. With this, we can construct a binary image, which divides the image into bone (white) and no bone (black).\nSelect output of the Threshold module to see the binary image in Output Inspector.\nImage Threshold Overlays The module SoView2DOverlay blends a 2D image over another one in a 2D viewer. In this case, all voxels with contrast above the Threshold are colored and therefore highlighted. The colored voxels are then blended over the original image. Using the panel of SoView2DOverlay, you can select the color of the overlay.\nSoView2DOverlay Extra Infos:\u0026nbsp; The SoView2DOverlay module is explained here Exercises Play around with different Threshold values and SoView2DOverlay colors. Visualize your generated threshold mask in 3D by using the View3D module Summary The module Threshold applies a relative or an absolute threshold to a voxel image. The module SoView2DOverlay blends an 2D image over another one in a 2D viewer. You can also use a 3D SoRenderArea for the same visualizations. An example can be seen in the next Example 4 Warning:\u0026nbsp; The SoView2DOverlay module is not intended to work with OrthoView2D; in this case, use a GVROrthoOverlay. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D","Overlays","Masks"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample4/","title":"Example 4: Display 2D images in Open Inventor SoRenderArea","summary":"Example 4: Display images converted to Open Inventor scene objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous example you learned how to use the module SoView2DOverlay together with a View2D. MeVisLab provides a whole family of SoView2D* modules (SoView2DOverlay, SoView2DRectangle, SoView2DGrid, \u0026hellip;). All these modules create or interact with scene objects and are based on the module SoView2D, which can convert a voxel-image into a scene object.","content":"Example 4: Display images converted to Open Inventor scene objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous example you learned how to use the module SoView2DOverlay together with a View2D. MeVisLab provides a whole family of SoView2D* modules (SoView2DOverlay, SoView2DRectangle, SoView2DGrid, \u0026hellip;). All these modules create or interact with scene objects and are based on the module SoView2D, which can convert a voxel-image into a scene object. In this example, you will get to know some members of the SoView2D-family.\nExtra Infos:\u0026nbsp; More information about the SoView2D-family can be found here\nand in the SoView2D Reference\nSteps to do Develop your network We will start the example by creating an overlay again. Add the following modules and connect them as shown. Select a Threshold and a Comparison Operator for the module Threshold as in the previous example. The module SoView2D converts the image into a scene-object. The image as well as the overlay is rendered and displayed by the module SoRenderArea.\nSoRenderArea Add Extension You may have noticed, that you are not able to scroll through the slices. This functionality is not yet implemented in the viewer SoRenderArea. To add a set of functionalities and viewer extensions, which are commonly used in conjunction with a 2D viewer, add the module View2DExtensions to the workspace and connect it as shown below. Now, additional information of the image can be displayed in the viewer and you can navigate and scroll through the slices.\nView2DExtensions Add Screenshot Gallery to Viewing Area With the help of the module SoRenderArea you can record screenshots and movies. Before we do that, open [ View \u0026rarr; Views \u0026rarr; Screenshot Gallery ], to add the Screenshot Gallery to your viewing area.\nScreenshot Gallery Create screenshots and movies If you now select your favorite slice of the bone in the Viewer SoRenderArea and press F11 , a screenshot is taken and displayed in the Screenshot Gallery. For recording a movie, press F9 to start the movie and F10 to stop recording. You can find the movie in the Screenshot Gallery.\nRecord Movies and Snapshots Exercises Create movies of a 3D Scene Summary Modules of the SoView2D-family create or interact with scene objects and are based on the module SoView2D, which can convert a voxel-image into a scene object The SoRenderArea module provides functionalities for screenshots and movie generation \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","2D","3D","Open Inventor","Snapshots","Movies"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample5/","title":"Example 5: Volume rendering and interactions","summary":"Example 5: Volume rendering and interactions \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to convert a scan of a head into a 3D scene-object. The scene-object allows to add some textures, interactions and animations.\nSteps to do Develop your network Implement the following network and open the image $(DemoDataPath)/BrainMultiModal/ProbandT1.tif.\nSoGVRVolumeRenderer The module SoGVRVolumeRenderer allows volume rendering of 3D and 4D images.\nExtra Infos:\u0026nbsp; Additional information about Volume Rendering can be found here: Giga Voxel Renderer Change LUT We like to add a surface color to the head.","content":"Example 5: Volume rendering and interactions \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to convert a scan of a head into a 3D scene-object. The scene-object allows to add some textures, interactions and animations.\nSteps to do Develop your network Implement the following network and open the image $(DemoDataPath)/BrainMultiModal/ProbandT1.tif.\nSoGVRVolumeRenderer The module SoGVRVolumeRenderer allows volume rendering of 3D and 4D images.\nExtra Infos:\u0026nbsp; Additional information about Volume Rendering can be found here: Giga Voxel Renderer Change LUT We like to add a surface color to the head. In order to do that, we add the module SoLUTEditor, which adds an RGBA Look up table (LUT) to the scene. Connecting this module to SoExaminerViewer left to the connection between SoGVRRenderer and SoExaminerViewer (remember the order in which Open Inventor modules are executed) allows you to set the surface color of the head.\nSoLUTEditor To change the color, open the panel of SoLUTEditor. In this editor we can change color and transparency interactively (for more information take a look at the help page ). Here, we have a range from black to white and from complete transparency to full opacity.\nSoLUTEditor change colors We now like to add color. New color-points can be added by clicking on the color bar at the bottom side of the graph and existing points can be moved by dragging. You can change the color of each point under Color.\nSoLUTEditor add colors Interactions As a next step, we add some dynamics to the 3D scene: We like to rotate the head. In Order to do this, add the modules SoRotationXYZ and SoElapsedTime to the workspace and connect the modules as shown.\nSoRotationXYZ Open the panels of both modules and select the axis the image should rotate around. In this case the z-axis was selected. Now, build a parameter connection from the parameter Time out of the module SoElapsedTime to the parameter Angle of the module SoRotationXYZ. The angle changes with time and the head starts turning.\nTime and Angle Exercises Change rotation speed change rotation angle Pause rotation on pressing SPACE Summary The module SoGVRVolumeRenderer renders paged images like DICOM files in a GVR. Lookup Tables (LUT) allow you to modify the color of your renderings \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","3D","Volume Rendering","GVR","LUT"],"section":"tutorials"},{"date":"1677110400","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample6/","title":"Example 6: MeVis Path Tracer","summary":"Example 6: MeVis Path Tracer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;We have a Short video showing the possibilities of the MeVis Path Tracer on YouTube. Introduction The MeVis Path Tracer offers a Monte Carlo Path Tracing framework running on CUDA GPUs. It offers photorealistic rendering of volumes and meshes, physically based lightning with area lights and soft shadows and fully integrates into MeVisLab Open Inventor (camera, depth buffer, clipping planes, etc.).\nExtra Infos:\u0026nbsp; CUDA is a parallel computing platform and programming model created by NVIDIA.","content":"Example 6: MeVis Path Tracer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;We have a Short video showing the possibilities of the MeVis Path Tracer on YouTube. Introduction The MeVis Path Tracer offers a Monte Carlo Path Tracing framework running on CUDA GPUs. It offers photorealistic rendering of volumes and meshes, physically based lightning with area lights and soft shadows and fully integrates into MeVisLab Open Inventor (camera, depth buffer, clipping planes, etc.).\nExtra Infos:\u0026nbsp; CUDA is a parallel computing platform and programming model created by NVIDIA. For further information, see NVIDIA website. PathTracer1 PathTracer2 PathTracer3 PathTracer4 PathTracer5 The SoPathTracer module implements the main renderer (like the SoGVRVolumeRenderer). It collects all SoPathTracer* extensions (on its left side) in the scene and renders them. Picking is also supported, but currently only the first hit position. It supports an arbitrary number of objects with different orientation and bounding boxes.\nPath Tracing Path Tracing allows interactive, photorealistic 3D environments with dynamic light and shadow, reflections and refractions.\nTraditional volume rendering is a technique used to visualize 3D volumetric data by rendering 2D images of the data from different viewpoints. It typically uses a transfer function that maps the scalar values of the volume to colors and opacities, which are then used to create a 2D image of the volume. This technique can produce visually pleasing images of volumetric data, but it can struggle with complex lighting and shadow effects, and it may not accurately capture the scattering and absorption of light within the volume.\nMonte Carlo path tracing is a technique used to simulate the behavior of light in a scene by tracing rays of light as they bounce around the scene and interact with objects. It uses statistical methods to simulate the behavior of light, making it more accurate than traditional volume rendering for simulating the scattering and absorption of light within the volume. However, path tracing can be computationally expensive, as it requires many iterations to produce a high-quality image.\nRay tracing is a technique for modeling light transport. It follows all light rays throughout the entire scene. Depending on the scene this takes a lot of time to fully compute the resulting pixels. Other than ray tracing, path tracing only traces the most likely path of the light by using the Monte Carlo method. Computation is much faster but the results are comparable.\nExtra Infos:\u0026nbsp; For more information about Path Tracing, see the NVIDIA website. Modules The SoPathTracer is the main renderer of the framework and should always appear on the right in your scene. It collects the current Open Inventor camera and clipping planes and uses them when rendering.\nThere are various extensions that can be used.\nVolumes SoPathTracerVolume loads and renders a volume, multiple volumes with arbitrary world coordinates are supported\nVolumes support: Diffuse/emissive/material LUT Shading options Subvolume mixing Additional transformation matrix Material selection SoPathTracerMaskVolume can be used to mask voxels in SoPathTracerVolume volumes\nAllows to load a 8bit mask volume The mask volume can be used by any volume or instance It allows to: Change the alpha and color of inside/outside voxels Change the tag value (see SoPathTracerVolume) Very useful in combination with SoVolumeCutting SoPathTracerTagVolume can be used to tag voxels in SoPathTracerVolume volumes\nAllows to load a 8bit tag volume The tags are used to select a per-object LUT and/or material A 2D LUT can be provided using LUTConcat or SoLUTEditor2D Per-tag materials can be provided by adding multiple materials to the inMaterial scene Useful to render segmented objects SoPathTracerVolumeInstance can be used to render a SoPathTracerVolume with differnt transformation, subvolume, LUT, material, \u0026hellip;\nAllows to instantiate an existing volume Supports all options of a normal volume, but only references the volume itself This allows to: Use a different LUT Change shading options Change subvolume Change transformation matrix Different material selection SoPathTracerSlice renders a slice at the given plane, showing the volume data of the given volume\nAllows to render a cut slice through a volume Allows to set an arbitrary plane and works on volumes and instances Has its own LUT and can be opaque or transparent SoPathTracerIsoSurface renders an iso surface (with first hit refinement) on the given base volume\nAllows to render an ISO surface of a volume Works on volumes and instances Supports opaque and transparent surfaces ISO surface is rendered on-the-fly Hit refinement is used to provide high-quality surfaces Arbitrary material can be specified Geometry SoPathTracerMesh scans the input scene for triangle meshes and ray traces them Allows to render arbitrary triangle meshes Scans the input scene for triangle meshes and converts them to a bounding volume hierarchy (BVH) Supports different materials by adding SoPathTracerMaterials Objects can be turned on/off via material nodes (without BVH rebuilding) Supports opaque and transparent meshes SoPathTracerLines scans the input scene for line sets and ray traces them as cylinders with round caps Allows to render thick lines (capsules) Scans the input scene for SoLineSet/SoIndexedLineSet and converts them to a BVH Different materials are supported by adding SoPathTracerMaterial nodes Supports opaque and transparent lines Useful to render fibers or stream lines SoPathTracerSpheres renders a marker list as ray traced spheres Allows to render markers as spheres Converts marker list to spheres and creates a BVH Currently only supports single material Lights SoPathTracerAreaLight provides a realistic area light with attenuation, area and distance Provides an area light Multiple area lights are supported Lights can be placed: Around the scene bounding box using polar coordinates At absolute camera or world position (as head or local light) Light intensity is automatically adapted to the scene size Otherwise it would be hard to select an intensity that works on different scales SoPathTracerBackgroundLight provides a background light, using image based lighting from a sphere or cube map Provides environmental lighting It supports: Specifying environment light colors Image based lighting using a cube map or sphere map Only one background light can be added to a scene Material SoPathTracerMaterial allows to specify which material should be used for a given object. Provides material settings for other nodes Offers to select the used material and its parameters Is connected to the inMaterial of other nodes Multiple materials may be placed into the input scene of SoPathTracerMesh and SoPathTracerLines Allows to override volume shader settings as well The following examples shall help you to learn how to use the modules.\n","tags":["Advanced","Tutorial","Visualization","3D","Volume Rendering","Path Tracer"],"section":"tutorials"},{"date":"1677110400","url":"https://mevislab.github.io/examples/tutorials/visualization/pathtracer/pathtracerexample1/","title":"Example 6.1: Volume Rendering vs. Path Tracer","summary":"Example 6.1: Volume Rendering vs. Path Tracer Introduction In this example you develop a network to show some differences between volume rendering and the MeVisLab Path Tracer. You will visualize the same scene using both 3D rendering techniques and some of the modules for path tracing.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace.","content":"Example 6.1: Volume Rendering vs. Path Tracer Introduction In this example you develop a network to show some differences between volume rendering and the MeVisLab Path Tracer. You will visualize the same scene using both 3D rendering techniques and some of the modules for path tracing.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace. You will see a message if your hardware does not support CUDA:\nMeVisLab detected an Intel onboard graphics adapter. If you experience rendering problems, try setting the environment variables SOVIEW2D_NO_SHADERS and GVR_NO_GLSL. Handling cudaGetDeviceCount returned 35 (CUDA driver version is insufficient for CUDA runtime version)\nSteps to do As a first step for comparison, you are creating a 3D scene with 2 spheres using the already known volume rendering.\nVolume Rendering Create 3D objects Add 3 WEMInitialize modules for 1 Cube and 2 Icosphere to your workspace and connect each of them to a SoWEMRenderer. Set instanceName of the WEMInitialize modules to Sphere1, Sphere2 and Cube. Set instanceName of the SoWEMRenderer modules to RenderSphere1, RenderSphere2 and RenderCube.\nFor RenderSphere1 define a Diffuse Color yellow and set Face Alpha to 0.5. The RenderCube remains as is and the RenderSphere2 is defined as Diffuse Color red and Face Alpha 0.5.\nGroup your modules and name the group Initialization. Your network should now look like this:\nExample Initialization Use the Output Inspector for your SoWEMRenderer outputs and inspect the 3D rendering. You should have a yellow and a red sphere and a grey cube.\nSphere1 Sphere2 Cube Rendering Add 2 SoGroup modules and a SoBackground to your network. Connect the modules as seen below.\nExample Group If you now inspect the output of the SoGroup, you will see an orange sphere.\nMissing Translation You did not translate the locations of our 3 objects, they are all located at the same place in world coordinates. Open the WEMInitialize panels of your 3D objects and define the following translations and scalings:\nWEMInitializeSphere1 WEMInitializeSphere2 WEMInitializeCube The result of the SoGroup now shows 2 spheres on a rectangular cube.\nObjects Translated and Scaled For the viewer, you now add a SoCameraInteraction, a SoDepthPeelRenderer and a SoRenderArea module to your network and connect them.\nNetwork with Viewer You now have a 3D volume rendering of our 3 objects.\nIn order to distinguish between the 2 viewers, you now add a label to the SoRenderArea describing that this is the Volume Rendering. Add a SoMenuItem, a SoBorderMenu and a SoSeparator to your SoRenderArea.\nSoMenuItem Define the Label of the SoMenuItem as Volume Rendering and set Border Alignment to Top Right and Menu Direction to Horizontal for the SoBorderMenu.\nLabel in SoRenderArea Finally, you should group all modules belonging to your volume rendering.\nVolume Rendering Network Path Tracing For the Path Tracer, you can just re-use our 3D objects from volume rendering. This helps us to compare the rendering results.\nRendering Path Tracer modules fully integrate into MeVisLab Open Inventor, therefore the general principles and the necessary modules are not completely different. Add a SoGroup module to your workspace and connect it to your 3D objects from SoWemRenderer. A SoBackground as in volume rendering network is not necessary but you add a SoPathTracerMaterial and connect it to the SoGroup. You can leave all settings as default for now.\nPath Tracer Material Add a SoPathTracerAreaLight, a SoPathTracerMesh and a SoPathTracer to a SoSeparator and connect the SoPathTracerMesh to your SoGroup. This adds your 3D objects to a Path Tracer Scene.\nPath Tracer Selecting the SoSeparator output already shows a preview of the same scene rendered via Path Tracing.\nPath Tracer Preview Add a SoCameraInteraction and a SoRenderArea to your network and connect them as seen below.\nSoCameraInteraction You can now use both SoRenderArea modules to visualize the differences side by side. You should also add the SoMenuItem, a SoBorderMenu and a SoSeparator to your SoRenderArea in order to have a label for Path Tracing inside the viewer.\nDefine the Label of the SoMenuItem as Path Tracing and set Border Alignment to Top Right and Menu Direction to Horizontal for the SoBorderMenu.\nLabel in SoRenderArea Finally, group your Path Tracer modules to another group named Path Tracing.\nNew Group for Path Tracing Side by Side Share the same camera Finally, you want to have the same camera perspective in both viewers so that you can see the differences. Add a SoPerspectiveCamera module to your workspace and connect it to the volume rendering and the Path Tracer network. The Path Tracer network additionally needs a SoGroup, see below for connection details. You have to toggle detectCamera in both of your SoCameraInteraction modules in order to synchronize the view for both SoRenderArea viewers.\nCamera Synchronization Additional Info:\u0026nbsp; Path Tracing requires a lot of iterations through the image before reaching the best possible result. You can see the maximum number of iterations defined and the current iteration in the SoPathTracer panel. The more iterations, the better the result but the more time it takes to finalize your image. PathTracer_1_Iteration PathTracer_100_Iterations PathTracer_1000_Iterations Results Path Tracing provides a much more realistic way to visualize the behavior of light in a scene. It simulates the scattering and absorption of light within the volume.\nExercises Play around with different SoPathTracerMaterial settings and define different materials Change the maximum number of iterations in SoPathTracer module Change the configurations in SoPathTracerAreaLight module Summary Path Tracer modules can be used the same way as Open Inventor modules A SoPerspectiveCamera can be used for multiple viewers to synchronize camera position Path Tracing produces beautiful, photorealistic renderings, but can be computationally expensive \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Volume Rendering","Path Tracer"],"section":"tutorials"},{"date":"1704153600","url":"https://mevislab.github.io/examples/tutorials/visualization/pathtracer/pathtracerexample2/","title":"Example 6.2: Visualization using Path Tracer","summary":"Example 6.2: Visualization using SoPathTracer Introduction In this tutorial, we will explain the basics of using the SoPathTracer module in MeVisLab. You will learn how to create a scene, assign materials, add light sources, and configure the PathTracer to generate enhanced renderings.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace.","content":"Example 6.2: Visualization using SoPathTracer Introduction In this tutorial, we will explain the basics of using the SoPathTracer module in MeVisLab. You will learn how to create a scene, assign materials, add light sources, and configure the PathTracer to generate enhanced renderings.\nAttention:\u0026nbsp; The MeVis Path Tracer requires an NVIDIA graphics card with CUDA support. In order to check your hardware, open MeVisLab and add a SoPathTracer module to your workspace. You will see a message if your hardware does not support CUDA:\nMeVisLab detected an Intel onboard graphics adapter. If you experience rendering problems, try setting the environment variables SOVIEW2D_NO_SHADERS and GVR_NO_GLSL. Handling cudaGetDeviceCount returned 35 (CUDA driver version is insufficient for CUDA runtime version)\nSteps to do Develop your network Download and open the images by using a LocalImage module. Connect it to a View2D to visually inspect its contents.\nMR Image of Knee in 2D Replace the View2D module by a SoExaminerViewer. Add the modules SoPathTracerVolume and SoPathTracer to your workspace and connect them as seen below.\nThe SoPathTracerVolume enables the loading and transforming the data into renderable volumes for Path Tracing. The SoPathTracer is the main rendering module of the MeVis Path Tracer framework. It provides a much more realistic way to visualize the behavior of light in a scene. It simulates the scattering and absorption of light within the volume.\nAdditional Info:\u0026nbsp; It\u0026rsquo;s essential to consistently position the SoPathTracer module on the right side of the scene. This strategic placement ensures that the module can render all objects located in the scene before it accurately. SoPathTracerVolume \u0026amp; SoPathTracer If you check your SoExaminerViewer you will see a black box. We need to define a LUT for the grey values first.\nSoExaminerViewer Now connect the SoLUTEditor module to your SoPathTracerVolume as illustrated down bellow and you will be able to see the knee.\nSoLUTEditor Add a MinMaxScan module to the LocalImage module and open the panel. The module shows the minimal and maximal grey values of the volume.\nOpen the panel of the SoLUTEditor module and define Range between 0 and 2047 as calculated by the MinMaxScan.\nMinMaxScan Next, add lights to your scene. Connect a SoPathTracerAreaLight and a SoPathTracerBackgroundLight module to your SoExaminerViewer to improve scene lighting.\nThe SoPathTracerAreaLight module provides a physically based area light that illuminates the scene of a SoPathTracer. The lights can be rectangular or discs and have an area, color and intensity. They can be positioned with spherical coordinates around the bounding box of the renderer, or they can be position in world or camera space.\nThe SoPathTracerBackgroundLight module provides a background light for the SoPathTracer. It supports setting a top, middle and bottom color or alternatively it support image based lighting (IBL) using a sphere or cube map. Only one background light can be active for a given SoPathTracer.\nLights Open the panel of the SoPathTracerBackgroundLight module and choose your three colors.\nSoPathTracerBackgroundLight Manually define LUT Either define your desired colors for your LUT in the Editor tab manually as shown down below.\nLUT Load example LUT from file As an alternative, you can replace the SoLUTEditor with a LUTLoad and load this XML file to use a pre-defined LUT.\nLUTLoad Now, let\u0026rsquo;s enhance your rendering further by using the SoPathTracerMaterial module. This module provides essential material properties for geometry and volumes within the SoPathTracer scene.\nAdd a SoPathTracerMaterial module to your SoPathTracerVolume. Open it’s panel and navigate to the tab Surface Brdf. Change the Diffuse color for altering the visual appearance of surfaces. The Diffuse color determines how light interacts with the surface, influencing its overall color and brightness. Set Specular to 0.5, Shininess to 1.0 and Specular Intensity to 0.5.\nSoPathTracerMaterial Additional Info:\u0026nbsp; The resulting rendering in SoExaminerViewer might look different, depending on your defined LUT. Visualize Bones If you want to visualize multiple volumes at the same time, you need to add another SoPathTracerVolume and LocalImage for loading the mask to the SoExaminerViewer.\nAs the LUT of the second volume also differs, add another SoLUTEditor module or LUTLoad module to the new SoPathTracerVolume.\nFor later usage, you can also already add a SoPathTracerMaterial module to the SoPathTracerVolume module.\nLoad the Bones mask by using the new LocalImage module and preview it in a View2D.\nBones mask Start by disabling the visibility of your first volume by toggeling SoPathTracerVolume Enabled field to off. This helps improve the rendering of the bones itself and makes it easier to define colors for your LUT.\nLoad example LUT from file Once again, you can decide to define the LUT yourself in SoLUTEditor module, or load a prepared XML File in a LUTLoad module as provided here.\nManually define LUT If you want to define your own LUT, connect a MinMaxScan module to your LocalImage1 and define Range for the SoLUTEditor as already done before.\nMinMaxScan of Bones mask Open the panel of SoLUTEditor1 for the bones and go to tab Range and set New Range Min to 0 and New Range Max to 127. Define the following colors in tab Editor.\nSoLUTEditor1 You can increase the Shininess of the bones and change the Diffuse color in the Surface Brdf tab within the SoPathTracerMaterial1. Also set Specular to 0.5, Shininess to 0.904 and Specular Intensity to 0.466.\nSoPathTracerMaterial1 Visualize Vessels Repeat the process for the vessels. Add another LocalImage, SoPathTracerVolume, SoLUTEditor (or LUTLoad) and View2D module as seen below. Load this Vessels mask and check it using View2D.\nVessels mask Load example LUT from file Load a prepared XML File in a LUTLoad module as provided here\nManually define LUT Connect the MinMaxScan to your LocalImage2.\nAccess the SoLUTEditor2 panel in the tab Range and set the New Range Min to 0 and the New Range Max to 255. Additionally, modify the illustrated color settings within the Editor tab.\nMinMaxScan of Vessels mask SoLUTEditor2 Now you should set your first volume visible again by toggeling SoPathTracerVolume Enabled field to on.\nFinal Result Additional Info:\u0026nbsp; The resulting rendering in SoExaminerViewer might look different, depending on your defined LUTs. Final Result with Enhanced Visualization Summary: You can achieve photorealistic renderings using SoPathTracer and associated modules. Render volumes efficiently in SoPathTracer scenes with SoPathTracerVolume, enabling diverse rendering options, LUT adjustments, lights and material enhancements. Enhance your scene\u0026rsquo;s look by adjusting materials and colors interactively using SoPathTracerMaterial and SoLUTEditor. Use lighting modules such as SoPathTracerAreaLight and SoPathTracerBackgroundLight to optimize the illumination of your rendered scenes. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Path Tracer"],"section":"tutorials"},{"date":"1700524800","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample7/","title":"Example 7: Add 3D viewer to OrthoView2D","summary":"Example 7: Add 3D viewer to OrthoView2D \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will use the OrthoView2D module and add a 3D viewer to the layout Cube.\nSteps to do Develop your network Add the modules LocalImage and OrthoView2D to your workspace and connect them.\nNetwork The OrthoView2D module allows you to select multiple layouts. Select layout Cube Equal. The layout shows your image in three orthogonal viewing directions.","content":"Example 7: Add 3D viewer to OrthoView2D \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we will use the OrthoView2D module and add a 3D viewer to the layout Cube.\nSteps to do Develop your network Add the modules LocalImage and OrthoView2D to your workspace and connect them.\nNetwork The OrthoView2D module allows you to select multiple layouts. Select layout Cube Equal. The layout shows your image in three orthogonal viewing directions. The top left segment remains empty.\nOrthoView2D Layouts We now want to use a 3D rendering in the top left segment, whenever the layout Cube Equal is chosen. Add a View3D and a SoViewportRegion module to your workspace. Connect the LocalImage with your View3D. The image is rendered in 3D. Hit SPACE on your keyboard to make the hidden output of the View3D module visible. Connect it with your SoViewportRegion and connect the SoViewportRegion with the inInvPreLUT input of the OrthoView2D.\nNetwork with SoViewportRegion Open the OrthoView2D and inspect your layout.\nOrthoView2D with 3D You can see your View3D being visible in the bottom right segment of the layout behind the coronal view of the image. Open the panel of the SoViewportRegion module. In section X-Position and Width, set Left Border to 0 and Right Border to 0.5. In section Y-Position and Height, set Lower Border to 0 and Upper Border to 0.5. Also check Render delayed paths.\nDefine viewport region The View3D image is now rendered to the top left segment of the OrthoView2D, because the module SoViewportRegion renders a sub graph into a specified viewport region (VPR). The problem is: We cannot rotate and pan the 3D object, because there is no camera interaction available after adding the SoViewportRegion. The camera interaction is consumed by the View3D module before it can be used by the viewport.\nAdd a SoCameraInteraction module between the View3D and the SoViewportRegion. You can now interact with your 3D scene but the rotation is not executed on the center of the object. Trigger ViewAll on your SoCameraInteraction module.\nSoCameraInteraction You have now successfully added the View3D to the OrthoView2D, but there is still a problem remaining: If you change the layout to something different than LAYOUT_CUBE_EQUAL, the 3D content remains visible.\nWe can use a StringUtils module to resolve that. Set Operation to Compare and draw a parameter connection from the field OrthoView2D.layout to the field StringUtils.string1. The currently selected layout is displayed as String A. Enter LAYOUT_CUBE_EQUAL as String B. Now, draw a parameter connection from the field StringUtils.boolResult to the field SoViewportRegion.on.\nStringUtils If the selected layout in OrthoView2D now matches the string LAYOUT_CUBE_EQUAL (the field boolResult of the StringUtils module is TRUE), the SoViewportRegion is turned on. In any other case, the 3D segment is not visible.\nFinal Network Summary The module SoViewportRegion renders a sub graph into a specified viewport region (VPR) \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Visualization","3D","OrthoView2D"],"section":"tutorials"},{"date":"1701993600","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample8/","title":"Example 8: Vessel Segmentation using SoVascularSystem","summary":"Example 8: Vessel Segmentation using SoVascularSystem \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using an input mask to create a vessel centerline using the DtfSkeletonization module and visualize the vascular structures in 3D using the SoVascularSystem module. The second part uses the distance between centerline and surface of the vessel structures to color thin vessels red and thick vessels green.\nSteps to do Develop your network Load the example tree mask by using the LocalImage module.","content":"Example 8: Vessel Segmentation using SoVascularSystem \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using an input mask to create a vessel centerline using the DtfSkeletonization module and visualize the vascular structures in 3D using the SoVascularSystem module. The second part uses the distance between centerline and surface of the vessel structures to color thin vessels red and thick vessels green.\nSteps to do Develop your network Load the example tree mask by using the LocalImage module. Connect the output to a DtfSkeletonization module as seen below. The initial output of the DtfSkeletonization module is empty. Press the Update button to calculate the skeleton and the erosion distances.\nNetwork Below you can see the output of the original image taken from the LocalImage module (left) compared to the output after calculating the skeleton via DtfSkeletonization module (right).\nOutput comparison The output DtfSkeletonization.outBase1 shows nothing. Here you can find the 3-dimensional graph of the vascular structures. To generate it, open the panel of the DtfSkeletonization module, set Update Mode to Auto Update and select Update skeleton graph. Now the output additionally provides a 3D graph. Additionally, enable the Compile Graph Voxels to provide all object voxels at the output.\nDtfSkeletonization You can use the Output Inspector to see the 3D graph.\nGraph output of DtfSkeletonization If you want to visualize your graph, you should connect a GraphToVolume module to the DtfSkeletonization module. The result is a 2D or 3D volume of your graph which you can connect to any 2D or 3D viewer. Add a View2D and a View3D module to the GraphToVolume module and update the volume.\nGraphToVolume For coloring the vessels depending on their distances to the centerline, we need a SoLUTEditor module. Change your network to use a SoExaminerViewer module, a SoLUTEditor module and a SoBackground module instead of a View3D module.\nUse the SoLUTEditor for the View2D, too.\nNetwork Open the output of the GraphToVolume module and inspect the images in Output Inspector. You will see that the HU value of the black background is defined as -1, the vessel tree is defined as 0.\nOutput Inspector Open the Panel of the SoLUTEditor and select tab Range. Define New Range Min as -1 and New Range Max as 0.\nSoLUTEditor Range Change to Editor tab and define the following LUT:\nSoLUTEditor Editor The viewers now show your vessel graph.\nView2D and SoExaminerViewer Store Edge IDs in Skeletons with RunPythonScript Each edge of the calculated skeleton gets a unique ID defined by the DtfSkeletonization module. We now want to use this ID to define a different color for each edge of the skeleton. You can use the Label property of each skeleton to store the ID of the edge.\nAdd a RunPythonScript module to your network, open the panel of the module and enter the following Python code:\nctx.field(\u0026#34;DtfSkeletonization.update\u0026#34;).touch() graph = ctx.field(\u0026#34;DtfSkeletonization.outBase1\u0026#34;).object() if graph is not None: for edge in graph.getEdges(): print(edge.getId()) ctx.field(\u0026#34;GraphToVolume.update\u0026#34;).touch() First, we always want a fresh skeleton. We touch the update trigger of the module DtfSkeletonization. Then we get the graph from the DtfSkeletonization.outBase1 output. If a valid graph is available, we walk through all edges of the graph and print the ID of each edge. In the end, we update the GraphToVolume module to get the calculated values of the Python script in the viewers. Click Execute.\nThe Debug Output of the MeVisLab IDE shows a numbered list of edge IDs from 1 to 153.\nRunPythonScript We now want the edge ID to be used for coloring each of the skeletons differently. Open the Panel of the SoLUTEditor and select tab Range. Define New Range Min as 0 and New Range Max as 153. Define different colors for your LUT.\nSoLUTEditor The SoGVRVolumeRenderer module also needs a different setting. Open its panel in the Main tab, select Illuminated as the Render Mode. Adjust the Quality setting to 0.10. On tab Advanced, set Filter Volume Data to Nearest. Change to the Illumination tab and define below parameters:\nSoGVRVolumeRendererMain SoGVRVolumeRendererIllumination Change your Python script as follows: ctx.field(\u0026#34;DtfSkeletonization.update\u0026#34;).touch() graph = ctx.field(\u0026#34;DtfSkeletonization.outBase1\u0026#34;).object() if graph is not None: label = \u0026#34;Label\u0026#34; for edge in graph.getEdges(): for skeleton in edge.getSkeletons(): if label not in skeleton.properties: skeleton.createPropertyDouble(label, edge.getId()) skeleton.setProperty(label, edge.getId()) ctx.field(\u0026#34;GraphToVolume.update\u0026#34;).touch() In case the graph is valid, we now define a static text for the label. Instead of printing the edge ID, we also walk through each skeleton of the edge and define the property for the label using the ID of the edge as value.\nYour viewers now show a different color for each skeleton, based on our LUT.\nView2D and SoExaminerViewer Render Vascular System Using SoVascularSystem The SoVascularSystem module is optimized for rendering vascular structures. In comparison to the SoGVRVolumeRenderer module, it allows to render the surface, the skeleton or points of the structure in an open inventor scene graph. Interactions with edges of the graph are also already implemented.\nAdd a SoVascularSystem module to your workspace. Connect it to your DtfSkeletonization module and to the SoLUTEditor as seen below. Add another SoExaminerViewer for comparing the two visualization. The same SoBackground can be added to your new scene.\nUncheck Use skeleton colors and Use integer LUT on Appearance tab of the SoVascularSystem module panel.\nEditedNetwork Extra Infos:\u0026nbsp; More information about the SoVascularSystem module can be found in the help page\nof the module.\nDraw parameter connections from one SoExaminerViewer to the other. Use the fields seen below to synchronize your camera interaction.\nCamera positions Connect the backwards direction of the two SoExaminerViewer by using multiple SyncFloat modules and two SyncVector modules for position and orientation fields.\nExtra Infos:\u0026nbsp; To establish connections between fields with the type Float, you can use the SyncFloat module. For fields containing vector, the appropriate connection can be achieved using the SyncVector module. SyncFloat \u0026amp; SyncVector Camera interactions are now synchronized between both SoExaminerViewer modules.\nNow you can notice the difference between the two modules. We use SoVascularSystem for a smoother visualization of the vascular structures by using the graph as reference. The SoGVRVolumeRenderer renders the volume from the GraphToVolume module, including the visible stairs from pixel representations in the volume.\nSoVascularSystem \u0026amp; SoGVRVolumeRenderer The SoVascularSystem module has additional visualization examples unlike SoGVRVolumeRenderer. Open the panel of the SoVascularSystem module and select Random Points for Display Mode in the Main tab to see the difference.\nRandom Points Change it to Skeleton to only show the centerlines/skeletons of the vessels.\nSkeleton Warning:\u0026nbsp; For volume calculations, use the original image mask instead of the result from GraphToVolume. Enhance Vessel Visualization Based on Distance Information Now that you\u0026rsquo;ve successfully obtained the vessel skeleton graph using DtfSkeletonization, let\u0026rsquo;s take the next step to enhance the vessel visualization based on the radius information of the vessels. We will modify the existing code to use the minimum distance between centerline and surface of the vessels for defining the color.\nThe values for the provided vascular tree vary between 0 and 10mm. Therefore define the range of the SoLUTEditor to New Range Min as 1 and New Range Max as 10. On Editor tab, define the following LUT:\nSoLUTEditor In the RunPythonScript module, change the existing code to the following: ctx.field(\u0026#34;DtfSkeletonization.update\u0026#34;).touch() graph = ctx.field(\u0026#34;DtfSkeletonization.outBase1\u0026#34;).object() if graph is not None: label = \u0026#34;Label\u0026#34; print(\u0026#39;Num edges\u0026#39;, len(graph.getEdges())) for edge in graph.getEdges(): end_node = edge.getEndNode() for skeleton in edge.getSkeletons(): if label not in skeleton.properties: skeleton.createPropertyDouble(label, skeleton.getProperty(\u0026#34;MinDistance\u0026#34;)) skeleton.setProperty(label, skeleton.getProperty(\u0026#34;MinDistance\u0026#34;)) ctx.field(\u0026#34;GraphToVolume.update\u0026#34;).touch() ctx.field(\u0026#34;SoVascularSystem.apply\u0026#34;).touch() Warning:\u0026nbsp; Be aware that the MinDistance and MaxDistance values are algorithm-specific and don\u0026rsquo;t precisely represent vessel diameters. The result of DTFSkeletonization is a vascular graph with an idealized, circular profile while in reality, the vessels have more complicated profiles. It is an idealized graph where all vessels have a circular cross-section. This cross-section only has one radius, described by MinDistance and MaxDistance. Those are not the two radii of an elliptical cross-section, but the results of two different algorithms to measure the one, idealized radius at Skeletons. Instead of using the ID of each edge for the label property, we are now using the MinDistance property of the skeleton. The result is a color coded 3D visualization depending on the radius of the vessels. Small vessels are red, large vessels are green.\nRadius based Visualization Additional Info:\u0026nbsp; If you have a NIFTI file, convert it into an ML image. Load your tree mask NIfTI file using the itkImageFileReader module. Connect the output to a BoundingBox module, which removes black pixels and creates a volume without unmasked parts. In the end, add a MLImageFormatSave module to save it as *.mlimage file. They are much smaller than a NIFTI file.\nNIFTI file conversion Mouse Clicks on Vessel Graph Open the Interaction tab of the SoVascularSystem module. In SoExaminerViewer module, change to Pick Mode and click into your vessel structure. The panel of the SoVascularSystem module shows all information about the hit of your click in the vessel tree.\nGetting the click point in a vascular tree Summary Vessel centerlines can be created using a DtfSkeletonization module Vascular structures can be visualized using a SoVascularSystem module, which provides several vessel specific display modes The SoVascularSystem module provides information about mouse clicks into a vascular tree The labels of a skeleton can be used to store additional information for visualization \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Vessel Segmentation"],"section":"tutorials"},{"date":"1704672000","url":"https://mevislab.github.io/examples/tutorials/visualization/visualizationexample9/","title":"Example 9: Creating Dynamic 3D Animations using AnimationRecorder","summary":"Example 9: Creating Dynamic 3D Animations using AnimationRecorder \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using the AnimationRecorder module to generate dynamic and visually appealing animations of our 3D scenes. We will be recording a video of the results of our previous project, particularly the detailed visualizations of the muscles, bones and blood vessels created using PathTracer.\nSteps to do Open the network and files of Example 6.","content":"Example 9: Creating Dynamic 3D Animations using AnimationRecorder \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this tutorial, we are using the AnimationRecorder module to generate dynamic and visually appealing animations of our 3D scenes. We will be recording a video of the results of our previous project, particularly the detailed visualizations of the muscles, bones and blood vessels created using PathTracer.\nSteps to do Open the network and files of Example 6.2, add a SoSeparator module and an AnimationRecorder module to your workspace and connect them as shown below.\nThe SoSeparator module collects all components of our scene and provides one output to be used for the AnimationRecorder.\nThe AnimationRecorder module allows to create animations and record them as video streams. It provides an editor to create key frames for animating field values.\nAnimationRecorder Define the following LUTs in SoLUTEditor of the knee or load this XML file with LUTLoad1 to use a pre-defined LUT.\nSoLUTEditor Open the AnimationRecorder module and click on New to initiate a new animation, selecting a filename for the recorded key frames (*.mlmov).\nAt the bottom of the AnimationRecorder panel, you\u0026rsquo;ll find the key frame editor, which is initially enabled. It contains the camera track with a key frame at position 0. The key frame editor at the bottom serves as a control hub for playback and recording.\nExtra Infos:\u0026nbsp; Close the SoExaminerViewer while using the AnimationRecorder to prevent duplicate renderings and save resources. AnimationRecorder Key Frames in the AnimationRecorder mark specific field values at defined timepoints. You can add key frames on the timeline by double-clicking at the chosen timepoint or right-clicking and selecting Insert Key Frame. Between these key frames, values of the field are interpolated (linear or spline), or not. Selecting a key frame, a dialog Edit Camera Key Frame will open.\nWhen adding a key frame at a specific timepoint, you can change the camera dynamically in the viewer. This involves actions such as rotating to left or right, zooming in and out, and changing the camera\u0026rsquo;s location. Within the Edit Camera Key Frame dialog save each key frame by clicking on the Store Current Camera State button. Preview the video to observe the camera\u0026rsquo;s movement.\nThe video settings in the AnimationRecorder provide essential parameters for configuring the resulting animation. You can control the Framerate, determining the number of frames per second in the video stream. It\u0026rsquo;s important to note that altering the framerate may lead to the removal of key frames, impacting the animation\u0026rsquo;s smoothness.\nAdditionally, the Duration of the animation, specified as videoLength, defines how long the animation lasts in seconds. The Video Size determines the resolution of the resulting video.\nRepeat this process for each timepoint where adjustments to the camera position are needed, thus creating a sequence of key frames.\nBefore proceeding further, use the playback options situated at the base of the key frame editor. This allows for a quick preview of the initial camera sequence, ensuring the adjustments align seamlessly for a polished transition between key frames.\nExtra Infos:\u0026nbsp; Decrease the number of iterations in the SoPathTracer module for a quicker preview if you like. Make sure to increase again, before recording the final video. AnimationRecorder Modulating Knee Visibility with LUTRescale in Animation We want to show and hide the single segmentations during camera movements. Add two LUTRescale modules to your workspace and connect them as illustrated down below. The rationale behind using LUTRescale is to control the transparency or visibility of elements in the scene at different timepoints.\nLUTRescale Animate Bones and Vessels Now, let\u0026rsquo;s shift our focus to highlighting bones and vessels within the animation. Right-click on the LUTRescale module, navigate to Show Window, and select Automatic Panel. This will bring up the control window for the LUTRescale module. Search for the field named targetMax. You can either drag and drop it directly from the Automatic Panel, or alternatively, locate the Max field in the Output Index Range box within the module panel and then drag and drop it onto the fields section in the AnimationRecorder module, specifically under the Perspective Camera field.\nBy linking the targetMax field of the LUTRescale module to the AnimationRecorder, you establish a connection that allows you to define different values of the field for specific timepoints. The values between these timepoints can be interpolated as described above.\nLUTRescale \u0026amp; AnimationRecorder To initiate the animation sequence, start by adding a key frame at position 0 for the targetMax field. Set the Target Max value in the Edit Key Frame – [LUTRescale.targetMax] window to 1, and click on the Store Current Field Value button to save it.\nNext, proceed to add key frames at the same timepoints as the desired key frames of the Perspective Camera field\u0026rsquo;s first sequence. For each selected key frame, progressively set values for the Target Max field, gradually increasing to 10. This ensures specific synchronization between the visibility adjustments controlled by the LUTRescale module and the camera movements in the animation, creating a seamless transition. This gradual shift visually reveals the bones and vessels while concealing the knee structures and muscles.\nTo seamlessly incorporate the new key frame at the same timepoints as the Perspective Camera field, you have two efficient options. Simply click on the key frame of the first sequence, and the line will automatically appear in the middle of the key frame. A quick double-click will effortlessly insert a key frame at precisely the same position. If you prefer more accurate adjustments, you can also set your frame manually using the Edit Key Frame - [LUTRescale.targetMax] window. This flexibility allows for precise control over the animation timeline, ensuring key frames align precisely with your intended moments.\nLUTRescale \u0026amp; AnimationRecorder Showcasing only Bones To control the visibility of the vessels, right-click on the LUTRescale1 module connected to the vessels. Open the Show Window and select Automatic Panel. Drag and drop the targetMax field into the AnimationRecorder module\u0026rsquo;s fields section.\nLUTRescale1 \u0026amp; AnimationRecorder Add key frames for both the Perspective Camera field and the targetMax in LUTRescale1 at the same timepoints. Access the Edit Camera Key Frame window for the added key frame in the Perspective Camera field and save the current camera state. To exclusively highlight only bones, adjust the Target Max values from 1 to 10000 in Edit Key Frame - [LUTRescale1.targetMax].\nLUTRescale1 \u0026amp; AnimationRecorder To feature everything again at the end, copy the initial key frame of each field and paste it at the end of the timeline. This ensures a comprehensive display of all elements in the closing frames of your animation.\nFinal Animation Sequence Key Frames Finally, use the playback and recording buttons at the bottom of the key frame editor to preview and record your animation.\nSummary Animations are created by strategically placing key frames at different timepoints in the timeline using the AnimationRecorder module. It is possible to add any field of your network to your animation via drag-and-drop. The visibility of elements can be controlled using the LUTRescale module. Video settings in the AnimationRecorder can be adjusted to specify resolution, framerate, and duration of the resulting animation. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Visualization","3D","Animation Recorder","Movies"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/image_processing/","title":"Chapter IV: Image Processing","summary":"Image Processing in MeVisLab Digital image processing is the use of a digital computer to process digital images through an algorithm (see Wikipedia).\nMeVisLab provides multiple modules for image processing tasks, such as:\nFilters Masks Transformations Arithmetics Statistics For details about Image Processing in MeVisLab, see the MeVisLab Documentation In this chapter, you will find some examples for different types of image processing in MeVisLab.","content":"Image Processing in MeVisLab Digital image processing is the use of a digital computer to process digital images through an algorithm (see Wikipedia).\nMeVisLab provides multiple modules for image processing tasks, such as:\nFilters Masks Transformations Arithmetics Statistics For details about Image Processing in MeVisLab, see the MeVisLab Documentation In this chapter, you will find some examples for different types of image processing in MeVisLab.\n","tags":["Beginner","Tutorial","Image Processing"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/image_processing/image_processing1/","title":"Example 1: Arithmetic operations on two images","summary":"Example 1: Arithmetic operations on two images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We are using the Arithmetic2 module to apply basic scalar functions on two images. The module provides 2 inputs for images and 1 output image for the result.\nSteps to do Develop your network Add two LocalImage modules to your workspace for the input images. Select $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm and $(DemoDataPath)/BrainMultiModal/ProbandT2.dcm from MeVisLab demo data and add a SynchroView2D to your network.","content":"Example 1: Arithmetic operations on two images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We are using the Arithmetic2 module to apply basic scalar functions on two images. The module provides 2 inputs for images and 1 output image for the result.\nSteps to do Develop your network Add two LocalImage modules to your workspace for the input images. Select $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm and $(DemoDataPath)/BrainMultiModal/ProbandT2.dcm from MeVisLab demo data and add a SynchroView2D to your network.\nIn the end, add the Arithmetic2 module and connect them as seen below.\nExample Network Your SynchroView2D shows two images. On the left hand side, you can see the original image from your left LocalImage module. The right image shows the result of the arithmetic operation executed by the Arithmetic2 module on the two input images.\nSynchroView2D The SynchroView2D module automatically synchronizes the visible slice of both input images, you can see the same slice with and without applied filter.\nArithmetic operations Double-click the Arithmetic2 module to select different functions to be applied.\nArithmetic2 The selected function is applied automatically.\nSummary Arithmetic operations on two images can be applied on images by using Arithmetic* modules. The SynchroView2D module allows to scroll through slices synchronized on two images. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Image Processing","Arithmetic"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/image_processing/image_processing2/","title":"Example 2: Masking images","summary":"Example 2: Masking images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The background of medical images is black for most cases. In case an image is inverted or window/level values are adapted, these black pixels outside clinical relevant pixels might become very bright or even white.\nBeing in a dark room using a large screen, the user might be blended by these large white regions.\nImage masking is a very good way to select a defined region where image processing shall be applied.","content":"Example 2: Masking images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction The background of medical images is black for most cases. In case an image is inverted or window/level values are adapted, these black pixels outside clinical relevant pixels might become very bright or even white.\nBeing in a dark room using a large screen, the user might be blended by these large white regions.\nImage masking is a very good way to select a defined region where image processing shall be applied. A mask allows to define a region (the masked region) to allow image modifications whereas pixels outside the mask remain unchanged.\nSteps to do Develop your network Add a LocalImage and a SynchroView2D module to your network and connect the modules as seen below.\nExample Network Open the Automatic Panel of the SynchroView2D module via context menu and selecting [ Show Window \u0026rarr; Automatic Panel ]. Set the field synchLUTs to TRUE.\nSynchronize LUTs in SynchroView2D Double-click the SynchroView2D and change window/level values via right mouse button . You can see that the background of your images gets very bright and changes on the LUT are applied to all pixels of your input image - even on the background. Hovering your mouse over the image(s) shows the current gray value under your cursor in Hounsfield Unit (HU).\nWithout masking the image Hovering the mouse over black background pixels shows a value between 0 and about 60. This means we want to create a mask which only allows modifications on pixels having a grey value larger than 60.\nAdd a Mask and a Threshold module to your workspace and connect them as seen below.\nExample Network Changing the window/level values in your viewer still also changes background pixels. The Thereshold module still leaves the pixels as is because the threshold value is configured as larger than 0. Open the Automatic Panel of the modules Threshold and Mask via double-click and set the values as seen below.\nThreshold Mask Now all pixels having a HU value lower or equal 60 are set to 0, all others are set to 1. The resulting image from the Threshold module is a bit image which can now be used as a mask by the Mask module.\nOutput of the Threshold module The Mask module is configured to use the Masked Original image. Changing the window/level values in your images now, you can see that the background pixels are not affected anymore (at least as long as you do not reach a very large value).\nAfter masking the image Summary The module Threshold applies a relative or an absolute threshold to a voxel image. It can be defined what should be written to those voxels which pass or which fail the adjustable comparison. The module Mask masks the image of input one with the mask at input two. A mask can be used to filter pixels inside images \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Image Processing","Mask"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/image_processing/image_processing3/","title":"Example 3: Region Growing","summary":"Example 3: Region Growing \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction A very simple approach to segment parts of an image is the region growing method. A general explanation can be found here.\nIn this example, you will segment the brain of an image and show the segmentation results as an overlay on the original image.\nSteps to do Develop your network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.","content":"Example 3: Region Growing \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction A very simple approach to segment parts of an image is the region growing method. A general explanation can be found here.\nIn this example, you will segment the brain of an image and show the segmentation results as an overlay on the original image.\nSteps to do Develop your network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm. Add a View2D module and connect both as seen below.\nExample Network Add the RegionGrowing module Add the RegionGrowing module and connect the input with the LocalImage module. You will see a message results invalid. The reason is, that a region growing always needs a starting point for getting similar pixels. The output of the module does not show a result in Output Inspector.\nResults Invalid Add a SoView2DMarkerEditor to your network and connect it with your RegionGrowing and with the View2D. Clicking into your viewer now creates markers which can be used for the region growing.\nSoView2DMarkerEditor The region growing starts on manually clicking Update or automatically if Update Mode is set to Auto-Update. We recommend to set update mode to automatic update. Additionally you should set the Neighborhood Relation to 3D-6-Neighborhood (x,y,z), because then your segmentation will also affect the z-axis.\nSet Threshold Computation to Automatic and define Interval Size as 1.600 % for relative, automatic threshold generation.\nExtra Infos:\u0026nbsp; For more information, see MeVisLab Module Reference Auto-Update for RegionGrowing Clicking into your image in the View2D now already generates a mask containing your segmentation. As you did not connect the output of the RegionGrowing, you need to select the output of the module and use the Output Inspector to visualize your results.\nOutput Inspector Preview In order to visualize your segmentation mask as an overlay in the View2D, you need to add the SoView2DOverlay module. Connect it as seen below.\nSoView2DOverlay Your segmentation is now shown in the View2D. You can change the color and transparency of the overlay via SoView2DOverlay.\nClose gaps Scrolling through the slices, you will see that your segmentation is not closed. There are lots of gaps where the grey value of your image differs more than your threshold. You can simply add a CloseGap module to resolve this issue. Configure Filter Mode as Binary Dilatation, Border Handling as Pad Src Fill and set KernelZ to 3.\nThe difference before and after closing the gaps can be seen in the Output Inspector.\nOutput_Before Output_After You can play around with the different settings of the RegionGrowing and CloseGap modules to get a better result.\nVisualize 2D and 3D You can now also add a View3D to show your segmentation in 3D. Your final result should look similar to this.\nFinal Result Summary The module RegionGrowing allows a very simple segmentation of similar grey values. Gaps in a segmentation mask can be closed by using the CloseGap module. Segmentation results can be visualized in 2D and 3D. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Image Processing","Segmentation","Region Growing"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/image_processing/image_processing4/","title":"Example 4: Subtract 3D objects","summary":"Example 4: Subtract 3D objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we load an image and render it as WEMIsoSurface. Then we create a 3-dimensional SoSphere and subtract the sphere from the initial WEM.\nSteps to do Develop your network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm. Add a WEMIsoSurface, a SoWEMRenderer, a SoBackground and a SoExaminerViewer module and connect them as seen below.","content":"Example 4: Subtract 3D objects \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we load an image and render it as WEMIsoSurface. Then we create a 3-dimensional SoSphere and subtract the sphere from the initial WEM.\nSteps to do Develop your network Add a LocalImage module to your workspace and select load $(DemoDataPath)/BrainMultiModal/ProbandT1.dcm. Add a WEMIsoSurface, a SoWEMRenderer, a SoBackground and a SoExaminerViewer module and connect them as seen below. Make sure to configure the WEMIsoSurface to use a Iso Min. Value of 420 and a Voxel Sampling 1.\nExample Network The SoExaminerViewer now shows the head as a 3-dimensional rendering.\nSoExaminerViewer Add a 3D sphere to your scene We now want to add a 3-dimensional sphere to our scene. Add a SoMaterial and a SoSphere to your network, connect them to a SoSeparator and then to the SoExaminerViewer. Set your material to use a Diffuse Color red and adapt the size of the sphere to Radius 50.\nExample Network The SoExaminerViewer now shows the head and the red sphere inside.\nSoExaminerViewer Set location of your sphere In order to define the best possible location of the sphere, we additionally add a SoTranslation Module and connect it to the SoSeparator between the material and the sphere. Define a translation of x=0, y=20 and z=80.\nExample Network Subtract the sphere from the head We now want to subtract the sphere from the head to get a hole. Add another SoWEMRenderer, a WEMLevelSetBoolean and a SoWEMConvertInventor to the network and connect them to a SoSwitch as seen below. The SoSwitch also needs to be connected to the SoWEMRenderer of the head. Set your WEMLevelSetBoolean to use the Mode Difference.\nExample Network What happens in your network now?\nThe SoSphere is converted to a WEM. The WEMs from the head and from the sphere are subtracted by using a WEMLevelSetBoolean. The result of the subtraction is used for a SoWEMRenderer Both SoWEMRenderer (the head on the left side and the subtraction on the right side) are inputs for a SoSwitch. The SoSwitch toggles through its inputs and you can show the original WEM of the head or the subtraction. SoExaminerViewer_1 SoExaminerViewer_2 You can now toggle the hole to be shown or not, depending on your setting for the SoSwitch.\nSummary The module WEMLevelSetBoolean allows to subtract or add 3-dimensional WEM objects. The SoSwitch can toggle multiple inventor scenes as input \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Image Processing","3D","Subtraction"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/image_processing/image_processing5/","title":"Example 5: Clip Planes","summary":"Example 5: Clip Planes \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the SoGVRDrawOnPlane module to define the currently visible slice from a 2D view as a clip plane in 3D.\nSteps to do Develop your network First we need to develop the network to scroll through the slices. Add a LocalImage module to your workspace and select the file ProbandT1 from MeVisLab demo data.","content":"Example 5: Clip Planes \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the SoGVRDrawOnPlane module to define the currently visible slice from a 2D view as a clip plane in 3D.\nSteps to do Develop your network First we need to develop the network to scroll through the slices. Add a LocalImage module to your workspace and select the file ProbandT1 from MeVisLab demo data.\nAdd the modules OrthoReformat3, Switch, SoView2D, View2DExtensions and SoRenderArea and connect them as seen below.\nExample Network In previous tutorials, we already learned that it is possible to show 2D slices in a SoRenderArea. For scrolling through the slices, a View3DExtensions module is necessary. In this network, we also have a OrthoReformat3 module. It allows us to transform the input image (by rotating and/or flipping) into the three main views commonly used:\nAxial Coronal Sagittal The Switch takes multiple input images and you can toggle between them to show one of the orthogonal transformations to be used as output.\nThe SoRenderArea now shows the 2D images in a view defined by the Switch.\nView0 View1 View2 Current 2D slice in 3D We now want to visualize the slice visible in the 2D images as a 3D plane. Add a SoGVRDrawOnPlane and a SoExaminerViewer to your workspace and connect them. We should also add a SoBackground and a SoLUTEditor. The viewer remains empty because no source image is selected to display. Add a SoGVRVolumeRenderer and connect it to your viewer and the LocalImage.\nExample Network A 3-dimensional plane of the image is shown. Adapt the LUT as seen below.\nSoLUTEditor We now have a single slice of the image in 3D, but the slice is static and cannot be changed. In order to use the currently visible slice from the 2D viewer, we need to create a parameter connection from the SoView2D Position Slice as plane to the SoGVRDrawOnPlane Plane vector.\nSoView2D Position SoGVRDrawOnPlane Plane Now the plane representation of the visible slice is synchronized to the plane of the 3D view. Scrolling through your 2D slices changes the plane in 3D.\nVisible slice in 3D Current 2D slice as clip plane in 3D This slice shall now be used as a clip plane in 3D. In order to achieve this, you need another SoExaminerViewer and a SoClipPlane. Add them to your workspace and connect them as seen below. You can also use the same SoLUTEditor and SoBackground for the 3D view. Also use the same SoGVRVolumeRenderer, the 3D volume does not change.\nExample Network Now your 3D scene shows a 3-dimensional volume cut by a plane in the middle. Once again, the clipping is not the same slice as your 2D view shows.\nClip plane in 3D Again create a parameter connection from the SoView2D Position Slice as plane, but this time to the SoClipPlane.\nSoClipPlane Plane If you now open all 3 viewers and scroll through the slices in 2D, the 3D viewers are both synchronized with the current slice. You can even toggle the view in the Switch and the plane is adapted automatically.\nFinal 3 views Summary The module OthoReformat3 transforms input images to the three viewing directions: coronal, axial and sagittal A Switch can be used to toggle through multiple input images The SoGVRDrawOnPlane module renders a single slice as a 3-dimensional plane 3-dimensional clip planes on volumes can be created by using a SoClipPlane module \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Image Processing","3D","Clip Planes"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/","title":"Chapter V: Data Objects","summary":"Data Objects in MeVisLab MeVisLab provides pre-defined data objects, e. g.\nContour Segmented Objects (CSOs) which are three-dimensional objects encapsulating formerly defined contours within images. Surface Objects (Winged Edge Meshes or WEMs) represent the surface of geometrical figures and allow the user to manipulate them. Markers are used to mark specific locations or aspects of an image and allow to process those later on. Curves can print the results of a function as two-dimensional mathematical graphs into a diagram.","content":"Data Objects in MeVisLab MeVisLab provides pre-defined data objects, e. g.\nContour Segmented Objects (CSOs) which are three-dimensional objects encapsulating formerly defined contours within images. Surface Objects (Winged Edge Meshes or WEMs) represent the surface of geometrical figures and allow the user to manipulate them. Markers are used to mark specific locations or aspects of an image and allow to process those later on. Curves can print the results of a function as two-dimensional mathematical graphs into a diagram. Usage, advantages and disadvantages of each above mentioned data object type will be covered in the following specified chapters, where you will be building example networks for some of the most common use cases.\n","tags":["Beginner","Tutorial","Data Objects","2D","Contours","3D","Surfaces"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/contourobjects/","title":"Contour Objects (CSO)","summary":"Contour Segmented Objects (CSOs) in MeVisLab Introduction Structure of CSOs MeVisLab provides modules to create contours in images. 3D objects which encapsulate these contours are called Contour Segmented Objects (CSOs).\nIn the next image, you can see a rectangular shaped CSO. The pink circles you can see are called Seed Points.\nSeed Points define the shape of the CSO. In case of a rectangle, you need four Seed Points forming the corners, to define the whole rectangle.","content":"Contour Segmented Objects (CSOs) in MeVisLab Introduction Structure of CSOs MeVisLab provides modules to create contours in images. 3D objects which encapsulate these contours are called Contour Segmented Objects (CSOs).\nIn the next image, you can see a rectangular shaped CSO. The pink circles you can see are called Seed Points.\nSeed Points define the shape of the CSO. In case of a rectangle, you need four Seed Points forming the corners, to define the whole rectangle.\nThe points forming the blue lines are called Path Points.\nThe Path Points form the connection between the Seed Points whereby contour objects (CSOs) are generated. CSOs are often closed, but do not need to be.\nIn general, the Seed Points are created interactively using an editor module and the Path Points are generated automatically by interpolation or other algorithms.\nContour Segmented Object (CSO) CSO Editors As mentioned, when creating CSOs, you can do this interactively by using an editor.\nThe following images show editors available in MeVisLab for drawing CSOs:\nSoCSOPointEditor SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor SoCSODistancePolylineEditor SoCSOEllipseEditor SoCSORectangleEditor SoCSOSplineEditor SoCSOPolygonEditor SoCSOIsoEditor SoCSOLiveWireEditor Extra Infos:\u0026nbsp; The SoCSOIsoEditor and SoCSOLiveWireEditor are special, because they are using an algorithm to detect edges themselves.\nThe SoCSOIsoEditor generates iso-contours interactively. The SoCSOLiveWireEditor renders and semi-interactively generates CSOs based on the LiveWire algorithm. CSO Lists and CSO Groups All created CSOs are stored in CSO lists, which can be saved and loaded on demand. The lists can not only store the coordinates of the CSOs, but also additional information in the form of name-value pairs (using specialized modules or Python scripting).\nBasic CSO Network Each SoCSO*Editor requires a SoView2DCSOExtensibleEditor which manages attached CSO editors and renderers and offers an optional default renderer for all types of CSOs. In addition to that, the list of CSOs needs to be stored in a CSOManager.\nThe appearance of the CSO can be defined by using a SoCSOVisualizationSettings module.\nCSOs can also be grouped together. The following image shows two different CSO groups. Groups can be used to organize CSOs, in this case to distinguish the CSOs of the right and the left lung. Here you can find more information about CSO Groups.\nCSO Groups Extra Infos:\u0026nbsp; For more information, see CSO Overview ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/contours/contourexample1/","title":"Contour Example 1: Creation of Contours","summary":"Contour Example 1: Creation of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We like to start with the creation of CSOs. To create CSOs, you need a SoCSO*-Editor. There are several different editors, which can be used to create CSOs (see here). Some of them are introduced in this example.\nSteps to do Develop your network For this example, we need the following modules. Add the modules to your workspace, connect them as shown below and load the example image $(DemoDataPath)/BrainMultiModal/ProbandT1.","content":"Contour Example 1: Creation of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction We like to start with the creation of CSOs. To create CSOs, you need a SoCSO*-Editor. There are several different editors, which can be used to create CSOs (see here). Some of them are introduced in this example.\nSteps to do Develop your network For this example, we need the following modules. Add the modules to your workspace, connect them as shown below and load the example image $(DemoDataPath)/BrainMultiModal/ProbandT1.tif.\nData Objects Contours Example 1 Edit rectangular CSO Now, open the module View2D. Use your left mouse key , to draw a rectangle, which is your first CSO.\nRectangle Contour The involved modules have the following tasks:\nSoCSORectangleEditor: Enables the creation of the CSO and defines the shape of the CSOs\nSoView2DCSOExtensibleEditor: Manages attached CSO editors and the appearance of CSOs\nCSOManager: Creates a list of all drawn CSOs and offers the possibility to group CSOs\nIf you now open the panel of the CSOManager, you will find one CSO, the one we created before. If you like, you can name the CSO.\nCSO Manager Change properties of CSO Now, add the module SoCSOVisualizationSettings to your workspace and connect it as shown below.\nCSO Manager Open the module to change the visualization settings of your CSOs. In this case, we change the line style (to dashed lines) and the color (to be red). Tick the Auto apply box at the bottom or press Apply.\nVisualization Settings CSOs of different shapes Exchange the module SoCSORectangleEditor with another editor, for example the SoSCOPolygonEditor or SoCSOSplineEditor. Other editors allow to draw CSOs of other shapes. For polygon-shaped CSOs or CSOs consisting of splines, left-click on the image viewer to add new points to form the CSO. Double\u0026ndash;click to finish the CSO.\nSoSCOPolygonEditor SoCSOSplineEditor Exercises Create CSOs with green color and ellipsoid shapes.\nSummary CSOs can be created using a SoCSO-Editor CSOs of different shapes can be created A list of CSOs can be stored in the CSOManager Properties of CSOs can be changed using SoCSOVisualizationSettings \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/contours/contourexample2/","title":"Contour Example 2: Contour Interpolation","summary":"Contour Example 2: Creating Contours using Live Wire and Interpolation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to create CSOs using the Live Wire Algorithm, which allows semi-automatic CSO creation. The algorithm uses edge detection to support the user creating CSOs.\nWe also like to interpolate CSOs over slices. That means additional CSOs are generated between manual segmentations based on a linear interpolation.\nAs a last step, we will group together CSOs of the same anatomical unit.","content":"Contour Example 2: Creating Contours using Live Wire and Interpolation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to create CSOs using the Live Wire Algorithm, which allows semi-automatic CSO creation. The algorithm uses edge detection to support the user creating CSOs.\nWe also like to interpolate CSOs over slices. That means additional CSOs are generated between manual segmentations based on a linear interpolation.\nAs a last step, we will group together CSOs of the same anatomical unit.\nSteps to do Develop your network and create CSOs In order to do that, create the shown network. You can use the network from the previous example and exchange the SoCSO-Editor. In addition to that, load the example image $(DemoDataPath)/Thorax1_CT.small.tif . Now, create some CSOs on different, not consecutive slices. Afterwards, hover over the CSOManager and press the emerging plus-sign. This displays the amount of existing CSOs.\nData Objects Contours Example 2 Create CSO interpolations We like to generate interpolated contours for existing CSOs. In order to do that, add the module CSOSliceInterpolator to your workspace and connect it as shown.\nSlice Interpolation Open the panel of module CSOSliceInterpolator and change the Group Handling and the Mode as shown. If you now press Update interpolating CSOs are created.\nSlice Interpolation Settings You can see the interpolated CSOs are added to the CSOManager. If you now scroll through your slices, you can find the interpolated CSOs.\nYou can also take a look on all existing CSOs by inspecting the output of the CSOManager using the Output Inspector. Custom CSOs are displayed in white and interpolated CSOs are marked in yellow.\nInterpolated CSOs Group CSOs We like to segment both lobes of the lung. To distinguish the CSOs of both lungs, we like to group CSOs together, according to the lung, they belong to. First, we like to group together all CSOs belonging to the lung we already segmented. In order to do this, open the CSOManager. Create a new Group and label that Group. We chose the label Left Lung. Now, mark the created Group and all CSOs you want to include into that group and press Combine. If you click on the Group, all CSOs belonging to this Group are marked with a star.\nAttention:\u0026nbsp; Keep in mind, that the right lung might be displayed on the left side of the image and vice versa, depending on your view. Creating CSO Groups Creating CSO Groups As a next step, segment the right lung by creating new CSOs. Creation of further CSOs Create a new Group for all CSOs of the right lung. We labeled this Group Right Lung. Again, mark the group and the CSOs you like to combine and press Combine. Grouping remaining CSOs To visually distinguish the CSOs of both groups, change the color of each group under [ Group \u0026rarr; Visuals ]. We changed the color of the Left Lung to be green and of the Right Lung to be orange of path and seed points. In addition, we increased the Width of the path points. Interpolated CSOs As a last step, we need to disconnect the module SoCSOVisualizationSettings, as this module overwrites the visualization settings we enabled for each group in the CSOManager. Interpolated CSOs Summary SoCSOLiveWireEditor can be used to create CSOs semi-automatically CSO interpolations can be created using CSOSliceInterpolator CSOs can be grouped together using the CSOManager \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","Interpolation"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/contours/contourexample3/","title":"Contour Example 3: 2D and 3D Visualization of Contours","summary":"Contour Example 3: Overlay Creation and 3D Visualization of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we\u0026rsquo;d like to use the created CSOs to display an overlay. This allows us to mark one of two lungs. In addition to that, we will display the whole segmented lobe of the lung in a 3D image.\nSteps to do Develop your network Use the network from the contour example 2 and add the modules VoxelizeCSO, SoView2DOverlay and View2D to your workspace.","content":"Contour Example 3: Overlay Creation and 3D Visualization of Contours \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we\u0026rsquo;d like to use the created CSOs to display an overlay. This allows us to mark one of two lungs. In addition to that, we will display the whole segmented lobe of the lung in a 3D image.\nSteps to do Develop your network Use the network from the contour example 2 and add the modules VoxelizeCSO, SoView2DOverlay and View2D to your workspace. Connect the module as shown. The module VoxelizeCSO allows to convert CSOs into voxel images.\nData Objects Contours Example 3 Convert CSOs into voxel images Update the module VoxelizeCSOs to create overlays based on your CSOs. The result can be seen in View2D1.\nOverlay Next, we like to inspect the marked lobe of the lung. This means, we like to inspect the object, build out of CSOs. In order to do that, add the View3D module. The 3D version of the lung can be seen in the viewer.\nAdditional 3D Viewer Extracted Object Summary The module VoxelizeCSO converts CSOs to voxel images Create an overlay out of voxel images using SoView2DOverlay \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/contours/contourexample4/","title":"Contour Example 4: Annotation of Images","summary":"Contour Example 4: Annotation of Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to calculate the volume of our object, in this case the part of the lung we have segmented.\nSteps to do Develop your network and calculate the lung volume Add the module CalculateVolume and SoView2DAnnotation to your workspace and connect both modules as shown. Update the module CalculateVolume, which directly shows the volume of our object.","content":"Contour Example 4: Annotation of Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example we like to calculate the volume of our object, in this case the part of the lung we have segmented.\nSteps to do Develop your network and calculate the lung volume Add the module CalculateVolume and SoView2DAnnotation to your workspace and connect both modules as shown. Update the module CalculateVolume, which directly shows the volume of our object.\nData Objects Contours Example 4 Display the lung volume in the image We now like to display the volume in the image viewer. For this, open the panel of the modules CalculateVolume and SoView2DAnnotation. Open the tab Input in the panel of the module SoView2DAnnotation. Now construct a parameter connection between Total Volume calculated in the module CalculateVolume and the input00 of the module SoView2DAnnotation. This connection projects the Total Volume to the input of SoView2DAnnotation.\nDisplay Volume Go back to the tab General to select the Annotation Mode User. A separate tab exists for each annotation mode.\nAnnotate Image We select the tab User which we like to work on. You can see four fields, which display four areas of a viewer in which you can add information text to the image.\nAnnotate Image In this example we only like to add the volume, so delete all present input and replace that by the shown text. Now, you can see that the volume is displayed in the image viewer. If this is not the case, switch the annotations of the viewer by pressing the keyboard shortcut A .\nDisplay Volume in Image Summary CalculateVolume can calculate the volume of a voxel image SoView2DAnnotation enables to manually change the annotation mode of a viewer Annotations shown in a View2D can be customized by using a SoView2DAnnotation module \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","Annotations"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/contours/contourexample5/","title":"Contour Example 5: Contours and Ghosting","summary":"Contour Example 5: Visualizing Contours and Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to automatically create CSOs based on a predefined iso value.\nSteps to do Develop your network Add the following modules to your workspace and connect them as shown. Load the example image Bone.tiff.\nAutomatic creation of CSOs based on the iso value Now, open the panel of CSOIsoGenerator to set the Iso Value to 1200.","content":"Contour Example 5: Visualizing Contours and Images \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to automatically create CSOs based on a predefined iso value.\nSteps to do Develop your network Add the following modules to your workspace and connect them as shown. Load the example image Bone.tiff.\nAutomatic creation of CSOs based on the iso value Now, open the panel of CSOIsoGenerator to set the Iso Value to 1200. If you press Update in the panel, you can see the creation of CSOs on every slide, when opening the module View2D. In addition to that the number of CSOs is displayed in the CSOManager. The module CSOIsoGenerator generates iso-contours for each slice at a fixed iso value. This means that closed CSOs are formed based on the detection of the voxel value of 1200 on every slice.\nData Objects Contours Example 5 Ghosting Now, we like to make CSOs of previous and subsequent slices visible (Ghosting). In order to do that, open the panel of SoCSOVisualizationSettings and open the tab Misc. Increase the parameter Ghosting depth in voxel, which shows you the number of slices above and below the current slice, which CSOs are also seen in the viewer. The result can be seen in the viewer.\nGhosting Display created CSOs At last, we like to make all CSOs visible in a 3D viewer. To do that, add the modules SoCSO3DRenderer and SoExaminerViewer to your network and connect them as shown. In the viewer SoExaminerViewer you can see all CSOs together. In this case all scanned bones can be seen.\nCSOs in 3D View Summary CSOIsoGenerator enables automatic COS generation based on an iso value Ghosting allows to display CSOs of previous and following slices \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO"],"section":"tutorials"},{"date":"1710115200","url":"https://mevislab.github.io/examples/tutorials/dataobjects/contours/contourexample6/","title":"Contour Example 6: Adding Labels to Contours","summary":"Contour Example 6: Adding Labels to Contours Introduction In this example, we are adding a label to a contour. The label provides information about measurements and about the contour itself. The label remains connected to the contour and can be moved via mouse interactions.\nSteps to do Develop your network Add a LocalImage and a View2D module to your workspace and connect them as shown below. Load the file ProbandT1.dcm from MeVisLab demo data.","content":"Contour Example 6: Adding Labels to Contours Introduction In this example, we are adding a label to a contour. The label provides information about measurements and about the contour itself. The label remains connected to the contour and can be moved via mouse interactions.\nSteps to do Develop your network Add a LocalImage and a View2D module to your workspace and connect them as shown below. Load the file ProbandT1.dcm from MeVisLab demo data. In order to create contours (CSOs), we need a SoView2DCSOExtensibleEditor module. It manages attached CSO editors, renderers and offers an optional default renderer for all types of CSOs.\nThe first CSO we want to create is a distance line. Add a SoCSODistanceLineEditor to the SoView2DCSOExtensibleEditor. It renders and interactively generates CSOs that consist of a single line segment. The line segment can be rendered as an arrow; it can be used to measure distances.\nWe are going to add some more editors later. In order to have the same look and feel for all types of CSOs, add a SoCSOVisualizationSettings module as seen below. The module is used to adjust visual parameters like color and line style for CSOs. Also add a CSOManager module to organize CSOs and CSOGroups within a network.\nInitial Network We are now able to create lines in the View2D. You can also modify the lines by dragging the seed points to a different location.\nSoCSODistanceLineEditor The created lines do neither provide any details about the length of your measurement, nor a unique ID to identify it in case of multiple CSOs.\nAdd a CSOLabelRenderer module to your network and connect it to a SoGroup. Also connect your SoCSODistanceLineEditor to the SoGroup as seen below. The ID of each CSO appears next to your distance lines. Moving the ID also shows the name of the contour.\nCSOLabelRenderer We now want to customize the details to be shown for each distance line. Open the panel of the CSOLabelRenderer. You can see the two parameters labelString and labelName. The labelString is set to the ID of the CSO. The labelName is set to a static text and the label property of the CSO. The label can be defined in the module CSOManager. You can do this, but we are not defining a name for each contour in this example.\nEnter the following to the panel of the CSOLabelRenderer module: CSOLabelRenderer\nlabelString = f\u0026#34;Length {cso.getLength()}\u0026#34; labelName = f\u0026#34;Distance: {cso.getId()}\u0026#34; deviceOffsetX = 0 deviceOffsetY = 0 We are setting the labelName to a static text showing the type of the CSO and the unique ID of the contour. We also define the labelString to the static description of the measurement and the length parameter of the CSO.\nlabelString and labelName You can also round the length by using: CSOLabelRenderer\nlabelString = f\u0026#39;Length: {cso.getLength():.2f} mm\u0026#39; In order to see all possible parameters of a CSO, add a CSOInfo module to your network and connect it to the CSOManager. The geometric informations of the selected CSO from CSOManager can be seen there.\nCSOInfo For labels shown on greyscale images, it makes sense to add a shadow. Open the panel of the SoCSOVisualizationSettings module and on tab Misc check the option Should render shadow. This increases the readability of your labels.\nEx6_NoShadow Ex6_Shadow If you want to define your static text as a parameter in multiple labels, you can open the panel of the CSOLabelRenderer module and define text as User Data. The values can then be used in Python via userData.\nUser Data You can also add multiple CSO editors to see the different options. Add the SoCSORectangleEditor module to your workspace and connect it to the SoGroup module. As we now have two different editors, we need to tell the CSOLabelRenderer which CSO is to be rendered. Open the panel of the SoCSODistanceLineEditor. You can see the field Extension Id set to distanceLine. Open the panel of the SoCSORectangleEditor. You can see the field Extension Id set to rectangle.\nExtension ID We currently defined the labelName and labelString for the distance line. If we want to define different labels for different types of CSOs, we have to change the CSOLabelRenderer Python script. Open the panel of the CSOLabelRenderer and change the Python code to the following:\nCSOLabelRenderer\nif cso.getSubType() == \u0026#39;distanceLine\u0026#39;: labelString = f\u0026#39;{userData0} {cso.getLength():.2f} mm\u0026#39; labelName = userData1 labelName += str(cso.getId()) elif cso.getSubType() == \u0026#39;rectangle\u0026#39;: labelString = f\u0026#39;{userData0} {cso.getLength():.2f} mm\\n\u0026#39; labelString += f\u0026#39;{userData2} {cso.getArea():.2f} mm^2\u0026#39; labelName = userData3 labelName += str(cso.getId()) deviceOffsetX = 0 deviceOffsetY = 0 SoCSORectangleEditor If you now draw new CSOs, you will notice that you still always create distance lines. Open the panel of the SoView2DCSOExtensibleEditor. You can see that the Creator Extension Id is set to __default. By default, the first found eligible editor is used to create a new CSO. In our case this is the SoCSODistanceLineEditor.\nSoCSORectangleEditor Change Creator Extension Id to rectangle.\nSoCSORectangleEditor \u0026amp; SoView2DCSOExtensibleEditor Newly created CSOs are now rectangles. The label values are shown as defined in the CSOLabelRenderer and show the length and the area of the rectangle.\nLabeled Rectangle in View2D Extra Infos:\u0026nbsp; The Length attribute in the context of rectangles represents the perimeter of the rectangle, calculated as 2a + 2b, where a and b are the lengths of the two sides of the rectangle. You will find a lot more information in the CSOInfo module for your rectangles. The exact meaning of the values for each type of CSO is explained in the table below.\nCSOInfo Parameters and meanings for all CSO types CSO Editor PCA X Ext. PCA Y Ext. PCA Z Ext. Length Area SoCSOPointEditor n.a. n.a. n.a. n.a. n.a. SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor Length (in mm) SoCSODistancePolylineEditor Length of all lines (in mm) SoCSOEllipseEditor Perimeter (in mm) Area (in mm2) SoCSORectangleEditor Length of all sides (in mm) Area (in mm2) SoCSOSplineEditor SoCSOPolygonEditor Length of all lines (in mm) SoCSOIsoEditor SoCSOLiveWireEditor Summary Custom labels can be added to contours using the CSOLabelRenderer module. Python scripting is used within the CSOLabelRenderer module to customize label content based on CSO types. Visual properties can be adjusted within the CSOLabelRenderer and the SoCSOVisualizationSettings modules to improve label visibility and appearance. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Contours","CSO","Label"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/surfaceobjects/","title":"Surface Objects (WEM)","summary":"Surface Objects (WEMs) Introduction In MeVisLab it is possible to create, visualize, process and manipulate surface objects, also known as polygon meshes. Here, we call surface objects Winged Edge Mesh, in short WEM. In this chapter you will get an introduction into WEMs. In addition, you will find examples on how to work with WEMs. For more information on WEMs take a look at the MeVislab Toolbox Reference . If you like to know which WEM formats can be imported into MeVisLab, take a look at the assimp documentation here.","content":"Surface Objects (WEMs) Introduction In MeVisLab it is possible to create, visualize, process and manipulate surface objects, also known as polygon meshes. Here, we call surface objects Winged Edge Mesh, in short WEM. In this chapter you will get an introduction into WEMs. In addition, you will find examples on how to work with WEMs. For more information on WEMs take a look at the MeVislab Toolbox Reference . If you like to know which WEM formats can be imported into MeVisLab, take a look at the assimp documentation here.\nWEM explained with MeVisLab To explain WEMs in MeVisLab we will build a network, which shows the structure and the characteristics of WEMs. We will start the example by generating a WEM forming a cube. With this, we will explain structures of WEMs called Edges, Nodes, Surfaces, and Normals.\nInitialize a WEM Add the module WEMInitialize to your workspace, open its panel and select a Cube. In general, a WEM is made up of surfaces. Here all surfaces are squares. In MeVisLab it is common to build WEMs out of triangles.\nWEM initializing Rendering of WEMs For rendering WEMs, you can use the module SoWEMRenderer in combination with the viewer SoExaminerViewer. Add both modules to your network and connect them as shown. A background is always a nice feature to have. WEM rendering Geometry of WEMs The geometry of WEMs is given by different structures. Using specialized WEM-Renderer modules, all structures can be visualized.\nEdges Add and connect the module SoWEMRendererEdges to your workspace to enable the rendering of WEM Edges. Here, we manipulated the line thickness, to make the lines of the edges thicker. WEM Edges Nodes Nodes mark the corner points of each surface. Therefore, nodes define the geometric properties of every WEM. To visualize the nodes, add and connect the module SoWEMRendererNodes as shown. Per default, the nodes are visualized with an offset to the position they are located in. We reduced the offset to be zero, increased the point size and changed the color. WEM Nodes Faces Between the nodes and alongside the edges surfaces are created. The rendering of these surfaces can be enabled and disabled using the panel of SoWEMRenderer. WEM Faces Normals Normals display the orthogonal vector either to the faces (Face Normals) or to the nodes (Nodes Normals). With the help of the module SoWEMRendererNormals these structures can be visualized.\nWEM normal editor WEMNodeNormals WEMFaceNormals WEMs in MeVisLab In MeVisLab WEMs can consist of triangles, squares or other polygons. Most common in MeVisLab are surfaces composed of triangles, as shown in the following example. With the help of the module WEMLoad existing WEMs can be loaded into the network.\nWEMTriangles WEMNetwork WEMSurface Summary WEMs are polygon meshes, in most cases composed of triangles WEM\u0026rsquo;s geometry is determined by nodes, edges, faces and normals, which can be visualized using renderer modules ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/surfaces/surfaceexample1/","title":"Surface Example 1: Creation of WEMs","summary":"Surface Example 1: Create Winged Edge Mesh out of voxel images and CSOs \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you will learn how to create a Winged Edge Mesh (WEM). There are several approaches on creating WEMs, a few of them are shown in this example. Instead of creating WEMs, they can also be imported, see chapter Surface Objects (WEM).\nSteps to do From image to surface: Generating WEMs out of voxel images At first, we will create a WEM out of a voxel image using the module WEMIsoSurface.","content":"Surface Example 1: Create Winged Edge Mesh out of voxel images and CSOs \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example you will learn how to create a Winged Edge Mesh (WEM). There are several approaches on creating WEMs, a few of them are shown in this example. Instead of creating WEMs, they can also be imported, see chapter Surface Objects (WEM).\nSteps to do From image to surface: Generating WEMs out of voxel images At first, we will create a WEM out of a voxel image using the module WEMIsoSurface. Add and connect the shown modules. Load the image $(DemoDataPath)/Bone.tiff and set the Iso Min. Value in the panel of WEMIsoSurface to 1200. Tick the box Use image max. value. The module WEMIsoSurface creates surface objects out of all voxels with an Iso value equal or above 1200 (and smaller than the image max value). The module SoWEMRenderer can now be used to generate an Open Inventor scene, which can be displayed by the module SoExaminerViewer.\nWEM From surface to image: Generating voxel images out of WEM It is not only possible to create WEMs out of voxel images. You can also transform WEMs into voxel images: Add and connect the modules VoxelizeWEM and View2D as shown and press the Update button of the module VoxelizeWEM.\nWEM From Contour to Surface: Generating WEMs out of CSOs Now we like to create WEMs out of CSOs. To create CSOs load the network from Contour Example 2 and create some CSOs.\nNext, add and connect the module CSOToSurface to convert CSOs into a surface object. To visualize the created WEM, add and connect the modules SoWEMRenderer and SoExaminerViewer.\nWEM It is also possible to display the WEM in 2D in addition to the original image. In order to do that, add and connect the modules SoRenderSurfaceIntersection and SoView2DScene. The module SoRenderSurfaceIntersection allows to display the voxel image and the created WEM in one viewer using the same coordinates. In its panel, you can choose the color used for visualizing the WEM. The module SoView2DScene renders an Open Inventor scene graph into 2D slices.\nWEM If you like to transform WEMs back into CSOs, take a look at the module WEMClipPlaneToCSO.\nSummary Voxel images can be transformed into WEMs using WEMIsoSurface WEMs can be transformed into voxel images using VoxelizeWEM CSOs can be transformed into WEMS using CSOToSurface WEMs can be transformed into voxel images using WEMClipPlaneToCSO Warning:\u0026nbsp; Whenever converting voxel data to pixel data, keep the so called Partial Volume Effect in mind, see wikipedia for details. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/surfaces/surfaceexample2/","title":"Surface Example 2: Processing and Modification of WEM","summary":"Surface Example 2: Processing and Modification of WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to modify and process WEMs.\nSteps to do Develop your network Modification of WEMs Use the module WEMLoad to load the file venus.off. Then add and connect the shown modules. We like to display the WEM venus two times, one time this WEM is modified. You can use the module WEMModify to apply modifications.","content":"Surface Example 2: Processing and Modification of WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to modify and process WEMs.\nSteps to do Develop your network Modification of WEMs Use the module WEMLoad to load the file venus.off. Then add and connect the shown modules. We like to display the WEM venus two times, one time this WEM is modified. You can use the module WEMModify to apply modifications. In its panel, change the scale and the size of the WEM. Now you see two times the venus next to each other.\nWEMModify Smoothing of WEMs It is possible to smooth the WEM using the module WEMSmooth. Add this module to your network as shown. You can see the difference of the smoothed and the unsmoothed WEM in your viewer. There are more modules, which can modify WEMs, for example WEMExtrude. You can find them via search or in [ Modules \u0026rarr; Visualization \u0026rarr; Surface Meshes (WEM) ].\nWEMSmooth Calculate distance between WEMs Now, we like to calculate the distance between our two WEMs. In order to do this, add and connect the module WEMSurfaceDistance as shown.\nCalculate surface distance Annotations in 3D As a last step, we like to draw the calculated distances as annotations into the image. This is a little bit tricky as we need the module SoView2DAnnotation to create annotations in a 3D viewer. Add and connect the following modules as shown. What is done here? We use the module SoView2D to display a 2D image in the SoExaminerViewer, in addition to the WEMs we already see in the viewer. We do not see an additional image in the viewer, as we chose no proper input image to the module SoView2D using the module ConstantImage with value 0. Thus, we pretend to have a 2D image, which we can annotate. Now, we use the module SoView2DAnnotation to annotate the pretended-2D-image, displayed in the viewer of SoExaminerViewer. We already used the module SoView2DAnnotation in Contour Example 4.\nIn the SoView2D module, you need to uncheck the option Draw image data.\nAnnotation modules Now, change the Annotation Mode to User, as we like to insert custom annotations. In addition, disable to Show vertical ruler.\nSelect annotation mode Next, open the tab Input and draw parameter connections from the results of the distance calculations, which can be found in the panel of WEMSufaceDistance, to the input fields in the panel of SoView2DAnnotation.\nDefine annotation parameters You can design the annotation overlay as you like in the tab User. We decided to only display the minimum and maximum distance between both WEMs.\nAnnotation design As we use a 2D annotation module to annotate a 3D viewer, it is important to get rid of all 2D orientation annotations, which you can edit in the tab Orientation.\nDisable 2D orientation annotations Now, you can see the result in the viewer. If the annotations are not visible, press a a few times to change the annotation mode. Display surface distance in viewer Summary There are several modules to modify and process WEMs, e.g. WEMModify, WEMSmooth. To calculate the minimal and maximal surface distance between two WEMs, use the module WEMSurfaceDistance. To create annotations in 3D, the module SoView2DAnnotation can be used, when adapted to be used in combination with a 3D viewer. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1679356800","url":"https://mevislab.github.io/examples/tutorials/dataobjects/surfaces/surfaceexample3/","title":"Surface Example 3: Interactions with WEM","summary":"Surface Example 3: Interactions with WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In these examples, we are showing 2 different possibilities to interact with a WEM:\nScale, rotate and move a WEM in a scene Modify a WEM in a scene Scale, rotate and move a WEM in a scene We are using a SoTransformerDragger module to apply transformations on a 3D WEM object via mouse interactions.\nAdd a SoCube and a SoBackground module and connect both to a SoExaminerViewer.","content":"Surface Example 3: Interactions with WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In these examples, we are showing 2 different possibilities to interact with a WEM:\nScale, rotate and move a WEM in a scene Modify a WEM in a scene Scale, rotate and move a WEM in a scene We are using a SoTransformerDragger module to apply transformations on a 3D WEM object via mouse interactions.\nAdd a SoCube and a SoBackground module and connect both to a SoExaminerViewer. For a better understanding, you should also add a SoCoordinateSystem module and connect it to the viewer. Change the User Transform Mode to User Transform Instead Of Input and set User Scale to 2 for x, y and z.\nInitial Network The SoExaminerViewer shows your cube and the world coordinate system. You can interact with the camera (rotate, zoom and pan), the cube itself does not change and remains in the center of the coordinate system.\nInitial Cube Scaling, rotation and translations on the cube itself can be done by using the module SoTransformerDragger. Additionally add a SoTransform module to your network. Add all modules but the SoCoordinateSystem to a SoSeparator so that transformations are not applied to the coordinate system.\nSoTransformerDragger and SoTransform Draw parameter connections from Translation, Scale Factor and Rotation of the SoTransformerDragger to the same fields of the SoTransform module.\nOpening your SoExaminerViewer now allows you to use handles of the SoTransformerDragger to scale, rotate and move the cube. You can additionally interact with the camera as already done before.\nInfo:\u0026nbsp; You need to change the active tool on the right side of the SoExaminerViewer. Use the Pick Mode for applying transformations and the View Mode for using the camera. Moved, Rotated and Scaled Cube You can also try the other So*Dragger modules in MeVisLab for variations of the SoTransformerDragger.\n\u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Interactively modify WEMs We are using the WEMBulgeEditor module to interactively modify the WEM via mouse interactions.\nAdd a WEMInitialize, a SoWEMRenderer and a SoBackground module to your workspace and connect them to a SoExaminerViewer as seen below. Select model Icosahedron for the WEMInitialize module.\nWEMLoad and SoWEMRenderer You can see the WEM and interact with it in the viewer (zoom, move and rotate). In case the object does not rotate around its center, trigger the field viewAll of the SoExaminerViewer.\nAdd a WEMBulgeEditor and a SoWEMBulgeEditor to your network and connect them as seen below.\nWEMBulgeEditor and SoWEMBulgeEditor Opening the viewer, you can still not edit the object.\nWe need a lookup table (LUT) to interact with the WEM. Add a WEMGenerateStatistics between the WEMInitialize and the WEMBulgeEditor. The module WEMGenerateStatistics generates node, edge, and face statistics of a WEM and stores the information in the WEM\u0026rsquo;s Primitive Value Lists.\nInfo:\u0026nbsp; More information about Primitive Value Lists (PVL) can be found in Surface Example 5. Check New node PVL and set New PVL Name to myPVL.\nWEMGenerateStatistics In the WEMBulgeEditor, set PVL Used as LUT Values to previously generated myPVL.\nWEMBulgeEditor PVL Add a SoLUTEditor and connect it to SoWEMRenderer. You also have to connect the WEMGenerateStatistics to the SoWEMRenderer. Set SoWEMRenderer Color Mode to Lut Values and select PVL Used as LUT Values to myPVL.\nFinal Network Open the panel of the SoLUTEditor. Configure New Range Min as -1 and New Range Max as 1 in Range tab. Apply the new range. Define the LUT as seen below in Editor tab.\nSoLUTEditor Now your Primitive Value List is used to colorize the affected region for your tansformations. You can see the region by the color on hovering the mouse over the WEM.\nAffected region colored The size of the region can be changed via ALT and mouse wheel . Make sure that the Influence Radius in WEMBulgeEditor is larger than 0.\nInfo:\u0026nbsp; You need to change the active tool on the right side of the SoExaminerViewer. Use the Pick Mode for applying transformations and the View Mode for using the camera. Modify WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. A much more complex example using medical images and allowing to modify in 3D and on 2D slices can be seen by opening the example network of the WEMBulgeEditor.\nWEMBulgeEditor Example Network Info:\u0026nbsp; For other interaction possibilities, you can play around with the example networks of the modules SoCSODrawOnSurface, SoVolumeCutting and WEMExtrude. Summary MeVisLab provides multiple options to interact with 3D surfaces. Modules of the So*Dragger family allow to scale, rotate and translate a WEM. You can always use a SoCoordinateSystem to see the current world coordinates. The WEMBulgeEditor allows you to interactively modify a WEM via mouse. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/surfaces/surfaceexample4/","title":"Surface Example 4: Interactively moving WEM","summary":"Surface Example 4: Interactively moving WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to interactively move WEMs using SoDragger modules insight a viewer.\nDevelop your network Interactively translating objects in 3D using SoDragger modules Add and connect the following modules as shown. In the panel of the module WEMInitialize select the Model Octasphere. After that, open the viewer SoExaminerViewer and make sure to select the Interaction Mode.","content":"Surface Example 4: Interactively moving WEM \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we like to interactively move WEMs using SoDragger modules insight a viewer.\nDevelop your network Interactively translating objects in 3D using SoDragger modules Add and connect the following modules as shown. In the panel of the module WEMInitialize select the Model Octasphere. After that, open the viewer SoExaminerViewer and make sure to select the Interaction Mode. Now, you are able to click on the presented Octaspehere and move it alongside one axis. The following modules are involved in the interactions:\nSoMITranslate1Dragger: This module allows interactive translation of the object alongside one axis. You can select the axis for translation in the panel of the module. SoMIDraggerContainer: This module is responsible for actually changing the translation values of the object. Interactive dragging of objects Interactively translating a WEM alongside three axis We like to be able to interactively move a WEM alongside all three axis. In MeVisLab exists the module SoMITranslate2Dragger which allows translations alongside two axis, but there is no module which allows object translation in all three directions. Therefore, we will create a network, which solves this task. The next steps will show you, how you create three planes intersecting the objects. Dragging one plane, will drag the object alongside one axis. In addition, these planes will only be visible when hovering over them.\nCreation of planes intersecting an object We start creating a plane, which will allow dragging in x direction. In order to do that, modify your network as shown: Add the modules WEMModify, and SoBackground and connect the module SoCube to the dragger modules. You can select the translation direction in the panel of SoMITranslate1Dragger.\nInteractive dragging of objects We will modify the cube to be able to use it as a dragger plane. In order to do this, open the panel of SoCube and reduce the Width to be 0. This sets a plane in Y and Z direction.\nInteractive dragging of objects We like to move the object, when dragging the plane. Thus, we need to modify the translation of our object, when moving the plane. Open the panels of the modules WEMModify and SoMIDraggerContainer and draw a parameter connection from one Translation vector to the other.\nInteractive dragging of objects As a next step, we like to adapt the size of the plane, to the size of the object we have. Add the modules WEMInfo and DecomposeVector3 to your workspace and open their panels. The module WEMInfo presents information about the given WEM, for example its position and size. The module DecomposeVector3 splits a 3D vector into its components. Now, draw a parameter connection from Size of WEMInfo to the vector in DecomposeVector3. As a next step, open the panel of SoCube and draw parameter connections from the fields Y and Z of DecomposeVector3 to Height and Depth of SoCube. Now, the size of the plane adapts to the size of the object.\nInteractive dragging of objects The result can be seen in the next image. You can now select the plane in the Interaction Mode of the module SoExaminerViewer and move the plane together with the object alongside the x-axis.\nInteractive dragging of objects Modifying the appearance of the plane For changing the visualization of the dragger plane add the modules SoGroup, SoSwitch and SoMaterial to your network and connect them as shown. In addition, group together all the modules, which are responsible for the translation in X direction.\nInteractive dragging of objects We like to switch the visualization of the plane, in dependence of the mouse position in the viewer. In other words, when hovering over the plane, the plane should be visible, when the mouse is in another position and the possibility to drag the object is not given, the plane should be invisible. We use the module SoMaterial to edit the appearance of the plane. Open the panel of the module SoMITranslate1Dragger. The box of the field Highlighted is ticked, when the mouse hovers over the plane. Thus, we can use the field\u0026rsquo;s status to switch between different presentations of the plane. In order to do this, open the panel of SoSwitch and draw a parameter connection from Highlighted of SoMITranslate1Dragger to Which Child of SoSwitch.\nInteractive dragging of objects Open the panels of the modules SoMaterial. Change the Transparency of the first SoMaterial module to make the plane invisible, when not hovering over the plane. Furthermore, we changed the Diffuse Color of the module SoMaterial1 to red, so that the plane appears in red, when hovering over it.\nInteractive dragging of objects When hovering over the plane, the plane becomes visible and the option to move the object alongside the x-axis is given. When you do not hover over the plane, the plane is invisible.\nInteractive dragging of objects Interactive object translation in three dimensions We do not only want to move the object in one direction, we like to be able to do interactive object translations in all three dimensions. For this, copy the modules responsible for the translation in one direction and change the properties to enable translations in other directions.\nWe need to change the size of SoCube1 and SoCube2 to form planes, which cover surfaces in X and Z as well as X and Y directions. To do that, draw the respective parameter connections from DecomposeVector3 to the fields of the modules SoCube. In addition, we need to adapt the field Direction in the panels of the modules SoMITranslate1Dragger.\nInteractive dragging of objects Change width, height and depth of the 3 cubes so that each of them represents one plane. The values need to be set to (0, 2, 2), (2, 0, 2) and (2, 2, 0).\nAs a next step, we like to make sure, that all planes always intersect the object, even though the object is moved. To to this, we need to synchronize the field Translation of all SoMIDraggerContainer modules and the module WEMModify. Draw parameter connections from one Translation field to the next, as shown below.\nInteractive dragging of objects We like to close the loop, so that a change in one field Translation causes a change in all the other Translation fields. To do this, we need to include the module SyncVector. The module SyncVector avoids an infinite processing loop causing a permanent update of all fields Translation.\nAdd the module SyncVector to your workspace and open its panel. Draw a parameter connection from the field Translation of the module SoMIDraggerContainer2 to Vector1 of SyncVector. The field Vector1 is automatically synchronized to the field Vector2. Now, connect the field Vector2 to the field Translate of the module WEMModify. Your synchronization network is now established.\nInteractive dragging of objects To enable transformations in all directions, we need to connect the modules SoMIDraggerContainer to the viewer. First, connect the modules to SoGroup, after that connect SoGroup to SoExaminerViewr.\nInteractive dragging of objects As a next step, we like to enlarge the planes, to make them exceed the object. For that, add the module CalculateVectorFromVectors to your network. Open its panel and connect the field Size of WEMInfo to Vector 1. We like to enlarge the size by one, so we add the vector (1,1,1), by editing the field Vector 2. Now, connect the Result to the field V of the module DecomposeVector3.\nInteractive dragging of objects At last, we can condense all the modules enabling the transformation into one local macro module. For that, group all the modules together and convert the group into a macro module as shown in Chapter I: Basic Mechanisms.\nInteractive dragging of objects The result can be seen in the next image. This module can now be used for interactive 3D transformations for all kind of WEMs.\nInteractive dragging of objects Summary A family of SoDragger modules is available, which can be used to interactively modify Open Inventor objects. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/surfaces/surfaceexample5/","title":"Surface Example 5: WEM - Primitive Value Lists","summary":"Surface Example 5: WEM - Primitive Value Lists \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction WEMs do not only contain the coordinates of nodes and surfaces, they can also contain additional information. These information are stored in so called Primitive Value Lists (PVLs). Every node, every surface and every edge can contains such a list. In these lists, you can for example store the color of the node or specific patient information.","content":"Surface Example 5: WEM - Primitive Value Lists \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction WEMs do not only contain the coordinates of nodes and surfaces, they can also contain additional information. These information are stored in so called Primitive Value Lists (PVLs). Every node, every surface and every edge can contains such a list. In these lists, you can for example store the color of the node or specific patient information. These information can be used for visualization or for further statistical analysis.\nIn this example, we like to use PVLs to color-code and visualize the distance between two WEMs.\nSteps to do Develop your network We start our network by initializing two WEMs using WEMInitialize. We chose an Octasphere and a resized Cube. Use the modules SoWEMRenderer, SoExaminerViewer and SoBackground to visualize the WEMs.\nWEMInitialize Subdividing WEM edges As a next step, add and connect two modules WEMSubdivide to further divide edges and surfaces. With this step we increase the node density to have an accurate distance measurement.\nWEMSubdivide The difference when selecting different maximum edge lengths can be seen in the following images.\nEdgeLength1 EdgeLength01 Distances between WEMs are stored in PVLs Now, add the modules WEMSurfaceDistance and WEMInfo to your workspace and connect them as shown. WEMSurfaceDistance calculates the minimum distance between the nodes of both WEM. The distances are stored in the nodes\u0026rsquo; PVLs as LUT values.\nDistances between surfaces Open the panels of the modules WEMSurfaceDistance and WEMInfo. In the panel of WEMInfo select the tab Statistics. You can see, the statistics of the stored PVLs. The Minimum Value and the Maximum Value are similar to the calculated Min Dist. and Max. Dist. of WEMSurfaceDistance.\nWEM information Color-coding the distance between WEMs What can we do with these information? We can use the calculated distances, stored in LUT values, to color-code the distance between the WEMs. For this, add and connect the module SoLUTEditor. Each LUT value from the PVLs will in the next step be translated into a color. But first, open the panel of SoWEMRenderer to select the Color Mode LUT Values. Now, the module SoLUTEditor defines the coloring of the WEM.\nSoWEMRenderer To translate the LUT values from the PVLs into color, open the panel of SoLUTEditor and select the tab Range. We need to define the value range, we like to work with. As the distance and thus the PVL-value is expected to be 0 when the surfaces of both WEMs meet, we set the New Range Min to 0. As the size of the WEMs does not exceed 3, we set the New Range Max to 3. After that, press Apply new Range.\nSoLUTEditor Our goal is to colorize faces of the Octasphere in red, if they are close to or even intersect the cubic WEM. And we like to colorize faces of the Octasphere in green, if these faces are far away from the cubic WEM.\nOpen the tab Editor of the panel of SoLUTEditor. This tab allows to interactively select a color for each PVL-value. Select the color point on the left side. Its Position value is supposed to be 0, so we like to select the Color red in order to color-code small distances between the WEMs in red. In addition to that, increase the Opacity of this color point. Next, select the right color point. Its Position is supposed to be 3 and thus equals the value of the field New Range Max. As these color points colorize large distances between WEMs, select the Color green. You can add new color points by clicking on the colorized bar in the panel. Select for example the Color yellow for a color point in the middle. Select and shift the color points to get the desired visualization.\nChanging the LUT Add the module WEMModify to your workspace and connect the module as shown. If you now shift the WEM using WEMModify, you can see that the colorization adapts.\nWEMModify Interactive shift of WEMs As a next step, we like to implement the interactive shift of the WEM. Add the modules SoTranslateDragger1 and SyncVector. Connect all translation vectors: Draw connections from the field Translate of SoTranslateDragger1 to Vector1 of SyncVector, from Vector2 of SyncVector to Translate of WEMModify, and at last from Translate of WEMModify to Translate of SoTranslateDragger1.\nYou can now interactively drag the WEM insight the viewer.\nDragging the WEM At last, exchange the WEMInitialize module showing the cube with WEMLoad and load venus.off. You can decrease the Face Alpha in the panel of SoWEMRenderer1 to make that WEM transparent.\nWEM transparency The result can be seen in the next image.\nYour final result Summary Additional information of WEMs can be stored in Primitive Value Lists (PVL), attached to nodes, edges or faces. The module WEMSurfaceDistance stores the minimum distance between nodes of different WEMs in PVLs, as LUT values. PVLs containing LUT values can be used to color-code additional information on the WEM surface. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Data Objects","3D","Surfaces","Meshes","WEM","PVM","Primitive Value Lists","LUT"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/testing/","title":"Chapter VI: Testing","summary":"MeVisLab Tutorial Chapter VI Testing, Profiling and Debugging in MeVisLab The MeVisLab Integrated Development Environment (IDE) provides tools to write automated tests in Python, profile your network performance and to debug your Python code. All of these funtionalities will be addressed in this chapter.\nTesting The MeVisLab TestCenter is the starting point of your tests. Select [ File \u0026rarr; Run TestCaseManager ] to open the user interface of the TestCaseManager.","content":"MeVisLab Tutorial Chapter VI Testing, Profiling and Debugging in MeVisLab The MeVisLab Integrated Development Environment (IDE) provides tools to write automated tests in Python, profile your network performance and to debug your Python code. All of these funtionalities will be addressed in this chapter.\nTesting The MeVisLab TestCenter is the starting point of your tests. Select [ File \u0026rarr; Run TestCaseManager ] to open the user interface of the TestCaseManager.\nMeVisLab TestCaseManager Test Selection The Test Selection allows you to define a selection of test cases to be executed. The list can be configured by defining a filter, manually selecting the packages (see Example 2.1: Package Creation) to be scanned for test cases. All test cases found in the selected packages are shown.\nOn the right side of the Test Selection tab, you can see a list of functions in the test case. Each list entry is related to a Python function. You can select the functions to be executed. If your test case contains a network, you can open the *.mlab file or edit the Python file in MATE.\nTest Reports The results of your tests are shown as a report after execution.\nTest Creation You can create your own test cases here. A package is necessary to store your network and Python file.\nConfiguration Here you can configure details of your tests and reports. The filepath to the directory of your MeVisLab installation is configured automatically.\nCheck:\u0026nbsp; If you have multiple versions installed, make sure to check and, if needed, alter the automatically configured filepath. Profiling Profiling allows you to get detailed information on the behavior of your modules and networks. You can add the profiling view via [ View \u0026rarr; Views \u0026rarr; Profiling ]. The Profiling will be displayed in the Views area of the MeVisLab IDE.\nMeVisLab Profiling With enabled profiling, your currently opened network will be inspected and the CPU and memory usage and many more details of each module and function are logged.\nDebugging Debugging can be enabled whenever the integrated text editor MATE is opened. Having a Python file opened, you can enable debugging via [ Debug \u0026rarr; Enable Debugging ]. You can define break points in Python, add variables to your watchlist and walk through your break points just like in other editors and debuggers.\nMeVisLab Debugging ","tags":["Beginner","Tutorial","Testing","Python","Automated Tests","Profiling","Debugging"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/markerobjects/","title":"Marker Objects","summary":"Markers in MeVisLab In MeVisLab you can equip images and other data objects with markers. In this example you will see how to create, process and use markers.\nCreation and Rendering To create markers, you can use a marker editor, for example the SoView2DMarkerEditor. Connect this editor to a viewer as shown below. Now you can interactively create new markers. Connect the module XMarkerListContainer to your marker editor to store markers in a list.","content":"Markers in MeVisLab In MeVisLab you can equip images and other data objects with markers. In this example you will see how to create, process and use markers.\nCreation and Rendering To create markers, you can use a marker editor, for example the SoView2DMarkerEditor. Connect this editor to a viewer as shown below. Now you can interactively create new markers. Connect the module XMarkerListContainer to your marker editor to store markers in a list.\nCreate Markers Using the module StylePalette you can define a style for your markers. In order to set different styles for different markers, change the field Color Mode in the Panel of SoView2DMarkerEditor to Index.\nStyle of Markers With the help of the module So3DMarkerRenderer markers of an XMarkerList can be rendered.\nRendering of Markers Working with Markers Info:\u0026nbsp; It is possible to convert other data objects into markers and also to convert markers into other data objects. It is, for example, possible to set markers by using the MaskToMarkers-module and later on generate a surface object from a list of markers, using the MaskToSurface-module. Marker conversion can also be done by various other modules, listed in [/Modules/Geometry/Markers]. Learn how to convert markers by building the following network. Press the Reload buttons of the modules MaskToMarkers and MarkersToSurface to enable the conversion. Now you can see both the markers and the created surface in the module SoExaminerViewer. Use the toggle options of SoToggle and SoWEMRenderer to enable or disable the visualization of markers and surface.\nInfo:\u0026nbsp; Make sure to set Lower Threshold of the MaskToMarkers module to 1000 so that the 3D object is rendered correctly. Convert Markers Exercise Get the HU value of the image at your markers location.\nSummary Markers are single point objects located at a defined location in your image. Markers can be converted to be rendered in 3D. ","tags":["Beginner","Tutorial","Data Objects","2D","Marker"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/markers/markerexample1/","title":"Example 1: Distance between Markers","summary":"Example 1: Calculating the distance between markers \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will measure the distance between one position in an image to a list of markers.\nSteps to do Develop your network Add the following modules and connect them as shown.\nWe changed the names of the modules SoView2DMarkerEditor and XMarkerLIstContainer, to distinguish these modules from two similar modules we will add later on.","content":"Example 1: Calculating the distance between markers \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will measure the distance between one position in an image to a list of markers.\nSteps to do Develop your network Add the following modules and connect them as shown.\nWe changed the names of the modules SoView2DMarkerEditor and XMarkerLIstContainer, to distinguish these modules from two similar modules we will add later on. Open the panel of SoView2DMarkerEditor and select the tab Drawing. Now chose the Color red.\nMarker Color As a next step, add two more modules: SoView2DMarkerEditor and XMarkerLIstContainer.\nChange their names and the marker color to green and connect them as shown. We also like to change the mouse button you need to press, in order to create a marker. This allows to place both types of markers, the red ones and the green ones. In order to do this, open the panel of GreenMarker. Under Buttons you can adjust, which button needs to be pressed in order to place a marker. Select the Button2 (the middle button of your mouse ) instead of Button1 (the left mouse button ).\nIn addition to that, we like to allow only one green marker to be present. If we place a new marker, the old marker should vanish. For this, select the Max Size to be one and select Overflow Mode: Remove All.\nMarker Editor Settings Create markers of different type Now we can place as many red markers as we like, using the left mouse button and one green marker using the middle mouse button .\nTwo Types of Markers Calculate the distance between markers We like to calculate the minimum and maximum distance of the green marker to all the red markers. In order to do this, add the module DistanceFromXMarkerList and connect it to RedMarkerList. Open the panels of DistanceFromXMarkerList and GreenMarkerList. Now, draw a parameter connection from the coordinates of the green marker, which are stored in the field Current Item -\u0026gt; Position in the panel of GreenMarkerList to the field Position of DistanceFromXMarkerList. You can now press Calculate Distance in the panel of DistanceFromXMatkerList to see the result, meaning the distance of the green marker to all the red markers in the panel of DistanceFromXMarkerList.\nModule DistanceFromXMarkerList Automation of distance calculation To automatically update the calculation when placing a new marker, we need to tell the module DistanceFromXMarkerList when a new green marker is placed. Open the panels of DistanceFromXMarkerList and GreenMarker and draw a parameter connection from the field Currently busy in the panel of GreenMarker to Calculate Distance in the panel of DistanceFromXMarkerList. If you now place a new green marker, the distance from the new green marker to all red markers is automatically calculated. Calculation of Distance between Markers Additional Information:\u0026nbsp; Another example for using a SoView2DMarkerEditor module can be found at Image Processing - Example 3: Region Growing Summary Markers can be created using SoView2DMarkerEditor Markers can be stored and managed using XMarkerListContainer The distance between markers can be calculated using DistanceFromXMarkerList \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","3D","Marker"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/testing/testingexample1/","title":"Example 1: Writing a simple test case in MeVisLab","summary":"Example 1: Writing a simple test case in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to write an automated test for a simple network using the DicomImport, MinMaxScan and View3D modules. Afterwards, you will be able to write test cases for any other module and network yourself.\nSteps to do Creating the network to be used for testing Add the following modules to your workspace and connect them as seen below:","content":"Example 1: Writing a simple test case in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will learn how to write an automated test for a simple network using the DicomImport, MinMaxScan and View3D modules. Afterwards, you will be able to write test cases for any other module and network yourself.\nSteps to do Creating the network to be used for testing Add the following modules to your workspace and connect them as seen below:\nTestcase network Save your network as NetworkTestCase.mlab.\nTest creation Open the MeVisLab TestCaseManager via menu [ File \u0026rarr; Run TestCaseManager ]. The following window will appear.\nTestCaseManager window Change to the Test Creation tab and enter details of your test case as seen below. Make sure to have a package available already.\nInfo:\u0026nbsp; Details on package creation can be found in Example 2.1: Package creation. Select your saved NetworkTestCase.mlab file.\nTest Creation window Click Create. The MeVisLab text editor MATE will automatically open and display the Python file of your test. Add the below listed code to the Python file.\nNetworkTestCase.py\nfrom mevis import * from TestSupport import Base, Fields, Logging from TestSupport.Macros import * filePath=\u0026#34;C:/Program Files/MeVisLab3.6.0/Packages/MeVisLab/Resources/DemoData/BrainT1Dicom\u0026#34; def OpenFiles(): ctx.field(\u0026#34;DicomImport.inputMode\u0026#34;).value = \u0026#34;Directory\u0026#34; ctx.field(\u0026#34;DicomImport.source\u0026#34;).value = filePath ctx.field(\u0026#34;DicomImport.triggerImport\u0026#34;).touch() MLAB.processEvents() while not ctx.field(\u0026#34;DicomImport.ready\u0026#34;).value: MLAB.sleep(1) Base.ignoreWarningAndError(MLAB.processEvents) ctx.field(\u0026#34;DicomImport.selectNextItem\u0026#34;).touch() MLAB.log(\u0026#34;Files imported from: \u0026#34;+ctx.field(\u0026#34;DicomImport.source\u0026#34;).value) def TEST_DicomImport(): expectedValue=1.0 OpenFiles() currentValue=ctx.field(\u0026#34;DicomImport.progress\u0026#34;).value ASSERT_FLOAT_EQ(expectedValue,currentValue) The filePath variable defines the absolute path to the DICOM files that will be given to source field of the DicomImport module in the second step of the OpenFiles function.\nThe OpenFiles function first defines the DicomImport field inputMode to be a Directory. If you want to open single files, set this field\u0026rsquo;s value to Files. Then the source field is set to your previously defined filePath. After clicking triggerImport, the DicomImport module needs some time to load all images in the directory and process the DICOM tree. We have to wait until the field ready is TRUE. While the import is not ready yet, we wait for 1 millisecond at a time and check again. MLAB.processEvents() lets MeVisLab continue execution while waiting for the DicomImport to be ready.\nWhen calling the function TEST_DicomImport, an expected value of 1.0 is defined. Then, the DICOM files are opened.\nCheck:\u0026nbsp; Call Base.ignoreWarningAndError(MLAB.processEvents) instead of MLAB.processEvents() if you receive error messages regarding invalid DICOM tags. When ready is true, the test touches the selectNextItem trigger, so that the first images of the patient are selected and shown. The source directory will be written on the console as an additional log message for informative purposes.\nThe value of our DicomImports progress field is saved as the currentValue variable and compared to the expectedValue variable by calling ASSERT_FLOAT_EQ(expectedValue,currentValue) to determine if the DICOM import has finished (currentValue and expectedValue are equal) or not.\nRun your test case Open the TestCase Manager und run your test by selecting your test case and clicking on the Play button in the bottom right corner.\nRun Test Case After execution, the ReportViewer will open automatically displaying your test\u0026rsquo;s results.\nReportViewer Writing a test for global macro modules Please observe that field access through Python scripting works differently for global macros. Instead of accessing a field directly by calling their respective module, the module itself needs to be accessed as part of the global macro first.\nNetworkTestCase.py\n... # Testing a network file ctx.field(\u0026#34;DicomImport.inputMode\u0026#34;).value = \u0026#34;Directory\u0026#34; # Testing a macro module ctx.field(\u0026#34;\u0026lt;MACRO_MODULE_NAME\u0026gt;.DicomImport.inputMode\u0026#34;).value = \u0026#34;Directory\u0026#34; Imagine unpeeled nuts in a bag as a concept - the field as a nut, their module as their nutshell and the bag as the global macro.\nInfo:\u0026nbsp; Example 2.2: Global macro modules provides additional info on global macro modules and their creation. Exercise Create a global macro module and implement the following test objectives for both (network and macro module):\nCheck, if the file exists. Check, if the max value of file is greater than zero. Check, if the View3D-Input and DicomImport-output have the same data. Summary MeVisLab provides a TestCenter for writing automated tests in Python. Tests can be executed on networks and macro modules. The test results are shown in a ReportViewer. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Beginner","Tutorial","Testing","Python","Automated Tests"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/testing/testingexample2/","title":"Example 2: Profiling in MeVisLab","summary":"Example 2: Profiling in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the MeVisLab Profiler to inspect the memory and CPU consumption of the modules in an example network.\nSteps to do Creating the network to be used for profiling You can open any network you like, here we are using the example network of the module MinMaxScan for profiling. Add the module MinMaxScan to your workspace, open the example network via right-click and select [ Help \u0026rarr; Show Example Network ].","content":"Example 2: Profiling in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we are using the MeVisLab Profiler to inspect the memory and CPU consumption of the modules in an example network.\nSteps to do Creating the network to be used for profiling You can open any network you like, here we are using the example network of the module MinMaxScan for profiling. Add the module MinMaxScan to your workspace, open the example network via right-click and select [ Help \u0026rarr; Show Example Network ].\nMinMaxScan Example Network Enable Profiling Next, enable the MeVisLab Profiler via menu item [ View \u0026rarr; Views \u0026rarr; Profiling ]. The Profiler is opened in your Views Area but can be detached and dragged over the workspace holding the left mouse button .\nMeVisLab Profiling Enable profiling by checking Enable in the top left corner of the Profiling window.\nInspect your network Now open the View2D module\u0026rsquo;s panel via double-click and scroll through the slices. Inspect the Profiler.\nMeVisLab Profiling Network The Profiler shows detailed information about each module in your network.\nInfo:\u0026nbsp; Macro modules are not profiled on default. You can check the Show macros option in order to have View2D and LocalImage profiled. Also, filtering by module name is handy when you are working with larger networks. Field values and their changes for all modules in your network can be inspected in the Fields tab:\nMeVisLab Profiling Fields In addition to the Profiler window, your modules also provide a tiny bar indicating their current memory and time consumption.\nMeVisLab Profiling Module Info:\u0026nbsp; More information about profiling in MeVisLab can be found here Attention:\u0026nbsp; You need to uncheck the Enable checkbox in the top left corner to stop profiling. Closing the window will not automatically end the profiling. Summary Profiling allows you to inspect the behavior of modules and networks including CPU and memory consumption. Field value changes can be observed in the Profiler\u0026rsquo;s Fields tab. ","tags":["Beginner","Tutorial","Profiling"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/testing/testingexample3/","title":"Example 3: Iterative tests in MeVisLab with Screenshots","summary":"Example 3: Iterative tests in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you are writing an iterative test. Iterative test functions run a function for every specified input. They return a tuple consisting of the function object called and the inputs iterated over. The iterative test functions are useful if the same function should be applied to different input data. These could be input values, names of input images, etc.","content":"Example 3: Iterative tests in MeVisLab \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you are writing an iterative test. Iterative test functions run a function for every specified input. They return a tuple consisting of the function object called and the inputs iterated over. The iterative test functions are useful if the same function should be applied to different input data. These could be input values, names of input images, etc.\nSteps to do Creating the network to be used for testing Add a LocalImage and a DicomTagViewer module to your workspace and connect them.\nExample Network Test case creation Open the panel of the DicomTagViewer and set Tag Name to WindowCenter. The value of the DICOM tag from the current input image is automatically set as value.\nSave the network.\nStart MeVisLab TestCaseManager and create a new test case called IterativeTestCase as seen in Example 1: Writing a simple testcase in MeVisLab.\nDicomTagViewer Defining the test data In TestCaseManager open the test case Python file via Edit File.\nAdd a list for test data to be used as input and a prefix for the path of the test data as seen below.\nIterativeTestCase.py\nfrom mevis import * from TestSupport import Base, Fields, ScreenShot, Logging from TestSupport.Macros import * patientPathPrefix = \u0026#34;$(DemoDataPath)/BrainMultiModal/\u0026#34; testData = { \u0026#34;ProbandT1\u0026#34;:(\u0026#34;ProbandT1.dcm\u0026#34;, \u0026#34;439.9624938965\u0026#34;), \u0026#34;ProbandT2\u0026#34;:(\u0026#34;ProbandT2.dcm\u0026#34;, \u0026#34;234.91\u0026#34;)} The above list contains an identifier for the test case (ProbandT1/2), the file names and a number value. The number value is the value of the DICOM tag (0028,1050) WindowCenter for each file.\nCreate your iterative test function Add the python function to your script file: IterativeTestCase.py\ndef ITERATIVETEST_TestWindowCenter(): return testData, testPatient This function defines that testPatient shall be called for each entry available in the defined list testData. Define the function testPatient: IterativeTestCase.py\ndef testPatient(path, windowCenter): ctx.field(\u0026#34;LocalImage.name\u0026#34;).value = patientPathPrefix + path tree = ctx.field(\u0026#34;LocalImage.outImage\u0026#34;).getDicomTree() importValue = str(tree.getTag(\u0026#34;WindowCenter\u0026#34;).value()) dicomValue = str(ctx.field(\u0026#34;DicomTagViewer.tagValue0\u0026#34;).value) ASSERT_EQ(windowCenter, importValue, \u0026#34;Checking expected WindowCenter value against DICOM tree value.\u0026#34;) ASSERT_EQ(windowCenter, dicomValue, \u0026#34;Checking expected WindowCenter value against DicomTagViewer value.\u0026#34;) Initially, the path and filename for the module LocalImage are set. The data is loaded automatically, because the module has the AutoLoad flag enabled by default. LocalImage Then, the DICOM tree of the loaded file is used to get the WindowCenter value (importValue). The previously defined value of the DicomTagViewer is set as dicomValue. The final test functions ASSERT_EQ evaluate if the given values are equal. Info:\u0026nbsp; You can use many other ASSERT* possibilities, just try using the MATE auto completion and play around with them. Run your iterative test Open MeVisLab TestCase Manager and select your package and test case. You will see 2 test functions on the right side.\nIterative Test The identifiers of your test functions are shown as defined in the list (ProbandT1/2). The TestWindowCenter now runs for each entry in the list and calls the function testPatient for each entry using the given values.\nAdding screenshots to your TestReport Now, extend your network by adding a View2D module and connect it with the LocalImage module. Add the following lines to the end of your function testPatient: IterativeTestCase.py\ndef testPatient(path, windowCenter): ... Fields.setValue(\u0026#34;View2D.startSlice\u0026#34;, 0) result = ScreenShot.createOffscreenScreenShot(\u0026#34;View2D.self\u0026#34;, \u0026#34;screentest.png\u0026#34;) Logging.showImage(\u0026#34;My screenshot\u0026#34;, result) Logging.showFile(\u0026#34;Link to screenshot file\u0026#34;, result) Your ReportViewer now shows a screenshot of the image in the View2D.\nScreenshot in ReportViewer Summary Iterative tests allow you to run the same test function on multiple input entries. It is possible to add screenshots to test cases ","tags":["Advanced","Tutorial","Testing","Python","Automated Tests","Iterative Test","Screenshot"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/curves/","title":"Curves","summary":"Curves in MeVisLab Introduction Curves can be used in MeVisLab to print the results of a function as two-dimensional mathematical curves into a diagram.\nCurves in MeVisLab In the given example, only modules available in commercial MeVisLab Professional SDK have been used. The non-commercial MeVisLab Standard SDK provides more modules for curves.","content":"Curves in MeVisLab Introduction Curves can be used in MeVisLab to print the results of a function as two-dimensional mathematical curves into a diagram.\nCurves in MeVisLab In the given example, only modules available in commercial MeVisLab Professional SDK have been used. The non-commercial MeVisLab Standard SDK provides more modules for curves.\n","tags":["Beginner","Tutorial","Data Objects","2D","Curves"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/dataobjects/curves/curvesexample1/","title":"Example 1: Drawing curves","summary":"Example 1: Drawing curves \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will draw one or more curves into a diagram and define different styles for the curves.\nSteps to do Develop your network A curve requires x- and y-coordinates to be printed. You can use the CurveCreator module as input for these coordinates. The SoDiagram2D draws the curves into a SoRenderArea. You can also define the style of the curves by using the StylePalette module.","content":"Example 1: Drawing curves \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, you will draw one or more curves into a diagram and define different styles for the curves.\nSteps to do Develop your network A curve requires x- and y-coordinates to be printed. You can use the CurveCreator module as input for these coordinates. The SoDiagram2D draws the curves into a SoRenderArea. You can also define the style of the curves by using the StylePalette module.\nAdd the modules to your workspace and connect them as seen below.\nExample Network Creating a curve Click on the output of the CurveCreator and open the Output Inspector.\nEmpty Output Inspector Double-click on the CurveCreator module and open the Panel.\nCurveCreator Module You can see a large input field Curve Table. Here you can enter the x and y values of your curve. The values of the first column will become the x-values and the 2nd any further column will become the y-series. Comment lines start with a \u0026lsquo;#\u0026rsquo; character.\nEnter the following into the Curve Table: Curve Table\n# My first curve 0 0 1 1 2 2 3 3 4 4 5 5 10 10 50 50 Now your Output Inspector shows a yellow line through the previously entered coordinates. Exactly the same curve is shown in the SoRenderArea.\nSoRenderArea Creating multiple curves Now, update the Curve Table so that you are using 3 columns and click Update : Curve Table\n# My first curve 0 0 0 1 1 2 2 2 4 3 3 6 4 4 8 5 5 10 10 10 20 50 50 100 You can see 2 curves. The second and third columns are printed as separate curves. Both appear yellow. After checking Split columns into data sets, you will see one yellow and one red curve.\nbefore_split after_split If the flag Split columns into data sets is set to TRUE, then a table with more than two columns is split into different CurveData objects. This gives the user the possibility to assign a different style and title for each series.\nTitles and styles Let\u0026rsquo;s do this. Open the panel of the SoDiagram2D module and check Draw legend. Enter \u0026ldquo;Curve1 Curve2\u0026rdquo; into the Title(s) text box of the CurveCreator module and click Update .\nSoRenderArea with Legend You can also define a different location of the legend and set font sizes.\nNow open the panel of the StylePalette module.\nStylePalette The StylePalette allows you to define 12 different styles for curves. Initially without manual changes, the styles are applied one after the other. The first curve gets style 1, the second curve style 2, and so on.\nOpen the Panel of your CurveCreator again and define Curve Style(s) as \u0026ldquo;3 6\u0026rdquo;. Update your curves.\nStylePalette applied You now applied the style 3 for your first curve and 6 for the second. This is how you can create 12 different curves with unique appearance.\nUsing multiple tables for curve generation In addition to adding multiple columns for different y-coordinates, you can also define multiple tables as input, so that you can also have different x-coordinates for multiple curves.\nUpdate the Curve Table as defined below and click Update : Curve Table\n# My first curve 0 0 0 1 1 2 2 2 4 3 3 6 4 4 8 5 5 10 10 10 20 50 50 100 --- # My second curve 0 0 1 1 2 4 3 9 4 16 5 25 6 36 7 49 8 64 9 81 10 100 Also add another title to your curves and define a third style.\nMultiple tables as input Additional Information:\u0026nbsp; For more complex visualizations, you can also use Matplotlib. See examples at Thirdparty - Matplotlib. Summary Curves can be created to draw 2-dimensional diagrams The StylePalette allows you to define the appearance of a curve Details of the different curves can be visualized by using the SoDiagram2D module Additional Information:\u0026nbsp; The attached example network shows the curves after clicking Update on CurveCreator module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Beginner","Tutorial","Data Objects","2D","Curves"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/summary/","title":"Chapter VII: Application Development","summary":"MeVisLab Tutorial Chapter VII Summary This chapter will summarize all previous chapters and you will develop a whole application in MeVisLab. The complete workflow from developing a prototype to delivering your final application to your customer is explained step-by-step.\nPrototype to Product Licensing:\u0026nbsp; Some of the features described here will require a separate license. Building an installable executable requires the MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK so that you can generate an installer of your developed macro module.","content":"MeVisLab Tutorial Chapter VII Summary This chapter will summarize all previous chapters and you will develop a whole application in MeVisLab. The complete workflow from developing a prototype to delivering your final application to your customer is explained step-by-step.\nPrototype to Product Licensing:\u0026nbsp; Some of the features described here will require a separate license. Building an installable executable requires the MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK so that you can generate an installer of your developed macro module.\nFree evaluation licenses of the MeVisLab ApplicationBuilder, time-limited to 3 months, can be requested at sales(at)mevislab.de.\nPrototype Step 1: Develop your network In the first step, you are developing an application based on the following requirements:\nRequirement 1: The application shall be able to load DICOM data. Requirement 2: The application shall provide a 2D and a 3D viewer. Requirement 3: The 2D viewer shall display the loaded images Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a RegionGrowing algorithm Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the RegionGrowing Requirement 4.2: It shall be possible to define a threshold for the RegionGrowing algorithm Requirement 5: The 2D viewer shall display the segmentation results as a semi-transparent overlay Requirement 5.1: It shall be possible to define the color of the overlay Requirement 6: The 3D viewer shall visualize the loaded data in a 3-dimensional volume rendering Requirement 7: The 3D viewer shall additionally show the segmentation result as a 3-dimensional mesh Requirement 8: The total volume of the segmented area shall be calculated and shown (in ml) Requirement 9: It shall be possible to toggle the visible 3D objects Requirement 9.1: Original data Requirement 9.2: Segmentation results Requirement 9.3: All Step 2: Create your macro module Your network will be encapsulated in a macro module for later application development. For details about macro modules, see Example 2.2: Global macro modules.\nStep 3: Develop a User Interface and add Python Scripting Develop the UI and Python Scripts based on your requirements from Step 1. The resulting UI will look like below mockup:\nUser Interface Design Review Step 4: Write automated tests for your macro module Test your macro module in MeVisLab. Your requirements from Step 1 are translated into test cases written in Python. The fields accessible via Python as defined in Step 2 shall be used to test your application.\nStep 5: Create an installable executable Create a standalone application by using the MeVisLab ApplicationBuilder and install the application on another system.\nRefine Step 6: Update your network and macro module Integrate feedback from customers having installed your executable and adapt your test cases from Step 4.\nStep 7: Update your installable executable Re-build your executable and release a new version of your application.\nThe above loop can easily be repeated until your product completely fulfills your defined requirements.\n","tags":["Advanced","Tutorial"],"section":"tutorials"},{"date":"1673740800","url":"https://mevislab.github.io/examples/tutorials/summary/summary1/","title":"Step 1: Prototyping - Develop your Network","summary":"Step 1: Prototyping - Develop your Network \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will develop a network which fulfills the requirements mentioned on the overview page. The network will be developed by re-using existing modules and defining basic field values.\nSteps to do 2D viewer The 2D viewer shall visualize the loaded images. In addition to that, it shall be possible to click into the image to trigger a RegionGrowing algorithm to segment parts of the loaded image based on a threshold.","content":"Step 1: Prototyping - Develop your Network \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we will develop a network which fulfills the requirements mentioned on the overview page. The network will be developed by re-using existing modules and defining basic field values.\nSteps to do 2D viewer The 2D viewer shall visualize the loaded images. In addition to that, it shall be possible to click into the image to trigger a RegionGrowing algorithm to segment parts of the loaded image based on a threshold.\nThe following requirements from the overview will be implemented:\nRequirement 1: The application shall be able to load DICOM data. Requirement 3: The 2D viewer shall display the loaded images Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a RegionGrowing algorithm Requirement 4.1: It shall be possible to click into the image to set a marker position to start the RegionGrowing Requirement 4.2: It shall be possible to define a threshold for the RegionGrowing algorithm Requirement 5: The 2D viewer shall display the segmentation results as a semi-transparent overlay Requirement 5.1: It shall be possible to define the color of the overlay Add a LocalImage and a View2D module to your workspace. You are now able to load an image and view the slices.\nLoading an image RegionGrowing requires a SoView2DMarkerEditor, a SoView2DOverlay and a RegionGrowing module. Add them to your network and connect them as seen below. Configure the RegionGrowing module to use a 3D-6-Neighborhood (x,y,z) relation and an automatic threshold value of 1.500. Also select Auto-Update.\nSet SoView2DMarkerEditor to allow only one marker by defining Max Size = 1 and Overflow Mode = Remove All. For our application we only want one marker to be set for defining the RegionGrowing.\nIf you now click into your loaded image via left mouse button , the RegionGrowing module segments all neighborhood pixels with a mean intensity value plus/minus defined percentage value from your click position.\nThe overlay is shown in white.\nRegionGrowing via marker editor Open the SoView2DOverlay module, change Blend Mode to Blend and select any color and Alpha Factor for your overlay. The applied changes are immediately visible. Overlay color and transparency The segmented results from the RegionGrowing module might contain gaps because of differences in the intensity value of neighboring pixels. You can close these gaps by adding a CloseGap module. Connect it to the RegionGrowing and the SoView2DOverlay module and configure Filter Mode as Binary Dilatation, Border Handling as Pad Dst Fill and set KernelZ to 3.\nLastly, we want to calculate the volume of the segmented parts. Add a CalculateVolume module to the CloseGap module. The 2D viewer now provides the basic functionalities.\nYou can group the modules in your network for an improved overview by selecting [ Grouping \u0026rarr; Add to new Group... ]. Leave LocalImage out of the group and name it 2D Viewer. Your network should now look like this:\nGroup 2D Viewer 3D Viewer The 3D viewer shall visualize your loaded image in 3D and additionally provide the possibility to render your segmentation results. You will be able to decide for different views, displaying the image and the segmentation, only the image or only the segmentation. The volume (in ml) of your segmentation results shall be calculated.\nThe following requirements from overview will be implemented:\nRequirement 2: The application shall provide a 2D and a 3D viewer. Requirement 6: The 3D viewer shall visualize the loaded data in a 3-dimensional volume rendering. Requirement 7: The 3D viewer shall additionally show the segmentation result as a 3-dimensional mesh. Requirement 8: The total volume of the segmented area shall be calculated and shown (in ml). Requirement 9: It shall be possible to toggle the visible 3D objects. Requirement 9.1: Original data Requirement 9.2: Segmentation results Requirement 9.3: All Add a SoExaminerViewer, a SoWEMRenderer and an IsoSurface module to your existing network and connect them to the LocalImage module. Configure the IsoSurface to use an IsoValue of 200, a Resolution of 1 and check Auto-Update and Auto-Apply.\n3D Viewer The result should be a 3-dimensional rendering of your image.\nSoExaminerViewer Info:\u0026nbsp; If the rendering is not immediately applied, click Apply in your IsoSurface module. Define the field instanceName of your IsoSurface module as IsoSurfaceImage and add another IsoSurface module to your network. Set the instanceName to IsoSurfaceSegmentation and connect the module to the output of the CloseGap module from the image segmentation. Set IsoValue to 420, Resolution to 1 and check Auto-Update and Auto-Apply.\nSet instanceName of the SoWEMRenderer module to SoWEMRendererImage and add another SoWEMRenderer module. Set this instanceName to SoWEMRendererSegmentation and connect it to the IsoSurfaceSegmentation module. Selecting the output of the new SoWEMRenderer shows the segmented parts as a 3D object in the output inspector.\nSegmentation preview in output inspector Once again, we should group the modules used for 3D viewing and name the new group 3D Viewer.\nGrouped network We now want to allow the user to toggle the different 3D visualizations as defined by the requirements above. It shall be possible to show:\nOriginal data only Segmentation only Original data and segmentation combined Add a SoSwitch module to your network. Connect the switch to both of your SoWEMRenderer modules and to the SoExaminerViewer.\nSoSwitch The default input of the switch is None. Your 3D viewer remains black. Using the arrows on the SoSwitch allows you to toggle between the segmentation and the image. Input 0 shows the segmented brain, input 1 shows the head. You are now able to toggle between them. A view with both objects is still missing.\nExample1_Segmentation Example1_Image Add a SoGroup module and connect both SoWEMRenderer modules as input. The output needs to be connected to the right input of the SoSwitch module.\nSoGroup You can now also toggle input 2 of the switch showing both 3D objects. The only problem is: You cannot see the brain because it is located inside the head. Open the SoWEMRendererImage module panel and set faceAlphaValue to 0.5. The viewer now shows the head in a semi transparent manner so that you can see the brain. Certain levels of opacity are difficult to render. Add a SoDepthPeelRenderer module and connect it to the semi transparent SoWEMRendererImage module. Set Layers of the renderer to 1.\nSoDepthPeelRenderer You have a 2D and a 3D viewer now. Let\u0026rsquo;s define the colors of the overlay to be re-used for the 3D segmentation.\nParameter connections for visualization Open the panels of the SoView2DOverlay and the SoWEMRendererSegmentation module. Draw a parameter connection from SoView2DOverlay.baseColor to SoWEMRendererSegmentation.faceDiffuseColor.\nSynchronized segmentation colors Now the 3D visualization uses the same color as the 2D overlay.\nSummary You built a network providing the basic functionalities of your application. Actions inside your application need to be executed by changing fields in your network or by manually touching a trigger. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. ","tags":["Advanced","Tutorial","Prototyping"],"section":"tutorials"},{"date":"1673827200","url":"https://mevislab.github.io/examples/tutorials/summary/summary2/","title":"Step 2: Prototyping - Create a macro module","summary":"Step 2: Prototyping - Create a macro module \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we encapsulate the previously developed prototype network into a macro module for future application development and automated testing.\nSteps to do Make sure to have your *.mlab file from the previous tutorial available.\nPackage creation Packages are described in detail in Example 2.1: Package creation. If you already have your own package, you can skip this part and continue creating a macro module.","content":"Step 2: Prototyping - Create a macro module \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this example, we encapsulate the previously developed prototype network into a macro module for future application development and automated testing.\nSteps to do Make sure to have your *.mlab file from the previous tutorial available.\nPackage creation Packages are described in detail in Example 2.1: Package creation. If you already have your own package, you can skip this part and continue creating a macro module.\nOpen Project Wizard via [ File \u0026rarr; Run Project Wizard... ] and select New Package. Run the Wizard and enter details of your new package and click Create.\nPackage wizard MeVisLab reloads and you can start creating your macro module.\nCreate a macro module Open Project Wizard via [ File \u0026rarr; Run Project Wizard... ] and select macro module. Run the Wizard and enter details of your new macro module.\nMacro module wizard Select the created package and click Next.\nMacro module wizard Select your *.mlab file from Step 1 and check Add Python file. Click Next.\nMacro module wizard You do not have to define fields of your macro module now, we will do that later. Click Create. The Windows Explorer opens showing the directory of your macro module. It should be the same directory you selected for your Package.\nDirectory Structure of a macro module The directory structure for a macro module is as follows:\nFrom Package Wizard: Package target directory is the root directory of the module The next directory is the package group and package name From macro module Wizard: The name of the macro module defines the directory containing all files of your module An additional directory Modules is created containing the following files: \u0026lt;MACRO_NAME\u0026gt;.def \u0026lt;MACRO_NAME\u0026gt;.mlab \u0026lt;MACRO_NAME\u0026gt;.py \u0026lt;MACRO_NAME\u0026gt;.script Directory Structure Definition (*.def) file The initial *.def file contains information you entered into the Wizard for the macro module.\n\u0026lt;MACRO_NAME\u0026gt;.def\nMacro module TutorialSummary { genre = \u0026#34;VisualizationMain\u0026#34; author = \u0026#34;MeVis Medical Solutions AG\u0026#34; comment = \u0026#34;Macro module for MeVisLab Tutorials\u0026#34; keywords = \u0026#34;2D 3D RegionGrowing\u0026#34; seeAlso = \u0026#34;\u0026#34; externalDefinition = \u0026#34;$(LOCAL)/TutorialSummary.script\u0026#34; } An externalDefinition to a script file is also added (see below for the *.script file).\nMeVisLab Network (*.mlab) file The *.mlab file is a copy of the *.mlab file you developed in Step 1 and re-used in the wizard. In the next chapters, this file will be used as internal network.\nPython (*.py) file The initial *.py file only contains the import of MeVisLab specific objects and functions. In the future steps, we will add functionalities to our application in Python.\n\u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * Script (*.script) file The script (*.script) file defines fields accessible from outside the macro module, inputs and outputs and allows you to develop a User Interface for your prototype and your final application.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters {} } Commands { source = $(LOCAL)/TutorialSummary.py } The source also defines your Python file to be used when calling functions and events from the User Interface.\nUsing your macro module As you created a global macro module, you can search for it in the MeVisLab Module Search.\nModule Search We did not define inputs or outputs. You cannot connect your module to others. In addition to that, we did not develop a User Interface. Double-clicking your module only opens the Automatic Panel showing the instanceName.\nAutomatic Panel Right-click on your module allows you to open the internal network as developed in Step 1.\nSummary Macro modules encapsulate an entire MeVisLab network including all modules. The internal network can be shown (and edited) via right-click [ Show Internal Network ] The Wizard already creates the necessary folder structure and generates files for User Interface and Python development. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","Macro modules"],"section":"tutorials"},{"date":"1673913600","url":"https://mevislab.github.io/examples/tutorials/summary/summary3/","title":"Step 3: Prototyping - User Interface and Python scripting","summary":"Step 3: Prototyping - User Interface and Python scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step, we will develop a user interface and add Python scripting to the macro module you created in Step 2.\nSteps to do Develop the User Interface A mockup of the user interface you are going to develop is available here. The interface provides the possibility to load files and shows a 2D and a 3D viewer.","content":"Step 3: Prototyping - User Interface and Python scripting \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step, we will develop a user interface and add Python scripting to the macro module you created in Step 2.\nSteps to do Develop the User Interface A mockup of the user interface you are going to develop is available here. The interface provides the possibility to load files and shows a 2D and a 3D viewer. In addition to that, some settings and information for our final application are available.\nSearch for your macro module and add it to your workspace. Right-click and select [ Related Files \u0026rarr; \u0026lt;MACRO_MODULE_NAME\u0026gt;.script ].\nThe MeVisLab text editor MATE opens showing the *.script file of your module.\nLayout You can see that the interface is divided into 4 parts in vertical direction:\nSource or file/directory selection Viewing (2D and 3D) Settings Info Inside the vertical parts, the elements are placed next to each other horizontally.\nAdd a Window section to your *.script file. Inside the Window, we need a Vertical for the 4 parts and a Box for each part. Name the Boxes Source, Viewing, Settings and Info. The layout inside each Box shall be Horizontal.\nIn addition to that, we define the minimal size of the Window as 400 x 300 pixels.\n\u0026lt;MACRO_NAME\u0026gt;.script\nWindow { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal } Box Viewing { layout = Horizontal } Box Settings { layout = Horizontal } Box Info { layout = Horizontal } } } You can preview your initial layout in MeVisLab by double-clicking your module .\nInitial Window Layout You can see the 4 vertical aligned parts as defined in the *.script file. Now we are going to add the content of the Boxes.\nAdditional Info:\u0026nbsp; An overview over the existing layout elements in MeVisLab Definition Language (MDL) can be found here Adding the UI elements Source The Source Box shall provide the possibility to select a file for loading into the viewers. You have many options to achieve that in MeVisLab and Python. The easiest way is to re-use the existing field of the LocalImage module in your internal network.\nAdd a field to the Parameters section of your *.script file. Name the field openFile and set type to String and internalName to LocalImage.name.\nThen add another field to your Box for the Source and use the field name from Parameters section, in this case openFile. Set browseButton = True and browseMode = open and save your script.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { Field openFile { type = String internalName = LocalImage.name } } } ... Window { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } } Box Viewing { layout = Horizontal } Box Settings { layout = Horizontal } Box Info { layout = Horizontal } } } Again, you can preview your user interface in MeVisLab directly. You can already select a file to open. The image is available at the output of the LocalImage module in your internal network but the Viewers are missing in our interface.\nSource Box Viewing Add the 2 viewer modules to the Viewing section of your *.script file and define their field as View2D.self and SoExaminerViewer.self. Set expandX = Yes and expandY = Yes for both viewing modules. We want them to resize in case the size of the Window changes.\nSet the 2D Viewer type to SoRenderArea and the 3D Viewer type to SoExaminerViewer and inspect your new user interface in MeVisLab.\n\u0026lt;MACRO_NAME\u0026gt;.script\n... Box Viewing { layout = Horizontal Viewer View2D.self { expandX = True expandY = True type = SoRenderArea } Viewer SoExaminerViewer.self { expandX = True expandY = True type = SoExaminerViewer } } ... 2D and 3D Viewer The images selected in the Source section are shown in 2D and 3D. We simply re-used the existing fields and viewers from your internal network and are already able to interact with the images. As the View2D of your internal network itself provides the possibility to accept markers and starts the RegionGrowing, this is also already possible and the segmentations are shown in 2D and 3D.\nSettings Let\u0026rsquo;s define the Settings section. Once again we first define the necessary fields. For automated tests which we are going to develop later, it makes sense to make some of the fields of the internal network available from outside.\nThe following shall be accessible as Field for our macro module:\nFilename to be opened Color of the 2D overlay and 3D segmentation Transparency of the 3D image Threshold to be used for RegionGrowing Iso value of the 3D surface to use for rendering Position of the Marker to use for RegionGrowing Selection for 3D visualization (image, segmentation or both) Trigger to reset the application to its initial state We already defined the filename as a field. Next we want to change the color of the overlay. Add another field to your Parameters section as selectOverlayColor. Define internalName = SoView2DOverlay.baseColor and type = Color. You may also define a title for the field, for example Color.\nThe baseColor field of the SoView2DOverlay already has a parameter connection to the color of the SoWEMRendererSegmentation. This has been done in the internal network. The defined color is used for 2D and 3D automatically.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { ... Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } } } ... Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } } ... The next elements follow the same rules, therefore the final script will be available at the end for completeness.\nIn order to set the transparency of the 3D image, we need another field re-using the SoWEMRendererImage.faceAlphaValue. Add a field imageAlpha to the Parameters section. Define internalName = SoWEMRendererImage.faceAlphaValue, type = Integer, min = 0 and max = 1.\nAdd the field to the Settings Box and set step = 0.1 and slider = True.\nFor the RegionGrowing threshold, add the field thresholdInterval to Parameters section and set type = Integer, min = 1, max = 100 and internalName = RegionGrowing.autoThresholdIntervalSizeInPercent.\nAdd the field to the Settings UI and define step = 0.1 and slider = True.\nDefine a field isoValueImage in the Parameters section and set internalName = IsoSurfaceImage.isoValue, type = Integer, min = 1 and max = 1000.\nIn the Settings section of the UI, set step = 2 and slider = True.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { Field openFile { type = String internalName = LocalImage.name } Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } Field imageAlpha { internalName = SoWEMRendererImage.faceAlphaValue type = Integer min = 0 max = 1 } Field thresholdInterval { internalName = RegionGrowing.autoThresholdIntervalSizeInPercent type = Integer min = 0 max = 100 } Field isoValueImage { internalName = IsoSurfaceImage.isoValue type = Integer min = 0 max = 1000 } } } Commands { source = $(LOCAL)/TutorialSummary.py } Window { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } } Box Viewing { layout = Horizontal Viewer View2D.self { expandX = True expandY = True type = SoRenderArea } Viewer SoExaminerViewer.self { expandX = True expandY = True type = SoExaminerViewer } } Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } Field imageAlpha { step = 0.1 slider = True } Field thresholdInterval { step = 0.1 slider = True } Field isoValueImage { step = 2 slider = True } } Box Info { layout = Horizontal } } } Your user interface of the macro module should now look similar to this:\nUser Interface without Python Scripting For the next elements, we require Python scripting. Nevertheless, you are already able to use your application and perform the basic functionalities without writing any line of code.\nPython scripting Python scripting is always necessary in case you do not want to re-use an existing field for your user interface but implement functions to define what happens in case of any event.\nEvents can be raised by the user (i.e. by clicking a button) or by the application itself (i.e. when the window is opened).\n3D visualization selection You will now add a selection possibility for the 3D viewer. This allows you to define the visibility of the 3D objects File, Segmented or Both.\nAdd another field to your Parameters section. Define the field as selected3DView and set type = Enum and values =Segmented,File,Both.\nAdd a ComboBox to your Settings and use the field name defined above. Set alignX = Left and editable = False and open the Window of the macro module in MeVisLab.\nThe values of the field can be selected, but nothing happens in our viewers. We need to implement a FieldListener in Python which reacts on any value changes of the field selected3DView.\nOpen your script file and go to the Commands section. Add a FieldListener and re-use the name of our internal field selected3DView. Add a Command to the FieldListener calling a Python function viewSelectionChanged.\n\u0026lt;MACRO_NAME\u0026gt;.script\nCommands { source = $(LOCAL)/TutorialSummary.py FieldListener selected3DView { command = viewSelectionChanged } } Right-click the command select [ Create Python Function \u0026#39;viewSelectionChanged\u0026#39; ]. MATE automatically opens the Python file of your macro module and creates a function viewSelectionChanged.\n\u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def viewSelectionChanged(field): if field.value == \u0026#34;Segmented\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 0 if field.value == \u0026#34;File\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 1 if field.value == \u0026#34;Both\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 2 The function sets the SoSwitch to the child value depending on the selected field value from the ComboBox and you should now be able to switch the 3D rendering by selecting an entry in the user interface.\nSetting the Marker The Marker for the RegionGrowing is defined by the click position as Vector3. Add another field markerPosition to the Parameters section and define type = Vector3.\nThen, add a trigger field applyMarker to your Parameters section. Set type = Trigger and title = Add.\n\u0026lt;MACRO_NAME\u0026gt;.script\n... Field markerPosition { type = Vector3 } Field applyMarker { type = Trigger title = Add } ... Add another FieldListener to both fields: \u0026lt;MACRO_NAME\u0026gt;.script\n... FieldListener markerPosition { command = insertPosition } FieldListener applyMarker { command = applyPosition } ... Finally, add both fields to the Settings section of your user interface: \u0026lt;MACRO_NAME\u0026gt;.script\n... Field markerPosition {} Field applyMarker {} ... The Python functions should look like this: \u0026lt;MACRO_NAME\u0026gt;.py\n... def insertPosition(field): ctx.field(\u0026#34;SoView2DMarkerEditor.newPosXYZ\u0026#34;).value = field.value def applyPosition(): ctx.field(\u0026#34;SoView2DMarkerEditor.useInsertTemplate\u0026#34;).value = True ctx.field(\u0026#34;SoView2DMarkerEditor.add\u0026#34;).touch() ... Whenever the field markerPosition changes its value, the value is automatically applied to the SoView2DMarkerEditor.newPosXYZ. Clicking SoView2DMarkerEditor.add adds the new Vector to the SoView2DMarkerEditor and the region growing starts.\nInfo:\u0026nbsp; The Field SoView2DMarkerEditor.useInsertTemplate needs to be set to True in order to allow adding markers via Python. Reset Add a new field resetApplication to the Parameters section and set type = Trigger and title = Reset.\nAdd another FieldListener to your Commands and define command = resetApplication.\nAdd the field to your Source region.\n\u0026lt;MACRO_NAME\u0026gt;.script\n... Parameters { Field resetApplication { type = Trigger title = Reset } } ... Commands { ... FieldListener resetApplication { command = resetApplication } } ... Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } Field resetApplication { } } ... What shall happen when we reset the application?\nThe loaded image shall be unloaded, the Viewer shall be empty The marker shall be reset if available Add the Python function resetApplication and implement the following: \u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() You can also reset the application to initial state by adding a initCommand to your Window. Call the resetApplication function here, too and whenever the window is opened, the application is reset to its initial state.\n\u0026lt;MACRO_NAME\u0026gt;.script\nWindow { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 initCommand = resetApplication ... } This can also be used for setting/resetting to default values of the application. For example update your Python function resetApplication the following way:\n\u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() ctx.field(\u0026#34;imageAlpha\u0026#34;).value = 0.5 ctx.field(\u0026#34;thresholdInterval\u0026#34;).value = 1.0 ctx.field(\u0026#34;isoValueImage\u0026#34;).value = 200 ctx.field(\u0026#34;selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; Information In the end, we want to provide some information about the volume of the segmented area (in ml).\nAdd one more field to your Parameters section and re-use the internal network fields CalculateVolume.totalVolume. Set field to editable = False\nAdd the field to the Info section of your window.\nOpening the window of your macro module in MeVisLab now provides all functionalities we wanted to achieve. You can also play around in the window and define some additional Boxes or MDL controls but the basic application prototype is now done.\nFinal Macro module MeVisLab GUI Editor MATE provides a powerful GUI Editor showing a preview of your current user interface and allowing to re-order elements in the UI via drag and drop. In MATE open [ Extras \u0026rarr; Enable GUI Editor ].\nMeVisLab GUI Editor Changing the layout via drag and drop automatically adapts your *.script file. Save and Reload the script and your changes are applied.\nInfo:\u0026nbsp; If the GUI Editor is not shown in MATE, make sure to check [View → Preview]. Final Script and Python files \u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters { Field openFile { type = String internalName = LocalImage.name } Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } Field imageAlpha { internalName = SoWEMRendererImage.faceAlphaValue type = Integer min = 0 max = 1 } Field thresholdInterval { internalName = RegionGrowing.autoThresholdIntervalSizeInPercent type = Integer min = 0 max = 100 } Field isoValueImage { internalName = IsoSurfaceImage.isoValue type = Integer min = 0 max = 1000 } Field selected3DView { type = Enum values = Segmented,File,Both } Field totalVolume { internalName = CalculateVolume.totalVolume editable = False } Field resetApplication { type = Trigger title = Reset } Field markerPosition { type = Vector3 } Field applyMarker { type = Trigger title = Add } } } Commands { source = $(LOCAL)/\u0026lt;MACRO_NAME\u0026gt;.py FieldListener selected3DView { command = viewSelectionChanged } FieldListener resetApplication { command = resetApplication } FieldListener markerPosition { command = insertPosition } FieldListener applyMarker { command = applyPosition } } Window { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 initCommand = resetApplication // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } Field resetApplication { } } Box Viewing { layout = Horizontal Viewer View2D.self { expandX = True expandY = True type = SoRenderArea } Viewer SoExaminerViewer.self { expandX = True expandY = True type = SoExaminerViewer } } Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } Field imageAlpha { step = 0.1 slider = True } Field thresholdInterval { step = 0.1 slider = True } Field isoValueImage { step = 2 slider = True } Field markerPosition {} Field applyMarker {} ComboBox selected3DView { alignX = Left editable = False } } Box Info { layout = Horizontal Field totalVolume {} } } } \u0026lt;MACRO_NAME\u0026gt;.py\nfrom mevis import * def viewSelectionChanged(field): if field.value == \u0026#34;Segmented\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 0 if field.value == \u0026#34;File\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 1 if field.value == \u0026#34;Both\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 2 def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() ctx.field(\u0026#34;imageAlpha\u0026#34;).value = 0.5 ctx.field(\u0026#34;thresholdInterval\u0026#34;).value = 1.0 ctx.field(\u0026#34;isoValueImage\u0026#34;).value = 200 ctx.field(\u0026#34;selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; def insertPosition(field): ctx.field(\u0026#34;SoView2DMarkerEditor.newPosXYZ\u0026#34;).value = field.value def applyPosition(): ctx.field(\u0026#34;SoView2DMarkerEditor.useInsertTemplate\u0026#34;).value = True ctx.field(\u0026#34;SoView2DMarkerEditor.add\u0026#34;).touch() Summary You now added a user interface to your macro module. The window opens automatically on double-click Fields defined in the Parameters section can be modified in the MeVisLab Module Inspector Python allows to implement functions executed on events raised by the user or by the application itself. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","User Interface","Python","GUI Editor"],"section":"tutorials"},{"date":"1674000000","url":"https://mevislab.github.io/examples/tutorials/summary/summary4/","title":"Step 4: Review - Automated Tests","summary":"Step 4: Review - Automated Tests \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous chapters you developed a macro module with User Interface and Python scripting. In this step you will see how to implement an automated test to verify and validate the Requirements defined in Overview.\nSteps to do Create a test network using your macro module Create a new and empty network and save it as *.","content":"Step 4: Review - Automated Tests \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In the previous chapters you developed a macro module with User Interface and Python scripting. In this step you will see how to implement an automated test to verify and validate the Requirements defined in Overview.\nSteps to do Create a test network using your macro module Create a new and empty network and save it as *.mlab file. Remember the location.\nUse Module Search and add your macro module developed in previous steps to your Workspace.\nMacro module You can see that the module does not have any inputs or outputs. You cannot connect it to other modules. For testing purposes it makes sense to provide the viewers and images as outputs so that you can use them for generating screenshots.\nOpen the *.script file in MATE as already explained in Step 3. In the Outputs section, add the following:\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { Inputs {} Outputs { Field out2D { internalName = LocalImage.outImage } Field out3D { internalName = SoSwitch.self } Field outSegmentationMask { internalName = CloseGap.output0 } } ... } Macro module with outputs You can now add a viewer or any other module to your macro module and use them for testing. In our example, we add a CalculateVolume module to the segmentation mask and a SoCameraInteraction with two OffscreenRenderer modules to the 3D output. In the end, we need an ImageCompare module to compare expected and real image in our test.\nTest Network Create test case Open MeVisLab TestCaseManager via [ File \u0026rarr; Run TestCaseManager... ]. On tab Test Creation define a name of your test case, for example TutorialSummaryTest. Select Type as Macros, define the package and use the same as for your macro module, select Import Network and Select your saved *.mlab file from the step above. Click Create.\nTest Creation MATE automatically opens the Python file of your test case and it appears in MeVisLab TestCaseManager.\nTest Creation Write test functions in Python Preparations Before writing a test case, we need some helper functions in Python, which we will use in our test cases. The first thing we need is a function to load images.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\nfrom mevis import * from TestSupport import Base, Fields, Logging, ScreenShot from TestSupport.Macros import * path_to_image = \u0026#34;$(DemoDataPath)/BrainMultiModal/ProbandT1.dcm\u0026#34; marker_location = [-29, -26, 45] marker_location_new = [-20, -30, 35] new_color = [0.5, 0.5, 0] def loadImage(full_path): MLAB.log(\u0026#34;Setting image path to \u0026#39;\u0026#34; + full_path + \u0026#34;\u0026#39;...\u0026#34;) ctx.field(\u0026#34;TutorialSummary.openFile\u0026#34;).value = full_path We define the path to a file to be loaded. The function loadImage sets the openFile field of the TutorialSummary module.\nThe arrays for the marker location and color will be used later.\nNext we need a function to check if the loaded image available at the first output of our macro module (out2D) is valid.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def isImageValid(): MLAB.log(\u0026#34;Checking if image is valid...\u0026#34;) data_valid = ctx.field(\u0026#34;TutorialSummary.out2D\u0026#34;).isValid() if data_valid: return True else: return False ... We also need to set a marker in our macro module.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def setMarkerPosition(vector): MLAB.log(\u0026#34;Setting marker position to [\u0026#34; + str(vector[0]) + \u0026#34;,\u0026#34; + str(vector[1]) + \u0026#34;,\u0026#34; + str(vector[2]) + \u0026#34;]...\u0026#34;) ctx.field(\u0026#34;TutorialSummary.markerPosition\u0026#34;).setValue(vector[0], vector[1], vector[2]) ctx.field(\u0026#34;TutorialSummary.applyMarker\u0026#34;).touch() MLAB.processEvents() while not ctx.field(\u0026#34;TutorialSummary.outSegmentationMask\u0026#34;).isValid(): MLAB.msleep(100) MLAB.processEvents() MLAB.log(\u0026#34;Marker position set to \u0026#39;\u0026#34; + str(ctx.field(\u0026#34;TutorialSummary.markerPosition\u0026#34;).value) + \u0026#34;\u0026#39;...\u0026#34;) ... The setMarkerPosition function gets a 3-dimensional vector and sets the markerPosition field of our module. Then the applyMarker trigger is touched. As the region growing algorithm might need some time to segment, we need to wait until the outSegmentationMask output field is valid, meaning that there is a valid segmentation mask at the segmentation mask output of our macro module.\nFinally, we need to reset the application to its initial state, so that each test case has the initial start conditions of the application. A test case should never depend on another test case so that they all can be executed exclusively.\nExample: Having one test case for the requirement to load images and one for setting the marker depending on the image to be loaded by the previous test case, you will never be able to execute the marker test case without executing the load image first.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def reset(): MLAB.log(\u0026#34;Resetting application...\u0026#34;) ctx.field(\u0026#34;TutorialSummary.resetApplication\u0026#34;).touch() ... For a reset, we just touch the resetApplication field of our macro module TutorialSummary.\nRequirement 1: The application shall be able to load DICOM data The first requirement we want to test is the possibility to load DICOM data. After setting the file to be loaded, the output provides a valid image. Resetting the application shall unload the image.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 1: The application shall be able to load DICOM data def TEST_LoadDICOMData(): # Set path to image and expect a valid image loadImage(path_to_image) ASSERT_TRUE(isImageValid()) # Reset again and expect an invalid image reset() ASSERT_FALSE(isImageValid()) ... Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a RegionGrowing algorithm Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the RegionGrowing This test case shall make sure the RegionGrowing module calculates the total volume and number of voxels to be larger than 0 in case a marker has been set. Without loading an image or after resetting the application, the values shall be 0.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a RegionGrowing algorithm # Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the RegionGrowing def TEST_RegionGrowing(): # Load image and expect volumes and voxels without marker to be 0 loadImage(path_to_image) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) # Set marker and expect volumes and voxels to be larger than 0 reset() setMarkerPosition(marker_location) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_GT(region_growing_voxels, 0) ASSERT_GT(region_growing_volume, 0) # Reset application and expect volumes and voxels to be 0 again reset() region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) ... Requirement 4.2: It shall be possible to define a threshold for the RegionGrowing algorithm For the threshold of the region growing it makes sense to extend the previous test case instead of writing a new one. We already have a segmentation based on the default threshold value and can just change the threshold and compare the resulting volumes.\nIncreasing the threshold shall result in larger volumes, decreasing shall result in smaller values.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 4: The 2D viewer shall provide the possibility to segment parts of the image based on a RegionGrowing algorithm # Requirement 4.1: It shall be possible to click into the image for defining a marker position for starting the RegionGrowing # Requirement 4.2: It shall be possible to define a threshold for the RegionGrowing algorithm def TEST_RegionGrowing(): # Load image and expect volumes and voxels without marker to be 0 loadImage(path_to_image) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) # Set marker and expect volumes and voxels to be larger than 0 setMarkerPosition(marker_location) region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_GT(region_growing_voxels, 0) ASSERT_GT(region_growing_volume, 0) # Test the threshold functionality by changing the value and comparing the results current_threshold = ctx.field(\u0026#34;TutorialSummary.thresholdInterval\u0026#34;).value current_threshold = current_threshold + 0.5 ctx.field(\u0026#34;TutorialSummary.thresholdInterval\u0026#34;).value = current_threshold region_growing_voxels_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_GT(region_growing_voxels_new, region_growing_voxels) ASSERT_GT(region_growing_volume_new, region_growing_volume) current_threshold = current_threshold - 0.7 ctx.field(\u0026#34;TutorialSummary.thresholdInterval\u0026#34;).value = current_threshold region_growing_voxels_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume_new = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_LT(region_growing_voxels_new, region_growing_voxels) ASSERT_LT(region_growing_volume_new, region_growing_volume) # Reset application and expect volumes and voxels to be 0 again reset() region_growing_voxels = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.numSegmentedVoxels\u0026#34;).value region_growing_volume = ctx.field(\u0026#34;TutorialSummary.RegionGrowing.segmentedVolume_ml\u0026#34;).value ASSERT_EQ(region_growing_voxels, 0) ASSERT_EQ(region_growing_volume, 0) ... Requirement 5: The 2D viewer shall display the segmentation results as a semi-transparent overlay Requirement 5.1: It shall be possible to define the color of the overlay The requirement 5 can not be tested automatically. Transparencies should be tested by a human being.\nNevertheless, we can write an automated test checking the possibility to define the color of the overlay and the 3D segmentation.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... def TEST_OverlayColor(): reset() loadImage(path_to_image) setMarkerPosition(marker_location) ctx.field(\u0026#34;SoCameraInteraction.viewAll\u0026#34;).touch() ctx.field(\u0026#34;SoCameraInteraction.viewFromLeft\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() MLAB.processInventorQueue() current_color = ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).setValue(new_color) ctx.field(\u0026#34;SoCameraInteraction.viewAll\u0026#34;).touch() ctx.field(\u0026#34;SoCameraInteraction.viewFromLeft\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() MLAB.processInventorQueue() ASSERT_NE(current_color, ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value) ASSERT_EQ(ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value, ctx.field(\u0026#34;TutorialSummary.SoView2DOverlay.baseColor\u0026#34;).value) ASSERT_EQ(ctx.field(\u0026#34;TutorialSummary.selectOverlayColor\u0026#34;).value, ctx.field(\u0026#34;TutorialSummary.SoWEMRendererSegmentation.faceDiffuseColor\u0026#34;).value) ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ... Again, we reset the application to an initial state, load the image and set a marker. We remember the initial color and set a new color for our macro module. Then we check if the new color differs from the old color and if the colors used by the internal modules SoWEMRendererSegmentation and SoView2DOverlay changed to our new color.\nFinally an image comparison is done for the 3D rendering using the old and the new color. The images shall differ.\nThe call MLAB.processInventorQueue() is sometimes necessary if an inventor scene changed via Python scripting, because the viewers might not update immediately after changing the field. MeVisLab is now forced to process the queue in inventor and to update the renderings.\nRequirement 8: The total volume of the segmented area shall be calculated and shown (in ml) For the correctness of the volume calculation, we added the CalculateVolume module to our test network. The volume given by our macro is compared to the volume of the segmentation from output outSegmentationMask calculated by the CalculateVolume module.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 8: The total volume of the segmented area shall be calculated and shown (in ml) def TEST_VolumeCalculation(): # Reset and expect all volumes and number of voxels to be 0 reset() reference_volume = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value ASSERT_EQ(reference_volume, 0) # Load patient, set marker and expect all volumes and number of voxels to be \u0026gt; 0 loadImage(path_to_image) reference_volume = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value ASSERT_EQ(reference_volume, 0) setMarkerPosition(marker_location) reference_volume = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value current_volume = ctx.field(\u0026#34;TutorialSummary.totalVolume\u0026#34;).value # Expect the total volume of the application to be the same as our additional CalculateVolume module ASSERT_GT(reference_volume, 0) ASSERT_EQ(reference_volume, current_volume) #set marker to a different location and check if volumes change. setMarkerPosition(marker_location_new) reference_volume_new = ctx.field(\u0026#34;CalculateVolume.totalVolume\u0026#34;).value current_volume_new = ctx.field(\u0026#34;TutorialSummary.totalVolume\u0026#34;).value ASSERT_NE(reference_volume, reference_volume_new) ASSERT_NE(current_volume, current_volume_new) ASSERT_EQ(reference_volume_new, current_volume_new) ... Requirement 9: It shall be possible to toggle the visible 3D objects Requirement 9.1: Original data Requirement 9.2: Segmentation results Requirement 9.3: All In the end, we want to develop a testcase for the 3D toggling of the view. We can not exactly test if the rendering is correct, therefore we will check if the 3D rendering image changes when toggling the 3D view. We will use the modules OffscreenRenderer, ImageCompare and SoCameraInteraction which we added to our test network.\nInitially, without any marker and segmentation, the views Both and Head show the same result. After adding a marker, we are going to test if different views result in different images.\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... # Requirement 9: It shall be possible to toggle the visible 3D objects # Requirement 9.1: Original data # Requirement 9.2: Segmentation results # Requirement 9.3: All def TEST_Toggle3DVolumes(): # Set ImageCompare.postErrorOnDiff to False because otherwise differences will lead to a failed test ctx.field(\u0026#34;ImageCompare.postErrorOnDiff\u0026#34;).value = False # Reset application and check if number of voxels is 0 on output reset() loadImage(path_to_image) # Without marker, the content of the 3D viewer should be the same for File and All ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; MLAB.processInventorQueue() ctx.field(\u0026#34;SoCameraInteraction.viewFromLeft\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;File\u0026#34; MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_TRUE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) # With marker, the content of the 3D viewer should be different setMarkerPosition(marker_location) ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; MLAB.processInventorQueue() ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;File\u0026#34; ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Segmented\u0026#34; ctx.field(\u0026#34;OffscreenRenderer1.update\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ctx.field(\u0026#34;TutorialSummary.selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; ctx.field(\u0026#34;OffscreenRenderer.update\u0026#34;).touch() MLAB.processInventorQueue() ctx.field(\u0026#34;ImageCompare.compare\u0026#34;).touch() ASSERT_FALSE(ctx.field(\u0026#34;ImageCompare.testPassed\u0026#34;).value) ... Sorting order in TestCaseManager The MeVisLab TestCaseManager sorts your test cases alphabetically. Your test cases should look like this now:\nTestCaseManager Sorting Generally, test cases should not depend on each other and the order of their execution does not matter. Sometimes it makes sense though to execute tests in a certain order, for example for performance reasons. In this case you can add numeric prefixes to your test cases. This might look like this then:\nTestCaseManager Custom Sorting Not testable requirements As already mentioned, some requirements can not be tested in an automated environment. Human eyesight cannot be replaced completely.\nIn our application, the following tests have not been tested automatically:\nRequirement 2: The application shall provide a 2D and a 3D viewer. Requirement 3: The 2D viewer shall display the loaded images Requirement 5: The 2D viewer shall display the segmentation results as a semi-transparent overlay Requirement 6: The 3D viewer shall visualize the loaded data in a 3-dimensional volume rendering Requirement 7: The 3D viewer shall additionally show the segmentation result as a 3-dimensional mesh Test Reports The results of your tests are shown in a Report Viewer. You can also export the results to JUnit for usage in build environments like Jenkins.\nReportViewer Screenshots You can also add screenshots of your inventor scene to the report. Add the following to your Python script wherever you want to capture the content of the SoCameraInteraction module and a Snapshot of your 3D scene is attached to your test report:\n\u0026lt;TEST_CASE_NAME\u0026gt;.py\n... result = ScreenShot.createOffscreenScreenShot(\u0026#34;SoCameraInteraction.self\u0026#34;, \u0026#34;screenshot.png\u0026#34;) Logging.showImage(\u0026#34;My screenshot\u0026#34;, result) Logging.showFile(\u0026#34;Link to screenshot file\u0026#34;, result) ... Summary Define accessible fields for macro modules so that they can be set in Python tests Add outputs to your macro modules for automated testing and connecting testing modules Testcase numbering allows you to sort them and define execution order Info:\u0026nbsp; Additional information about MeVisLab TestCenter can be found in TestCenter Manual \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","Automated Tests","Python"],"section":"tutorials"},{"date":"1674086400","url":"https://mevislab.github.io/examples/tutorials/summary/summary5/","title":"Step 5: Review - Installer creation","summary":"Step 5: Review - Installer creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Your macro module has been tested manually and/or automatically? Then you should create your first installable executable and deliver it to your customer(s) for final evaluation.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK so that you can generate an installer of your developed macro module. Free evaluation licenses of the MeVisLab ApplicationBuilder, time-limited to 3 months, can be requested at sales(at)mevislab.","content":"Step 5: Review - Installer creation \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction Your macro module has been tested manually and/or automatically? Then you should create your first installable executable and deliver it to your customer(s) for final evaluation.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab ApplicationBuilder license. It extends the MeVisLab SDK so that you can generate an installer of your developed macro module. Free evaluation licenses of the MeVisLab ApplicationBuilder, time-limited to 3 months, can be requested at sales(at)mevislab.de. Steps to do Install tools necessary for installer generation The MeVisLab Project Wizard for Standalone Applications [ File \u0026rarr; Run Project Wizard... \u0026rarr; Standalone Application ] provides a check for all necessary tools you need to install before generating an installer.\nMeVisLab Project Wizard Click on Check if required tools are installed. The following dialog opens:\nCheck required tools You can see that NSIS and either Dependency Walker or Dependencies are necessary to create an installable executable. MeVisLab provides information about the necessary version(s).\nDownload and install/extract NSIS and Dependency Walker or Dependencies. Add both executables to your PATH environment variable, for example C:\\Program Files\\depends and C:\\Program Files (x86)\\NSIS.\nRestart MeVisLab and open Project Wizard again. All required tools should now be available.\nUse MeVisLab Project Wizard to generate the installer Select your macro module and the package and click Next.\nWelcome The general settings dialog allows you to define a name for your application. You can also define a version, in our case we decide not to be finished and have a version 0.5. You can include debug files and decide to build a desktop or web application. We want to build an Application Installer for a desktop system. You can decide to precompile your Python files and you have to select your MeVisLab MeVisLab ApplicationBuilder license.\nGeneral Settings Define your license text which is shown during installation of your executable. You can decide to use our pre-defined text, select a custom file or do not include any license text.\nLicense Text The next dialog can be skipped for now, you can include additional files into your installer which are not automatically added by MeVisLab from the dependency analysis.\nManual File Lists Define how the window of your application shall look.\nApplication Options Skip the next dialog, we do not need additional installer options.\nInstaller Options The MeVisLab ToolRunner starts generating your installer. After finishing installer generation, you will find a link to the target directory.\nMeVisLab ToolRunner The directory contains the following files (and some more maybe):\nBatch (*.bat) file Installer (*.exe) file MeVisLab Install (*.mlinstall) file Shell (*.sh) script ThirdParty list (*.csv) Batch file The batch file allows you to generate the executable again via a Windows batch file. You do not need the Project Wizard anymore now.\nInstaller file The resulting installer file for your application is an executable\nMeVisLab Install file The *.mlinstall file provides all information you just entered into the wizard. We will need this in Step 7: Refine - Re-Build Installer again.\nThe file is initially generated by the Project Wizard. Having a valid file already, you can create new versions by using the MeVisLab ToolRunner.\nShell skript The shell skript allows you to generate the executable again via a Unix shell like bash. You do not need the Project Wizard anymore now.\nThirdParty file The third party file contains all third party software tools MeVisLab integrated into your installer from dependency analysis. The file contains the tool name, version, license and general information about the tool.\nInstall your executable You can now execute the installer of your application.\nThe installer initially shows a welcome screen showing the name and version of your application.\nInstaller Next, you will see your selected license agreement from the project wizard and a selection to install for anyone or just for the current user.\nLicense Agreement You can also select to create shortcuts and desktop icons.\nShortcuts and icons The last step is to select the target directory for your application.\nTarget directory After the installer finished the setup, you will find a desktop icon and a start menu entry for your application.\nStartmenu Desktop Licensing:\u0026nbsp; MeVisLab executables require an additional MeVisLab Runtime license. It makes sure that your resulting application needs to be licensed, too. Free evaluation licenses of the MeVisLab ApplicationBuilder and MeVisLab Runtime licenses for testing purposes can be requested at sales(at)mevislab.de. Runtime License After entering your license file, the application runs and you can use it on a customer system.\nInstalled Application Info:\u0026nbsp; By default, your user interface uses a standard stylesheet for colors and appearance of your user interface elements. The style can be customized easily. Summary The MeVisLab ApplicationBuilder allows you to create installable executables from your MeVisLab networks The resulting application can be customized to your needs via Project Wizard Your application will be licensed separately so that you can completely control the usage ","tags":["Advanced","Tutorial","Prototyping","Application Builder","Installer"],"section":"tutorials"},{"date":"1674172800","url":"https://mevislab.github.io/examples/tutorials/summary/summary6/","title":"Step 6: Refine - Update Application","summary":"Step 6: Refine - Update Application \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In previous step you developed an application which can be installed on your customers systems for usage. In this step we are going to integrate simple feedback into our executable and re-create the installer.\nWe want to show you how easy it is to update your application using MeVisLab.\nYour customer requests an additional requirement to define the transparency of your 2D overlay in addition to defining the color.","content":"Step 6: Refine - Update Application \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In previous step you developed an application which can be installed on your customers systems for usage. In this step we are going to integrate simple feedback into our executable and re-create the installer.\nWe want to show you how easy it is to update your application using MeVisLab.\nYour customer requests an additional requirement to define the transparency of your 2D overlay in addition to defining the color.\nRequirement 5.2: It shall be possible to define the alpha value of the overlay Steps to do Adapt your macro module Use the module search to add your macro module to your workspace. We need an additional UI element for setting the alpha value of the overlay.\nRight-click your module and select [ Related Files \u0026rarr; \u0026lt;MACRO_NAME\u0026gt;.script ].\nIn MATE, add another field to your Parameters section and re-use the field by setting the internalName. Add the field to the Settings section of your Window, maybe directly after the color selection.\n\u0026lt;MACRO_NAME\u0026gt;.script\nInterface { ... Parameters { ... Field selectOverlayTransparency { internalName = SoView2DOverlay.alphaFactor } ... } } Window { ... Box Settings { ... Field selectOverlayTransparency { title = Alpha } ... } ... } Back in MeVisLab IDE, your user interface should now provide the possibility to define an alpha value of the overlay. Changes are applied automatically because you re-used the field of the SoView2DOverlay module directly.\nUpdated User Interface You can also update your Python files for new or updated requirements. In this example we just want to show the basic principles, therefore we only add this new element to the Script file.\nIf you want to write an additional Python test case, you can also do that.\nSummary Your application can be updated by modifying the macro module and/or internal network of your application Any changes will be applied to your installable executable in the next step \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping"],"section":"tutorials"},{"date":"1674259200","url":"https://mevislab.github.io/examples/tutorials/summary/summary7/","title":"Step 7: Refine - Re-Build Installer","summary":"Step 7: Refine - Re-Build Installer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step you are re-creating your application installer after changing the UI in previous Step 6: Refine - Update Application.\nSteps to do Update the *.mlinstall file You do not need to use the Project Wizard now, because you already have a valid *.mlinstall file. The location should be in your package, under .\\Configuration\\Installers\\TutorialSummary. Open the file in any text editor and search for the $VERSION 0.","content":"Step 7: Refine - Re-Build Installer \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction In this step you are re-creating your application installer after changing the UI in previous Step 6: Refine - Update Application.\nSteps to do Update the *.mlinstall file You do not need to use the Project Wizard now, because you already have a valid *.mlinstall file. The location should be in your package, under .\\Configuration\\Installers\\TutorialSummary. Open the file in any text editor and search for the $VERSION 0.5. Change the version to something else, in our case we now have our first major release 1.0.\nInfo:\u0026nbsp; You can also run the Project Wizard again but keep in mind that manual changes on your *.mlinstall file might be overwritten. The wizard re-creates your *.mlinstall file whereas the ToolRunner just uses it. Use MeVisLab ToolRunner Save the file and open MeVisLab ToolRunner.\nMeVisLab ToolRunner Open the *.mlinstall file in ToolRunner and select the file. Click Run on Selection.\nRun on Selection The ToolRunner automatically builds your new installer using version 1.0.\nInstall application again Execute your installable executable again. You do not have to uninstall previous version(s) of your application first. Already existing applications will be replaced by new installation - at least if you select the same target directory.\nInstall new version The installer already shows your updated version 1.0. It is not necessary to select your Runtime license again because it has not been touched during update.\nApplication version 1.0 The new installed application now provides your new UI element for defining the alpha value of the overlay.\nSummary Updates of your application installer can be applied by using the MeVisLab ToolRunner The executable can be updated on your customers system(s) and your changes on the macro module and network(s) are applied ","tags":["Advanced","Tutorial","Prototyping","Tool Runner","Installer"],"section":"tutorials"},{"date":"1677196800","url":"https://mevislab.github.io/examples/tutorials/summary/summary8/","title":"Extra: Run your application in Browser","summary":"Extra: Run your application in Browser \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This step explains how to run your developed application in a browser. The MeVisLab network remains the same, only some adaptations are necessary for running any macro module in a browser window.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab Webtoolkit license. It extends the MeVisLab SDK so that you can develop web macro modules. Free evaluation licenses of the MeVisLab Webtoolkit, time-limited to 3 months, can be requested at sales(at)mevislab.","content":"Extra: Run your application in Browser \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Introduction This step explains how to run your developed application in a browser. The MeVisLab network remains the same, only some adaptations are necessary for running any macro module in a browser window.\nLicensing:\u0026nbsp; This step requires a valid MeVisLab Webtoolkit license. It extends the MeVisLab SDK so that you can develop web macro modules. Free evaluation licenses of the MeVisLab Webtoolkit, time-limited to 3 months, can be requested at sales(at)mevislab.de. Steps to do Make sure to have your macro module from previous Step 2 available.\nCreate a Web macro module Open Project Wizard via [ File \u0026rarr; Run Project Wizard... ] and select Web Macro module. Run the Wizard and enter details of your new macro module.\nWeb macro module wizard Run the wizard and enter details of your web macro module.\nWeb macro module properties Click Next and select optional web plugin features. Click Create.\nWeb macro module The folder of your project automatically opens in explorer window.\nUsing your web macro module As you created a global web macro module, you can search for it in the MeVisLab Module Search. In case the module cannot be found, select [ Extras \u0026rarr; Reload Module Database (Clear Cache) ].\nWeb macro module The internal network of your module is empty. We will re-use the internal network of your macro module developed in Step 2.\nAdd internal network of your application Open the internal network of your previously created macro module from Step 2. Select all and copy to your internal network of the TutorialSummaryBrowser module. Save the internal network and close the tab in MeVisLab.\nInternal network We are going to develop a web application, therefore we need special RemoteRendering modules for the viewer. Add 2 RemoteRendering modules and a SoCameraInteraction to your workspace and connect them to your existing modules as seen below.\nRemote Rendering Additional Info:\u0026nbsp; We are using the hidden outputs of the View2D and the SoExaminerViewer. You can show them by pressing the SPACE key. Develop the user interface Make sure to have both macro modules visible in MeVisLab SDK, we are re-using the *.script and *.py files developed in Step 3.\nMacro modules Right-click the module TutorialSummaryBrowser and select [ Related Files \u0026rarr; TutorialSummaryBrowser.script ].\nThe file opens in MATE and you will see that it looks similar to the *.script file of a normal macro module. The only difference is an additional Web section at the end of the file. It defines the locations of some javascript libraries and the url to be used for a preview of your website.\nTutorialSummaryBrowser.script\nWeb { plugin = \u0026#34;$(MLAB_MeVisLab_Private)/Sources/Web/application/js/jquery/Plugin.js\u0026#34; plugin = \u0026#34;$(MLAB_MeVisLab_Private)/Sources/Web/application/js/yui/Plugin.js\u0026#34; // Specify web plugins here. If you have additional Javascript files, you can load them from // the plugin. It is also possible to load other plugins here. plugin = \u0026#34;$(LOCAL)/www/js/Plugin.js\u0026#34; Deployment { // Deploy the www directory recursively when building web application installers directory = \u0026#34;$(LOCAL)/www\u0026#34; } // The developer url is used by the startWorkerService.py user script. developerUrl = \u0026#34;MeVis/TutorialSummary/Projects/TutorialSummaryBrowser/Modules/www/TutorialSummaryBrowser.html\u0026#34; } Open the script file of the TutorialSummary module from Step 3. Copy the output section to your web macro and define internalName as the output of your RemoteRendering modules.\nYou can also copy all fields from Parameters section to your web macro module script.\nTutorialSummaryBrowser.script\nInterface { Inputs {} Outputs { Field out2D { internalName = RemoteRendering2D.output } Field out3D { internalName = RemoteRendering3D.output } Field outSegmentationMask { internalName = CloseGap.output0 } } Parameters { Field openFile { type = String internalName = LocalImage.name } Field selectOverlayColor { internalName = SoView2DOverlay.baseColor type = Color } Field selectOverlayTransparency { internalName = SoView2DOverlay.alphaFactor } Field imageAlpha { internalName = SoWEMRendererImage.faceAlphaValue type = Integer min = 0 max = 1 } Field thresholdInterval { internalName = RegionGrowing.autoThresholdIntervalSizeInPercent type = Integer min = 0 max = 100 } Field isoValueImage { internalName = IsoSurfaceImage.isoValue type = Integer min = 0 max = 1000 } Field selected3DView { type = Enum values = Segmented,File,Both } Field totalVolume { internalName = CalculateVolume.totalVolume editable = False } Field resetApplication { type = Trigger title = Reset } Field markerPosition { type = Vector3 } Field applyMarker { type = Trigger title = Add } } } Reloading your web macro in MeVisLab SDK now shows the same outputs as the original macro module. The only difference is the type of your output. It changed from MLImage and Inventor Scene to MLBase from your RemoteRendering modules.\nMacro modules The internal network of your web macro should look like this:\nMacro modules You can emulate the final viewer by adding a RemoteRenderingClient module to the outputs of your web macro.\nRemoteRenderingClient Open the *.script files of your macro modules and copy the FieldListeners from Commands section of your TutorialSummary.script to TutorialSummaryBrowser.script.\nTutorialSummaryBrowser.script\nCommands { source = $(LOCAL)/TutorialSummaryBrowser.py FieldListener selected3DView { command = viewSelectionChanged } FieldListener resetApplication { command = resetApplication } FieldListener markerPosition { command = insertPosition } FieldListener applyMarker { command = applyPosition } } Also copy the Window section to your web macro module. The Box of the Viewing tab needs to be modified because we are now using the RemoteRendering outputs instead of the View3D and SoExaminerViewer outputs.\nTutorialSummaryBrowser.script\nWindow \u0026#34;MainPanel\u0026#34; { // Define minimum width and height minimumWidth = 400 minimumHeight = 300 initCommand = resetApplication // Vertical Layout and 4 Boxes with Horizontal Layout Vertical { Box Source { layout = Horizontal Field openFile { browseButton = True browseMode = open } Field resetApplication { } } Box Viewing { layout = Horizontal RemoteRendering out2D { expandX = True expandY = True } RemoteRendering out3D { expandX = True expandY = True } } Box Settings { layout = Horizontal Field selectOverlayColor { title = Color } Field selectOverlayTransparency { title = Alpha } Field imageAlpha { step = 0.1 slider = True } Field thresholdInterval { step = 0.1 slider = True } Field isoValueImage { step = 2 slider = True } Field markerPosition {} Field applyMarker {} ComboBox selected3DView { alignX = Left editable = False } } Box Info { layout = Horizontal Field totalVolume {} } } } Python functions After we re-used the scripts, we now need to copy the Python functions from TutorialSummary.py to TutorialSummaryBrowser.py. Open the Python file of your web macro. You will see an additional import from MLABRemote, which is required for remote rendering calls. The MLABRemote context is already setup automatically and can be used.\nTutorialSummaryBrowser.py\nfrom mevis import * from MLABRemote import MLABRemote, allowedRemoteCall MLABRemote.setup(ctx) Copy the Python functions from TutorialSummary.py to TutorialSummaryBrowser.py. They can remain unchanged but require an additional @allowedRemoteCall function. This is necessary to explicitly allow remote execution of the function and is disabled by default for security reasons.\nTutorialSummaryBrowser.py\nfrom mevis import * from MLABRemote import MLABRemote, allowedRemoteCall MLABRemote.setup(ctx) @allowedRemoteCall def viewSelectionChanged(field): if field.value == \u0026#34;Segmented\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 0 if field.value == \u0026#34;File\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 1 if field.value == \u0026#34;Both\u0026#34;: ctx.field(\u0026#34;SoSwitch.whichChild\u0026#34;).value = 2 @allowedRemoteCall def resetApplication(): ctx.field(\u0026#34;RegionGrowing.clear\u0026#34;).touch() ctx.field(\u0026#34;SoView2DMarkerEditor.deleteAll\u0026#34;).touch() ctx.field(\u0026#34;LocalImage.close\u0026#34;).touch() ctx.field(\u0026#34;imageAlpha\u0026#34;).value = 0.5 ctx.field(\u0026#34;thresholdInterval\u0026#34;).value = 1.0 ctx.field(\u0026#34;isoValueImage\u0026#34;).value = 200 ctx.field(\u0026#34;selected3DView\u0026#34;).value = \u0026#34;Both\u0026#34; @allowedRemoteCall def insertPosition(field): ctx.field(\u0026#34;SoView2DMarkerEditor.newPosXYZ\u0026#34;).value = field.value @allowedRemoteCall def applyPosition(): ctx.field(\u0026#34;SoView2DMarkerEditor.useInsertTemplate\u0026#34;).value = True ctx.field(\u0026#34;SoView2DMarkerEditor.add\u0026#34;).touch() Run your application in browser MeVisLab provides a local webserver and you can preview your application in a browser by selecting the module and open [ Scripting \u0026rarr; Web \u0026rarr; Start Module Through Webservice ]. The integrated webserver starts and your default browser opens the local website showing your application.\nWebserver preview Select your web macro TutorialSummaryBrowser and right-click to select [ Related Files \u0026rarr; Show Definition Folder ]. You can see the folder structure of your web macro and modify the stylesheet depending on your needs.\nOpen current web instance in MeVisLab SDK If you want to inspect the internal state of the modules and your internal network, open the console of your browser and enter MLAB.GUI.Application.module(\u0026lsquo;TutorialSummaryBrowser\u0026rsquo;).showIDE(). MeVisLab opens and you can change your internal network while all modifications are applied on the website on-the-fly.\nMeVisLab SDK Summary MeVisLab macro modules can easily be adapted to run in a browser window MeVisLab RemoteRendering allows to run in a browser or embedded into other application user interfaces. It does so by sending updated images to a client and receiving input events from this client. Clients can be emulated by using a RemoteRenderingClient module. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","Prototyping","Browser","Web"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/thirdparty/","title":"Chapter VIII: ThirdParty components","summary":"MeVisLab Tutorial Chapter VIII Using ThirdParty software integrated into MeVisLab MeVisLab is equipped with a lot of useful software right out of the box, like the Insight Segmentation and Registration Toolkit (ITK) or the Visualization Toolkit (VTK). This chapter works as a guide on how to use some of the third party components integrated in MeVisLab for your projects via Python scripting. Additional Information:\u0026nbsp; You will also find instructions to install and use any Python package (e.","content":"MeVisLab Tutorial Chapter VIII Using ThirdParty software integrated into MeVisLab MeVisLab is equipped with a lot of useful software right out of the box, like the Insight Segmentation and Registration Toolkit (ITK) or the Visualization Toolkit (VTK). This chapter works as a guide on how to use some of the third party components integrated in MeVisLab for your projects via Python scripting. Additional Information:\u0026nbsp; You will also find instructions to install and use any Python package (e.g. PyTorch) in MeVisLab using the PythonPip module. OpenCV OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV includes, among others, algorithms to:\ndetect and recognize faces identify objects classify human actions on video track camera movements track moving objects extract 3D models of objects produce 3D point clouds from stereo cameras stitch images together to produce a high resolution image of an entire scene find similar images from an image database remove red eyes from images taken using flash follow eye movements recognize scenery establish markers to overlay with augmented reality assimp The THE ASSET IMPORTER LIBRARY supports loading and processing geometric scenes from various well known 3D formats. MeVisLab uses assimp to import these files and reuses the scenes directly in MeVisLab.\nA list of supported formats can be found here.\nPyTorch [not integrated initially] PyTorch is a machine learning framework based on the Torch library, used for applications such as Computer Vision and Natural Language Processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.\nThe tutorials available here shall provide examples on how to integrate AI into MeVisLab. You can also integrate other Python AI packages the same way.\nmatplotlib Matplotlib is a library for creating static, animated, and interactive visualizations in Python.\ncreate publication quality plots Make interactive figures that can be zoomed, panned and updated Customize visual style and layout Export to many file formats ","tags":["Advanced","Tutorial","ThirdParty"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/thirdparty/opencv/","title":"OpenCV","summary":"Open Source Computer Vision Library (OpenCV) Introduction OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\nThis chapter provides some examples how to use OpenCV in MeVisLab.\nOther resources You can find a lot of OpenCV examples and tutorials on their website.","content":"Open Source Computer Vision Library (OpenCV) Introduction OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\nThis chapter provides some examples how to use OpenCV in MeVisLab.\nOther resources You can find a lot of OpenCV examples and tutorials on their website.\n","tags":["Advanced","Tutorial","OpenCV","Python"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/thirdparty/opencv/thirdpartyexample1/","title":"Example 1: WebCam access with OpenCV","summary":"Example 1: WebCam access with OpenCV Introduction In this example, we are using the PythonImage module and access your WebCam to show the video in a View2D.\nSteps to do Creating the network to be used for testing Add the modules to your workspace and connect them as seen below.\nExample Network The viewer is empty because the image needs to be set via Python scripting.\nInfo:\u0026nbsp; More information about the PythonImage module can be found here Create a macro module Now you need to create a macro module from your network.","content":"Example 1: WebCam access with OpenCV Introduction In this example, we are using the PythonImage module and access your WebCam to show the video in a View2D.\nSteps to do Creating the network to be used for testing Add the modules to your workspace and connect them as seen below.\nExample Network The viewer is empty because the image needs to be set via Python scripting.\nInfo:\u0026nbsp; More information about the PythonImage module can be found here Create a macro module Now you need to create a macro module from your network. You can either group your modules, create a local macro and convert it to a global macro module, or you use the Project Wizard and load your *.mlab file.\nInfo:\u0026nbsp; A tutorial how to create your own macro modules can be found in Example 2.2: Global macro modules. Make sure to add a Python file to your macro module. Add the View2D to your UI Next, we need to add the View2D to a Window of your macro module. Right click on your module , open the context menu and select [ Related Files \u0026rarr; \u0026lt;YOUR_MODULE_NAME\u0026gt;.script ]. The text editor MATE opens. You can see the *.script file of your module.\nAdd the following to your file: \u0026lt;YOUR_MODULE_NAME\u0026gt;.script\nInterface { Inputs {} Outputs {} Parameters {} } Commands { source = $(LOCAL)/\u0026lt;YOUR_MODULE_NAME\u0026gt;.py } Window { h = 500 w = 500 initCommand = setupInterface destroyedCommand = releaseCamera Vertical { Horizontal { Button { title = Start command = startCapture } Button { title = Pause command = stopCapture } } Horizontal { expandX = True expandY = True Viewer View2D.self { type = SoRenderArea } } } } Now open the Python file of your module and define the commands to be called from the *.script file: \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# from mevis import * # Setup the interface for PythonImage module def setupInterface(): pass # Release camera in the end def releaseCamera(_): pass # Start capturing WebCam def startCapture(): pass # Stop capturing WebCam def stopCapture(): pass Use OpenCV Your View2D is still empty, lets get access to the WebCam and show the video in your module. Open the Python file of your network again and enter the following code: \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# from mevis import * import cv2 import OpenCVUtils _interfaces = [] camera = None # Setup the interface for PythonImage module def setupInterface(): global _interfaces _interfaces = [] interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) _interfaces.append(interface) # Release camera in the end def releaseCamera(_): pass # Start capturing WebCam def startCapture(): pass # Stop capturing WebCam def stopCapture(): pass We now imported cv2 and OpenCVUtils so that we can use them in Python. Additionally we defined a list of _interfaces and a camera. The import of mevis is not necessary for this example.\nThe setupInterfaces function is called whenever the Window of your module is opened. Here we are getting the interface of the PythonImage module and append it to our global list.\nAccess the WebCam Now we want to start capturing the camera. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# Start capturing WebCam def startCapture(): global camera if not camera: camera = cv2.VideoCapture(0) ctx.callWithInterval(0.1, grabImage) # Grab image from camera and update def grabImage(): _, img = camera.read() updateImage(img) # Update image in interface def updateImage(image): _interfaces[0].setImage(OpenCVUtils.convertImageToML(image), minMaxValues = [0,255]) The startCapture function gets the camera from the cv2 object if not already available. Then it calls the current MeVisLab network context and creates a timer which calls a grabImage function every 0.1 seconds.\nThe grabImage function reads an image from the camera and calls updateImage. The interface from the PythonImage module is used to set the image from the WebCam. The MeVisLab OpenCVUtils convert the OpenCV image to the MeVisLab image format MLImage.\nNext, we define what happens if you click the Pause button. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n... # Stop capturing WebCam def stopCapture(): ctx.removeTimers() ... As we started a timer in our network context which updates the image every 0.1 seconds, we just stop this timer and the camera is paused.\nIn the end, we need to release the camera whenever you close the Window of your macro module. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n... # Release camera in the end def releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None ... Again, the timers are removed, all interfaces are reset and the camera is released. The light indicating WebCam usage should turn off.\nOpening your macro module via double-click should now allow to start and pause your WebCam video in MeVisLab. You can modify your internal network using a Convolution filter module or any other module available in MeVisLab for modifying the stream on the fly.\nSummary The PythonImage module allows to use Python for defining the image output OpenCV can be used in MeVisLab via Python scripting Images and videos from OpenCV can be used in MeVisLab networks \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","OpenCV","Python","WebCam","Macro","Macro modules","Global Macro"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/thirdparty/opencv/thirdpartyexample2/","title":"Example 2: Face Detection with OpenCV","summary":"Example 2: Face Detection with OpenCV Introduction This example uses the OpenCV WebCam Python script and adds a basic face detection.\nInfo:\u0026nbsp; The Python code used in this example has been taken from Towards Data Science. Steps to do Open Example 1 Add the macro module developed in Example 1 to your workspace.\nDownload trained classifier XML file Initially you need to download the trained classifier XML file. It is available in the OpenCV GitHub repository.","content":"Example 2: Face Detection with OpenCV Introduction This example uses the OpenCV WebCam Python script and adds a basic face detection.\nInfo:\u0026nbsp; The Python code used in this example has been taken from Towards Data Science. Steps to do Open Example 1 Add the macro module developed in Example 1 to your workspace.\nDownload trained classifier XML file Initially you need to download the trained classifier XML file. It is available in the OpenCV GitHub repository. Save the file somewhere and remember the path for later usage in Python.\nExtend Python file Right click on your module , open the context menu and select [ Related Files \u0026rarr; \u0026lt;YOUR_MODULE_NAME\u0026gt;.py ]. The text editor MATE opens. You can see the Python file of your module.\nYou have to load the previously downloaded XML file first. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\n# from mevis import * import cv2 import OpenCVUtils _interfaces = [] camera = None face_cascade = cv2.CascadeClassifier(\u0026#39;\u0026lt;YOUR_PATH\u0026gt;/haarcascade_frontalface_default.xml\u0026#39;) After loading the file, go to the previously implemented grabImage function and extend it as follows: \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\ndef grabImage(): _, img = camera.read() updateImage(img) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray, 1.1, 4) for (x, y, w, h) in faces: cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) # Display the output cv2.imshow(\u0026#39;img\u0026#39;, img) In the end, destroy all OpenCV windows in releaseCamera function. \u0026lt;YOUR_MODULE_NAME\u0026gt;.py\ndef releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None cv2.destroyAllWindows() Opening your macro module and pressing Start should now open your WebCam stream and an additional OpenCV window which shows a blue rectangle around a detected face.\nFace Detection in MeVisLab using OpenCV Summary This is just one example for using OpenCV in MeVisLab. You will find lots of other examples and tutorials online, we just wanted to show one possibility.\nInfo:\u0026nbsp; You can download the Python file here ","tags":["Advanced","Tutorial","OpenCV","Python","WebCam","Face Detection"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/thirdparty/assimp/","title":"assimp","summary":"Asset-Importer-Lib (assimp) Introduction Assimp (Asset-Importer-Lib) is a library to load and process geometric scenes from various 3D data formats.\nThis chapter provides some examples of how 3D formats can be imported into MeVisLab. In general you always need a SoSceneLoader module. The SoSceneLoader allows to load meshes as Open Inventor points/lines/triangles/faces using the Open Asset Import Library.\nSoSceneLoader You can also use the SoSceneWriter module to export your 3D scenes from MeVisLab into any of the output formats listed below.","content":"Asset-Importer-Lib (assimp) Introduction Assimp (Asset-Importer-Lib) is a library to load and process geometric scenes from various 3D data formats.\nThis chapter provides some examples of how 3D formats can be imported into MeVisLab. In general you always need a SoSceneLoader module. The SoSceneLoader allows to load meshes as Open Inventor points/lines/triangles/faces using the Open Asset Import Library.\nSoSceneLoader You can also use the SoSceneWriter module to export your 3D scenes from MeVisLab into any of the output formats listed below.\nFile formats The Assimp-Lib currently supports the following file formats:\n3D Manufacturing Format (.3mf) Collada (.dae, .xml) Blender (.blend) Biovision BVH (.bvh) 3D Studio Max 3DS (.3ds) 3D Studio Max ASE (.ase) glTF (.glTF) glTF2.0 (.glTF) KHR_lights_punctual ( 5.0 ) KHR_materials_pbrSpecularGlossiness ( 5.0 ) KHR_materials_unlit ( 5.0 ) KHR_texture_transform ( 5.1 under test ) FBX-Format, as ASCII and binary (.fbx) Stanford Polygon Library (.ply) AutoCAD DXF (.dxf) IFC-STEP (.ifc) Neutral File Format (.nff) Sense8 WorldToolkit (.nff) Valve Model (.smd, .vta) Quake I (.mdl) Quake II (.md2) Quake III (.md3) Quake 3 BSP (.pk3) RtCW (.mdc) Doom 3 (.md5mesh, .md5anim, .md5camera) DirectX X (.x) Quick3D (.q3o, .q3s) Raw Triangles (.raw) AC3D (.ac, .ac3d) Stereolithography (.stl) Autodesk DXF (.dxf) Irrlicht Mesh (.irrmesh, .xml) Irrlicht Scene (.irr, .xml) Object File Format ( .off ) Wavefront Object (.obj) Terragen Terrain ( .ter ) 3D GameStudio Model ( .mdl ) 3D GameStudio Terrain ( .hmp ) Ogre ( .mesh.xml, .skeleton.xml, .material ) OpenGEX-Fomat (.ogex) Milkshape 3D ( .ms3d ) LightWave Model ( .lwo ) LightWave Scene ( .lws ) Modo Model ( .lxo ) CharacterStudio Motion ( .csm ) Stanford Ply ( .ply ) TrueSpace (.cob, .scn) XGL-3D-Format (.xgl) ","tags":["Beginner","Tutorial","assimp","3D"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/tutorials/thirdparty/assimp/assimpexample1/","title":"Example 1: 3D Printing in MeVisLab","summary":"Example 1: 3D Printing in MeVisLab Introduction This example uses the assimp library to load a 3D file and save the file as *.stl for 3D printing.\nSteps to do Develop your network Add the modules SoSceneLoader, SoBackground and SoExaminerViewer to your workspace and connect them as seen below.\nExample Network Open the 3D file Select the file vtkCow.obj from MeVisLab demo data directory. Open SoExaminerViewer and inspect the scene. You will see a 3D cow.","content":"Example 1: 3D Printing in MeVisLab Introduction This example uses the assimp library to load a 3D file and save the file as *.stl for 3D printing.\nSteps to do Develop your network Add the modules SoSceneLoader, SoBackground and SoExaminerViewer to your workspace and connect them as seen below.\nExample Network Open the 3D file Select the file vtkCow.obj from MeVisLab demo data directory. Open SoExaminerViewer and inspect the scene. You will see a 3D cow.\nInfo:\u0026nbsp; In case you cannot see the cow, it might be located outside your current camera location. Trigger the field rescanScene in case the cow is not visible. Cow in SoExaminerViewer Add a SoSphere to the workspace and connect it to your viewer. Define the Radius of your sphere to 2 and inspect your viewer.\nCow and Sphere in SoExaminerViewer You can also define a material for your sphere but what we wanted to show is: You can use the loaded 3D files in MeVisLab Open Inventor Scenes.\nCow and red Sphere in SoExaminerViewer Save your scene as *.stl file for 3D Printing Add a SoSceneWriter module to your workspace. The SoExaminerViewer has a hidden output which can be shown on pressing SPACE . Connect the SoSceneWriter to the output.\nName your output *.stl file and select Stl Ascii as output format so that we can inspect the result afterwards.\nSoSceneWriter Info:\u0026nbsp; The SoSceneWriter can save node color information when saving in Inventor (ASCII or binary) or in VRML format. The SoSceneWriter needs to be attached to a SoWEMRenderer that renders in ColorMode:NodeColor.\nThere are tools to convert from at least VRML to STL available for free.\nWrite your Scene and open the resulting file in your preferred editor. As an alternative, you can also open the file in an *.stl file reader like Microsoft 3D-Viewer.\nMicrosoft 3D-Viewer Load the file again For loading your *.stl file, you can use a SoSceneLoader and a SoExaminerViewer.\nInfo:\u0026nbsp; More information about the *.stl format can be found here SoSceneLoader Summary MeVisLab is able to load and write many different 3D file formats including *.stl format for 3D Printing. Inventor Scenes can be saved by using a SoExaminerViewer together with a SoSceneWriter ","tags":["Beginner","Tutorial","assimp","3D","3D Printing","stl"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/tutorials/thirdparty/pytorch/","title":"PyTorch","summary":"PyTorch Introduction PyTorch is a machine learning framework based on the Torch library, used for applications such as Computer Vision and Natural Language Processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.\nA lot of AI frameworks can be used within MeVisLab. We currently do not provide a preintegrated AI framework though as we try to avoid compatibility issues, and AI frameworks are very fast-moving by nature.","content":"PyTorch Introduction PyTorch is a machine learning framework based on the Torch library, used for applications such as Computer Vision and Natural Language Processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.\nA lot of AI frameworks can be used within MeVisLab. We currently do not provide a preintegrated AI framework though as we try to avoid compatibility issues, and AI frameworks are very fast-moving by nature.\nMaybe also take a look at:\nTensorFlow Keras scikit-learn Attention:\u0026nbsp; We are not explaining PyTorch itself. These tutorials are examples for how to integrate and use PyTorch in MeVisLab. Detailed tutorials for using PyTorch can be found here. Available Tutorials Install PyTorch by using the PythonPip module The first example shows how to install torch and torchvision by using the MeVisLab module PythonPip. This module can be used to install Python packages not integrated into MeVisLab.\nUse trained PyTorch networks in MeVisLab In this example, we are using a pre-trained network from torch.hub to generate an AI based image overlay of a brain parcellation map.\nSegment persons in webcam videos The second tutorial adapts the Example 2: Face Detection with OpenCV to segment a person shown in a webcam stream. The network has been taken from torchvision.\n","tags":["Advanced","Tutorial","PyTorch","AI"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/tutorials/thirdparty/pytorch/pytorchexample1/","title":"Example 1: Installing PyTorch using the PythonPip module","summary":"Example 1: Installing PyTorch using the PythonPip module Introduction The module PythonPip allows you to install additional Python packages to be used in MeVisLab.\nWarning:\u0026nbsp; You should not use the general Python pip command from a locally installed Python, because MeVisLab will not know these packages and they cannot be used in MeVisLab directly. The module either allows to install packages into the global MeVisLab installation directory, or into your defined user package.","content":"Example 1: Installing PyTorch using the PythonPip module Introduction The module PythonPip allows you to install additional Python packages to be used in MeVisLab.\nWarning:\u0026nbsp; You should not use the general Python pip command from a locally installed Python, because MeVisLab will not know these packages and they cannot be used in MeVisLab directly. The module either allows to install packages into the global MeVisLab installation directory, or into your defined user package. We will use the user package directory, because then the installed packages remain available in your packages even if you uninstall or update MeVisLab. In addition to that, no administrative rights are necessary if you did install MeVisLab for all users.\nWarning:\u0026nbsp; Installing additional Python packages into MeVisLab by using the PythonPip module requires administrative rights if you do not install into a user package. In addition to that, the installed packages are removed when uninstalling MeVisLab. Steps to do The PythonPip module Add a PythonPip module to your workspace.\nPythonPip module Double-click the module and inspect the panel.\nPythonPip panel The panel shows all currently installed Python packages including their version and the MeVisLab package they are saved in. You can see a warning that the target package is set to read-only in case you are selecting a MeVisLab package. Changing to one of your user packages (see Example 2.1: Package creation for details) makes the warning disappear.\nSelect user package Additional information:\u0026nbsp; Additional Information on the PythonPip module can be found in Example 4: Install additional Python packages via PythonPip module. Install torch and torchvision For our tutorials, we need to install torch and torchvision. Enter torch torchvision into the Command textbox and press Install.\nInfo:\u0026nbsp; We are using the CPU version of PyTorch for our tutorials as we want them to be as accessible as possible. If you happen to have a large GPU capacity (and CUDA support) you can also use the GPU version. You can install the necessary packages by using the PyTorch documentation available here. Continuing with CUDA support: Command\ntorch torchvision --index-url https://download.pytorch.org/whl/cu117 Attention:\u0026nbsp; If you are behind a proxy server, you may have to set the HTTP_PROXY and HTTPS_PROXY environment variables to the hostname and port of your proxy. These are used by pip when accessing the internet.\nAlternatively you can also add a parameter to pip install command: \u0026ndash;proxy https://proxy:port\nInstall torch and torchvision After clicking Install, the pip console output opens and you can follow the process of the installation.\nPython pip output After the installation was finished with exit code 0, you should see the new packages in the PythonPip module.\nPyTorch installed Summary PyTorch can be installed using the PythonPip module. There are different versions available (CPU and GPU) depending on the hardware that is used Additional steps have to be taken depending on the version one wishes to install The module displays newly installed packages as soon as the installation was successful ","tags":["Advanced","Tutorial","PyTorch","Python","PythonPip","AI"],"section":"tutorials"},{"date":"1688083200","url":"https://mevislab.github.io/examples/tutorials/thirdparty/pytorch/pytorchexample2/","title":"Example 2: Brain Parcellation using PyTorch","summary":"Example 2: Brain Parcellation using PyTorch Introduction In this example, you are using a pre-trained PyTorch deep learning model (HighRes3DNet) to perform a full brain parcellation. HighRes3DNet is a 3D residual network presented by Li et al. in On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task.\nSteps to do Add a LocalImage module to your workspace and select the file MRI_Head.dcm. For PyTorch it is necessary to resample the data to a defined size.","content":"Example 2: Brain Parcellation using PyTorch Introduction In this example, you are using a pre-trained PyTorch deep learning model (HighRes3DNet) to perform a full brain parcellation. HighRes3DNet is a 3D residual network presented by Li et al. in On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task.\nSteps to do Add a LocalImage module to your workspace and select the file MRI_Head.dcm. For PyTorch it is necessary to resample the data to a defined size. Add a Resample3D module to the LocalImage and open the panel. Change Keep Constant to Voxel Size and define Image Size as 176, 217, 160.\nResample3D module .\nThe coordinates in PyTorch are also a little different than in MeVisLab, therefore you have to rotate the image. Add an OrthoSwapFlip module and connect it to the Resample3D module. Change View to Other and set Orientation to YXZ. Also check Flip horizontal, Flip vertical and Flip depth. Apply your changes.\nOrthoSwapFlip module .\nYou can use the Output Inspector to see the changes on the images after applying the resample and a swap or flip.\nOriginal Resample3D OrthoSwapFlip Add an OrthoView2D module to your network and save the *.mlab file.\nOrthoView2D module .\nIntegrate PyTorch and scripting For integrating PyTorch and Python scripting, we need a PythonImage module. Add it to your workspace. Right-click on the PythonImage module and select [ Grouping \u0026rarr; Add to new Group... ]. Right-click your new group and select [ Grouping \u0026rarr; Add to new Group... ]. Name your new local macro DemoAI, select a directory for your project and leave all settings as default.\nOur new module does not provide an input or output.\nDemoAI local macro .\nAdding an interface to the local macro Right-click the local macro and select [ Related Files \u0026rarr; DemoAI.script ]. MATE opens showing the *.script file of our module. Add an input Field of type Image, an output Field using the internalName of the output of our PythonImage and a Trigger to start the segmentation.\nYou should also already add a Python file in the Commands section.\nDemoAI.script\nInterface { Inputs { Field inputImage { type = Image } } Outputs { Field outImage { internalName = PythonImage.output0 } } Parameters { Field start { type = Trigger } } } Commands { source = $(LOCAL)/DemoAI.py } In MATE, right-click the Project Workspace and add a new file DemoAI.py to your project. The workspace now contains an empty Python file.\nProject Workspace .\nChange to MeVisLab IDE, right-click the local macro and select [ Reload Definition ]. Your new input and output interface are now available and you can connect images to your module.\nDemoAI local macro with interfaces .\nExtend your network We want to show the segmentation results as an overlay on the original image. Add a SoView2DOverlayMPR module and connect it to your DemoAI macro. Connect the output of the SoView2DOverlayMPR to a SoGroup. We also need a lookup table for the colors to be used for the overlay. We already prepared a *.xml file you can simply use. Download the lut.xml file and save it in your current working directory of the project.\nAdd a LoadBase module and connect it to a SoMLLUT module. The SoMLLUT needs to be connected to the SoGroup so that it is applied to our segmentation results.\nFinal network .\nInfo:\u0026nbsp; If your PC is equipped with less than 16GBs of RAM/working memory we recommend to add a SubImage module between the OrthoSwapFlip and the Resample3D module. You should configure less slices in z-direction to prevent your system from running out of memory.\nSubImage module .\nInspect the output of the LoadBase module in the Output Inspector to see if the lookup table has been loaded correctly.\nLUT in LoadBase .\nWrite Python script You can now execute the pre-trained PyTorch network on your image. Right-click the local macro and select [ Related Files \u0026rarr; DemoAI.script ]. The Python function is supposed to be called whenever the Trigger is touched.\nAdd the following code to your Commands section:\nDemoAI.script\nCommands { source = $(LOCAL)/DemoAI.py FieldListener start { command = onStart } } The FieldListener always calls the Python function onStart when the Trigger start is touched. We now need to implement the Python function. Right-click the command onStart and select [ Create Python Function \u0026#39;onStart\u0026#39; ].\nThe Python file opens automatically and the function is created.\nDemoAI.py\nimport torch def onStart(): # Step 1: Get input image image = ctx.field(\u0026#34;inputImage\u0026#34;).image() imageArray = image.getTile((0, 0, 0, 0, 0, 0), image.imageExtent()) inputImage = imageArray[0,0,0,:,:,:].astype(\u0026#34;float\u0026#34;) # Step 2: Normalize input image values = inputImage[inputImage \u0026gt; inputImage.mean()] inputImage = (inputImage - values.mean()) / values.std() # Step 3: Convert into torch tensor of size: [Batch, Channel, z, y, x] inputTensor = torch.Tensor(inputImage[None, None, :, :, :]) # Step 4: Load and prepare AI model device = torch.device(\u0026#34;cpu\u0026#34;) model = torch.hub.load(\u0026#34;fepegar/highresnet\u0026#34;, \u0026#34;highres3dnet\u0026#34;, pretrained=True, trust_repo=True) model.to(device).eval() output = model(inputTensor.to(device)) brainParcellationMap = output.argmax(dim=1, keepdim=True).cpu()[0] print(\u0026#39;...done.\u0026#39;) # Step 6: Set output image to module interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) interface.setImage(brainParcellationMap.numpy(), voxelToWorldMatrix=image.voxelToWorldMatrix()) Warning:\u0026nbsp; When executing your script for the first time, you will get a ScriptError message in MeVisLab console. This only happens because the file of the trained network is missing and downloaded initially. You can ignore the message. Info:\u0026nbsp; The script uses the CPU, in case you want to use CUDA, you can replace the line device = torch.device(\u0026ldquo;cpu\u0026rdquo;) with: device = torch.device(\u0026lsquo;cuda\u0026rsquo; if torch.cuda.is_available() else \u0026lsquo;cpu\u0026rsquo;) The function does the following:\nGet the input image of the module PythonImage Normalize the input image Convert the image into a torch tensor of size: [Batch, Channel, z, y, x] Load and prepare AI model Set output image to module output Execute the segmentation Change alpha value of your SoView2DOverlayMPR to have a better visualization of the results.\nChange to MeVisLab IDE and select your module DemoAI. In Module Inspector click Trigger for start and wait a little until you can see the results.\nFinal result .\nWithout adding a SubImage the segmentation results should look like this:\nResults .\nSummary Pre-trained PyTorch networks can be used directly in MeVisLab via PythonImage module \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","PyTorch","Python","PythonPip","AI"],"section":"tutorials"},{"date":"1684195200","url":"https://mevislab.github.io/examples/tutorials/thirdparty/pytorch/pytorchexample3/","title":"Example 3: Segment persons in webcam videos","summary":"Example 3: Segment persons in webcam videos Introduction This tutorial is based on Example 2: Face Detection with OpenCV. You can re-use some of the scripts already developed in the other tutorial.\nSteps to do Add the macro module developed in the previous example to your workspace.\nWebCamTest module Open the internal network of the module via middle mouse button and right click on the tab of the workspace showing the internal network.","content":"Example 3: Segment persons in webcam videos Introduction This tutorial is based on Example 2: Face Detection with OpenCV. You can re-use some of the scripts already developed in the other tutorial.\nSteps to do Add the macro module developed in the previous example to your workspace.\nWebCamTest module Open the internal network of the module via middle mouse button and right click on the tab of the workspace showing the internal network. Select Show Enclosing Folder.\nShow Enclosing Folder The file browser opens showing the files of your macro module. Copy the *.mlab file somewhere you can remember.\nCreate the macro module Open the the Project Wizard via [ File \u0026rarr; Run Project Wizard ] and select Macro Module. Click Run Wizard.\nProject Wizard Define the module properties as shown below, though you can chose your own name. Click Next.\nModule Properties Define the module properties and select the copied *.mlab file. Make sure to add a Python file and click Next.\nMacro Module Properties Leave the module field reference as is and click Create. Close Project Wizard and select [ Extras \u0026rarr; Reload Module Database (Clear Cache) ].\nRe-use script and Python code Open the script file of the WebCamTest module and copy the contents to your new PyTorch module. The result should be something like this:\nPyTorchSegmentationExample.script\nInterface { Inputs {} Outputs {} Parameters {} } Commands { source = $(LOCAL)/PyTorchSegmentationExample.py } Window { h = 500 w = 500 destroyedCommand = releaseCamera initCommand = setupInterface Vertical { Horizontal { Button { title = Start command = startCapture } Button { title = Pause command = stopCapture } } Horizontal { expandX = True expandY = True Viewer View2D.self { type = SoRenderArea } } } } If you open the panel of your new module, you can see the UI elements added. You cannot use the buttons, because they require the Python function called. Copy the Python code to your new module, too.\nPyTorchSegmentationExample.py\n# from mevis import * import cv2 import OpenCVUtils _interfaces = [] camera = None face_cascade = cv2.CascadeClassifier(\u0026#39;C:/tmp/haarcascade_frontalface_default.xml\u0026#39;) # Setup the interface for PythonImage module def setupInterface(): global _interfaces _interfaces = [] interface = ctx.module(\u0026#34;PythonImage\u0026#34;).call(\u0026#34;getInterface\u0026#34;) _interfaces.append(interface) # Grab image from camera and update def grabImage(): _, img = camera.read() updateImage(img) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray, 1.1, 4) for (x, y, w, h) in faces: cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) # Display the output cv2.imshow(\u0026#39;img\u0026#39;, img) # Update image in interface def updateImage(image): _interfaces[0].setImage(OpenCVUtils.convertImageToML(image), minMaxValues = [0,255]) # Start capturing WebCam def startCapture(): global camera if not camera: camera = cv2.VideoCapture(0) ctx.callWithInterval(0.1, grabImage) # Stop capturing WebCam def stopCapture(): ctx.removeTimers() # Release camera in the end def releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None cv2.destroyAllWindows() You should now have the complete functionality of the Example 2: Face Detection with OpenCV.\nAdapt the network For PyTorch, we require some additional modules in our network. Open the internal network of your module and add another PythonImage module. Connect a Resample3D and an ImagePropertyConvert module.\nIn Resample3D module, define the Image Size 693, 520, 1. Change VoxelSize for all dimensions to 1.\nResample3D Open the Panel of the ImagePropertyConvert module and check World Matrix.\nImagePropertyConvert Then add a SoView2DOverlayMPR module and connect it to the ImagePropertyConvert and the View2D. Change Blend Mode to Blend, Alpha to something between 0 and 1 and define a color for the overlay.\nSoView2DOverlayMPR Save the internal network.\nRemove OpenCV specific code We want to use PyTorch for segmentation, therefore you need to add all necessary imports.\nPyTorchSegmentationExample.py\nimport cv2 import OpenCVUtils from torchvision.io.image import read_image from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights from torchvision.transforms.functional import to_pil_image import torch Additionally remove the face_cascade parameter from your Python code. This was necessary for detecting a face in OpenCV and is not necessary anymore in PyTorch. The only parameters you need here are:\nPyTorchSegmentationExample.py\n_interfaces = [] camera = None You can also remove the OpenCV specific lines in grabImage. The function should look like this now:\nPyTorchSegmentationExample.py\n# Grab image from camera and update def grabImage(): _, img = camera.read() updateImage(img) Adapt the function releaseCamera and remove the line cv2.destroyAllWindows().\nPyTorchSegmentationExample.py\n# Release camera in the end def releaseCamera(_): global camera, _interfaces ctx.removeTimers() _interfaces = [] if camera: camera.release() camera = None Implement PyTorch segmentation The first thing we need is a function for starting the camera. It closes the previous segmentation and calls the existing function startCapture.\nPyTorchSegmentationExample.py\ndef startWebcam(): # Close previous segmentation ctx.module(\u0026#34;PythonImage1\u0026#34;).call(\u0026#34;getInterface\u0026#34;).unsetImage() # Start webcam startCapture() As this function is not called in our User Interface, we need to update the *.script file. Change the first Button to below script:\nPyTorchSegmentationExample.script\nButton { title = \u0026#34;Start Webcam\u0026#34; command = startWebcam } Now your new function startWebcam is called whenever touching the left button. As a next step, define a Python function segmentSnapshot. We are using a pre-trained network from torchvision. In case you want to use other PyTorch possibilities, you can find lots of examples on their website.\nPyTorchSegmentationExample.py\ndef segmentSnapshot(): # Step 1: Get image from webcam capture stopCapture() inImage = ctx.field(\u0026#34;PythonImage.output0\u0026#34;).image() img = inImage.getTile((0,0,0,0,0,0), inImage.imageExtent())[0,0,:,0,:,:] # Step 2: Convert image into torch tensor img = torch.Tensor(img).type(torch.uint8) # Step 3: Initialize model with the best available weights weights = FCN_ResNet50_Weights.DEFAULT model = fcn_resnet50(weights=weights) model.eval() # Step 4: Initialize the inference transforms preprocess = weights.transforms() # Step 5: Apply inference preprocessing transforms batch = preprocess(img).unsqueeze(0) # Step 6: Use the model to segment persons in snapshot prediction = model(batch)[\u0026#34;out\u0026#34;] normalized_masks = prediction.softmax(dim=1) class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\u0026#34;categories\u0026#34;])} mask = normalized_masks[0, class_to_idx[\u0026#34;person\u0026#34;]] # Step 7: Set output image to module interface = ctx.module(\u0026#34;PythonImage1\u0026#34;).call(\u0026#34;getInterface\u0026#34;) interface.setImage(mask.detach().numpy()) # Step 8: Resize network output to original image size origImageSize = inImage.imageExtent() ctx.field(\u0026#34;Resample3D.imageSize\u0026#34;).value = (origImageSize[0], origImageSize[1], origImageSize[2]) In order to call this function, we have to change the command of the right button by adapting the *.script file.\nPyTorchSegmentationExample.script\nButton { title = \u0026#34;Segment Snapshot\u0026#34; command = segmentSnapshot } In step 5 we selected the class person. Whenever you click Segment Snapshot, PyTorch will try to segment all persons in the video.\nAdditional information:\u0026nbsp; The following classes are available:\naeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor The final result of the segmentation should be a semi-transparent red overlay of the persons segmented in your webcam stream.\nFinal Segmentation result Summary You can install additional Python AI packages by using the PythonPip module. PyTorch trained networks can be used directly in MeVisLab via PythonImage module. You can integrate AI algorithms into your MeVisLab networks. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Archive here. ","tags":["Advanced","Tutorial","PyTorch","Python","PythonPip","AI","Segmentation","WebCam"],"section":"tutorials"},{"date":"1684972800","url":"https://mevislab.github.io/examples/tutorials/thirdparty/matplotlib/","title":"Matplotlib","summary":"Matplotlib Matplotlib, introduced by John Hunter in 2002 and initially released in 2003, is a comprehensive data visualization library in Python. It is widely used among the scientific world as it is easy to grasp for beginners and provides high quality plots and images, that are widely customizable.\nInfo:\u0026nbsp; The documentation on Matplotlib along with general examples, cheat sheets and a starting guide can be found here. As MeVisLab supports the integration of Python scripts e.","content":"Matplotlib Matplotlib, introduced by John Hunter in 2002 and initially released in 2003, is a comprehensive data visualization library in Python. It is widely used among the scientific world as it is easy to grasp for beginners and provides high quality plots and images, that are widely customizable.\nInfo:\u0026nbsp; The documentation on Matplotlib along with general examples, cheat sheets and a starting guide can be found here. As MeVisLab supports the integration of Python scripts e. g. for test automation, Matplotlib can be used to visualize any data you might want to see. And as it is directly integrated into MeVisLab, you don\u0026rsquo;t have to install it (via PythonPip module) first.\nIn the following tutorial pages on Matplotlib, you will be shown how to create a module in MeVisLab, that helps you plot grayscale distributions of single slices or defined sequences of slices of a DICOM image and layer the grayscale distributions of two chosen slices for comparison.\nThe module that is adapted during the tutorials is set up in the Example 1: Module Setup tutorial. The panel and two dimensional plotting functionality is added in Example 2: 2D Plotting. In Example 3: Slice Comparison the comparison between two chosen slices is enabled by overlaying their grayscale distributions. Example 4: 3D Plotting adds an additional three dimensional plotting functionality to the panel. Check:\u0026nbsp; Notice that for the Matplotlib tutorials, the previous tutorial always works as a foundation for the following one. ","tags":["Advanced","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685059200","url":"https://mevislab.github.io/examples/tutorials/thirdparty/matplotlib/modulesetup/","title":"Example 1: Module Setup","summary":"Example 1: Module Setup Introduction To be able to access the data needed for our grayscale distribution plots, we need a network consisting of a module that imports DICOM data, a module that differentiates between slices and another that ouputs histogram data.\nSteps to do Open up your MeVisLab workspace and add the modules LocalImage, SubImage and Histogram to it. Connect the output of LocalImage to the input of SubImage and the output of SubImage with the input of Histogram.","content":"Example 1: Module Setup Introduction To be able to access the data needed for our grayscale distribution plots, we need a network consisting of a module that imports DICOM data, a module that differentiates between slices and another that ouputs histogram data.\nSteps to do Open up your MeVisLab workspace and add the modules LocalImage, SubImage and Histogram to it. Connect the output of LocalImage to the input of SubImage and the output of SubImage with the input of Histogram. If you feel like using a shortcut, you can also download the base network below and open it in your MeVisLab.\nYour finished network should look like this:\n\u0026nbsp;\u0026nbsp;\u0026nbsp;Download .mlab file here. Excursion on the concept behind modules To be able to build on the foundation we just set, it can be useful to understand how modules are conceptualized: You will have noticed how, for every module, a panel will pop up if you double-click it. The modules panel contains all of its functional parameters and enables you, as the user, to change them within a graphical user interface (GUI). We will do something similar later on. But where and how is a module panel created? To answer this question, please close the module panel and right-click on the module. A context menu will open, click on \u0026ldquo;Related Files\u0026rdquo;.\nAs you can see, each module has a .script and a .py file, named like the module itself:\nThe .script file is, where the appearance and structure of the module panel as well as their commands are declared. The .py file contains Python functions and methods, which are triggered by their referenced commands within the .script file. Some modules also reference a .mlab file which usually contains their internal network as the module is a macro.\nLet\u0026rsquo;s continue with our module setup now:\nIf your network is ready, group it by right-clicking on your group\u0026rsquo;s title and select \u0026ldquo;Grouping\u0026rdquo;, then \u0026ldquo;Add To A New Group\u0026rdquo;. After, convert your grouped network into a macro module.\nInfo:\u0026nbsp; Information on how to convert groups into macros can be found here. Depending on whether you like to reuse your projects in other workspaces, it can make sense to convert them. We\u0026rsquo;d recommend to do so.\nNow open the .script file of your newly created macro through the context menu. The file will be opened within MATE (MeVisLab Advanced Text Editor). Add this short piece of code into your .script file and make sure that the .script and the .py are named exactly the same as the module they are created for.\nBaseNetwork.script\nCommands{ source = $(LOCAL)/BaseNetwork.py } Click the \u0026ldquo;Reload\u0026rdquo; button that is located above the script for the .py file to be added into the module definition folder, then open it using the \u0026ldquo;Files\u0026rdquo; button on the same bar as demonstrated below: Info:\u0026nbsp; The MDL Reference is a very handy tool for this and certainly also for following projects. You have now created your own module and enabled the .script file (hence the GUI or panel later on) to access functions and methods written in the .py file.\nSummary Modules are defined by the contents within their definition folder. A module consists of of a .script file that contains the panel configuration and a .py file containing methods that are accessed via the panel and provide functionalities (Interacting with the parameters of modules in the macros internal network). A macro module\u0026rsquo;s panel can access parameters of its internal modules. The panel is layouted using MDL. ","tags":["Beginner","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685404800","url":"https://mevislab.github.io/examples/tutorials/thirdparty/matplotlib/2dplotting/","title":"Example 2: 2D Plotting","summary":"Example 2: 2D Plotting Introduction In this tutorial, we will equip the macro module we created in the previous tutorial with a responsive and interactable panel to plot grayscale distributions of single slices as well as defined sequences of slices in 2D.\nSteps to do Open the module definition folder of your macro module and the related .script file in MATE. Then activate the Preview as shown below:\nDrag the small Preview window to the bottom right corner of your window where it does not bother you.","content":"Example 2: 2D Plotting Introduction In this tutorial, we will equip the macro module we created in the previous tutorial with a responsive and interactable panel to plot grayscale distributions of single slices as well as defined sequences of slices in 2D.\nSteps to do Open the module definition folder of your macro module and the related .script file in MATE. Then activate the Preview as shown below:\nDrag the small Preview window to the bottom right corner of your window where it does not bother you. We will now be adding contents to be displayed there.\nAdding the following code to your .script file will open a panel window if the macro module is clicked. This new panel window contains a Matplotlib canvas where the plots will be displayed later on as well as two prepared boxes that we will add functions to in the next step.\nBaseNetwork.script\nWindow { Category { Horizontal { Vertical { expandY = True expandX = False Box { title= \u0026#34;Single Slice\u0026#34; } Box { title = \u0026#34;Sequence\u0026#34; } Empty { expandY = True } } Box { MatplotlibCanvas { expandY = True expandX = True name = canvas useToolBar = True } expandY = True expandX = True } } } } Letting a box expand on the x- or y-axis or adding an empty object do so contributes to the panel looking a certain way and helps the positioning of the elements. You can also try to vary the positioning by adding or removing expand-statements or moving boxes from a vertical to a horizontal alignment. Hover over the boxes in the preview to explore the concept.\nInfo:\u0026nbsp; You can click and hold onto a box to move it within the Preview. Your code will automatically be changed according to the new positioning. Now, we need to identify which module parameters we want to be able to access from the panel of our macro:\nTo plot a slice or a defined sequence of slices, we need to be able to set a start and an end. Go back into your MeVisLab workspace, right click your BaseNetwork module and choose \u0026ldquo;Show Internal Network\u0026rdquo;.\nThe `SubImage` module provides the option to set sequences of slices. The starting and ending slices of the sequence can be set in the module panel. Info:\u0026nbsp; To find out what the parameters are called, what type of values they contain and receive and what they refer to, you can right-click on them within the panel. We now know that we will need SubImage.z and SubImage.sz to define the start and end of a sequence. But there are a few other module parameters that must be set beforehand to make sure the data we extract to plot later is compareable and correct.\nTo do so, we will be defining a \u0026ldquo;setDefaults\u0026rdquo; function for our module. Open the .py file and add the code below.\nBaseNetwork.py\ndef setDefaults(): ctx.field(\u0026#34;SubImage.fullSize\u0026#34;).touch() ctx.field(\u0026#34;SubImage.autoApply\u0026#34;).value = True ctx.field(\u0026#34;Histogram.updateMode\u0026#34;).value = \u0026#34;AutoUpdate\u0026#34; ctx.field(\u0026#34;Histogram.xRange\u0026#34;).value = \u0026#34;Dynamic Min/Max\u0026#34; ctx.field(\u0026#34;Histogram.useZeroAsBinCenter\u0026#34;).value = False ctx.field(\u0026#34;Histogram.binSize\u0026#34;).value = 5.0 ctx.field(\u0026#34;Histogram.backgroundValue\u0026#34;).value = False ctx.field(\u0026#34;Histogram.curveType\u0026#34;).value = \u0026#34;Area\u0026#34; ctx.field(\u0026#34;Histogram.useStepFunction\u0026#34;).value = True ctx.field(\u0026#34;Histogram.curveStyle\u0026#34;).value = 7 As it is also incredibly important, that the values of the parameters we are referencing are regularly updated, we will be setting some global values containing those values.\nBaseNetwork.py\nstartSlice = None endSlice = None bins = None def updateSlices(): global startSlice, endSlice, bins startSlice = int(ctx.field(\u0026#34;SubImage.z\u0026#34;).value) endSlice = int(ctx.field(\u0026#34;SubImage.sz\u0026#34;).value) bins = ctx.field(\u0026#34;Histogram.binSize\u0026#34;).value Make sure that the variable declarations as none are put above the \u0026ldquo;setDefaults\u0026rdquo; function and add the execution of the \u0026ldquo;updateSlices()\u0026rdquo; function into the \u0026ldquo;setDefaults\u0026rdquo; function, like so:\nBaseNetwork.py\ndef setDefaults(): ctx.field(\u0026#34;Histogram.xRange\u0026#34;).value = \u0026#34;Dynamic Min/Max\u0026#34; ctx.field(\u0026#34;Histogram.useZeroAsBinCenter\u0026#34;).value = False ctx.field(\u0026#34;Histogram.binSize\u0026#34;).value = 5.0 ctx.field(\u0026#34;Histogram.backgroundValue\u0026#34;).value = False ctx.field(\u0026#34;Histogram.curveType\u0026#34;).value = \u0026#34;Area\u0026#34; ctx.field(\u0026#34;Histogram.useStepFunction\u0026#34;).value = True ctx.field(\u0026#34;Histogram.curveStyle\u0026#34;).value = 7 ctx.field(\u0026#34;SubImage.fullSize\u0026#34;).touch() ctx.field(\u0026#34;SubImage.autoApply\u0026#34;).value = True ctx.field(\u0026#34;Histogram.updateMode\u0026#34;).value = \u0026#34;AutoUpdate\u0026#34; updateSlices() Now we are ensuring, that the \u0026ldquo;setDefaults\u0026rdquo; function and therefore also the \u0026ldquo;updateSlices\u0026rdquo; function are executed everytime the panel is opened by setting \u0026ldquo;setDefaults\u0026rdquo; as a wake up command.\nBaseNetwork.script\nCommands { source = $(LOCAL)/BaseNetwork.py wakeupCommand = \u0026#34;setDefaults\u0026#34; } And we add field listeners, so that the field values that we are working with are updated everytime they are changed.\nBaseNetwork.script\nCommands { source = $(LOCAL)/BaseNetwork.py wakeupCommand = \u0026#34;setDefaults\u0026#34; FieldListener { listenField = \u0026#34;SubImage.sz\u0026#34; listenField = \u0026#34;SubImage.z\u0026#34; listenField = \u0026#34;Histogram.binSize\u0026#34; command = \u0026#34;updateSlices\u0026#34; } } To see if all of this is working, we need to embed fields into our panel. Put this inside of the box titled \u0026ldquo;Single Slice\u0026rdquo;:\nBaseNetwork.script\nField \u0026#34;SubImage.sz\u0026#34; { title = \u0026#34;Plot slice\u0026#34; } Button { title = \u0026#34;in 2D\u0026#34; command = \u0026#34;singleSlice2D\u0026#34; } Button { title = \u0026#34;in 3D\u0026#34; command = \u0026#34;click3D\u0026#34; } Empty {} And then add this to your box titled \u0026ldquo;Sequence\u0026rdquo;:\nBaseNetwork.script\nField \u0026#34;SubImage.z\u0026#34; { title = \u0026#34;From slice\u0026#34; } Field \u0026#34;SubImage.sz\u0026#34; { title = \u0026#34;To slice\u0026#34; } Button { title = \u0026#34;Plot 2D\u0026#34; command = \u0026#34;click2D\u0026#34; } Button { title = \u0026#34;Plot 3D\u0026#34; command = \u0026#34;click3D\u0026#34; } Lastly, put this under your two boxes, but above the empty element in the vertical alignment: BaseNetwork.script\nField \u0026#34;Histogram.binSize\u0026#34; { title = \u0026#34;Bin size\u0026#34; } If you followed all of the listed steps, your panel preview should look like this and display all the current parameter values. We can now work on the functions that visualize the data as plots on the Matplotlib canvas. You will have noticed how all of the buttons in the .script file have a command. Whenever that button is clicked, its designated command is executed. However, for any of the functions referenced via command to work, we need one that ensures, that the plots are shown on the integrated Matplotlib canvas. We will start with that one.\nBaseNetwork.py\ndef clearFigure(): control = ctx.control(\u0026#34;canvas\u0026#34;).object() control.figure().clear() Now that this is prepared and ready, we can add the functions to extract the data:\nBaseNetwork.py\ndef getX(): x = ctx.field(\u0026#34;Histogram.outputHistogramCurve\u0026#34;).object().getXValues() stringx = \u0026#34;,\u0026#34;.join([str(i) for i in x]) xValues = stringx.split(\u0026#34;,\u0026#34;) return [float(s) for s in xValues] def getY(): y = ctx.field(\u0026#34;Histogram.outputHistogramCurve\u0026#34;).object().getYValues() stringy = \u0026#34;,\u0026#34;.join([str(i) for i in y]) yValues = stringy.split(\u0026#34;,\u0026#34;) return [float(s) for s in yValues] And lastly enable the plotting of a single slice as well as a sequence in 2D through our panel by adding the code below.\nBaseNetwork.py\ndef singleSlice2D(): global endSlice lastSlice = endSlice ctx.field(\u0026#34;SubImage.z\u0026#34;).value = endSlice click2D() ctx.field(\u0026#34;SubImage.z\u0026#34;).value = lastSlice def plotSequence(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() values = [i for i in range(startSlice, endSlice + 1)] if len(values) \u0026lt;= 4: # adapt the height of the subplot to the number of plots sub = 100 * len(values) + 11 for i in values: subplot = figure.add_subplot(sub) sub += 1 ctx.field(\u0026#34;SubImage.z\u0026#34;).value = i ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = i subplot.bar(getX(), getY(), bins, color=\u0026#39;r\u0026#39;, label=f\u0026#39;Slice {i}\u0026#39;) subplot.legend([f\u0026#39;Slice {i}\u0026#39;]) else: subplot = figure.add_subplot() for i in values: ctx.field(\u0026#34;SubImage.z\u0026#34;).value = i ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = i subplot.plot(getX(), getY(), bins) subplot.legend([f\u0026#39;Slice {i}\u0026#39; for i in values]) ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] figure.canvas.draw() def click2D(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() if startSlice == endSlice: subplot = figure.add_subplot(111) subplot.bar(getX(), getY(), bins, color=\u0026#39;b\u0026#39;, label=f\u0026#34;Slice {endSlice}\u0026#34;) subplot.legend() subplot.plot() figure.canvas.draw() else: plotSequence() You should now be able to reproduce results like these:\n2D plot of slice 28 Smaller sequences are displayed as multiple single slice plots. Sequence in 2D Info:\u0026nbsp; Notice how the bin size affects the plots appearance. You can download the .py file below if you want. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Python file here. Summary Functions are connected to fields of the panel via commands The panel preview in MATE can be used to alter positioning of panel components without touching the code An \u0026ldquo;expand\u0026rdquo; statement can help the positioning of components in the panel ","tags":["Advanced","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685491200","url":"https://mevislab.github.io/examples/tutorials/thirdparty/matplotlib/slicecomparison/","title":"Example 3: Slice Comparison","summary":"Example 3: Slice Comparison Introduction We will adapt the previously created macro module to be able to overlay two defined slices to compare their grayscale distributions.\nThe module we are adapting has been set up in the Example 1: Module Setup tutorial. The panel and two-dimensional plotting functionality has been added in [Example 2: 2D Plotting] (/tutorials/thirdparty/matplotlib/2dplotting). Steps to do At first, we will extend the panel: Open your BaseNetwork macro module within an empty MeVisLab workspace and select the .","content":"Example 3: Slice Comparison Introduction We will adapt the previously created macro module to be able to overlay two defined slices to compare their grayscale distributions.\nThe module we are adapting has been set up in the Example 1: Module Setup tutorial. The panel and two-dimensional plotting functionality has been added in [Example 2: 2D Plotting] (/tutorials/thirdparty/matplotlib/2dplotting). Steps to do At first, we will extend the panel: Open your BaseNetwork macro module within an empty MeVisLab workspace and select the .script file from its related files.\nAdd the following code into your .script file, between the \u0026ldquo;Single Slice\u0026rdquo; and the \u0026ldquo;Sequence\u0026rdquo; box.\nBaseNetwork.script\nBox { title = \u0026#34;Comparison\u0026#34; Field \u0026#34;SubImage.z\u0026#34; { title = \u0026#34;Compare slice\u0026#34; } Field \u0026#34;SubImage.sz\u0026#34; { title = \u0026#34;With slice\u0026#34; } Button { title = \u0026#34;Plot\u0026#34; command = \u0026#34;comparison\u0026#34; } } Your panel should now be altered to look like this:\nWe will now add the \u0026ldquo;comparison\u0026rdquo; function, to give the \u0026ldquo;Plot\u0026rdquo; button in our \u0026ldquo;Comparison\u0026rdquo; box a purpose. To do so, change into your modules .py file and choose a cosy place for the following piece of code:\nBaseNetwork.py\ndef comparison(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() values = [startSlice, endSlice] ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = values[0] y1 = getY() x1 = getX() ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[1] ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = values[1] y2 = getY() x2 = getX() subplot = figure.add_subplot(211) subplot.bar(x1, y1, bins, color=\u0026#39;r\u0026#39;) subplot.bar(x2, y2, bins, color=\u0026#39;b\u0026#39;) subplot.legend([f\u0026#39;Slice {i}\u0026#39; for i in values]) subplot.plot() subplot = figure.add_subplot(212) subplot.bar(x2, y2, bins, color=\u0026#39;b\u0026#39;) subplot.bar(x1, y1, bins, color=\u0026#39;r\u0026#39;) subplot.legend([f\u0026#39;Slice {i}\u0026#39; for i in values]) figure.canvas.draw() ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] You should now be able to reproduce results like these:\nSummary Grayscale distributions of two slices can be layered to compare them and make deviations noticeable ","tags":["Beginner","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1685491200","url":"https://mevislab.github.io/examples/tutorials/thirdparty/matplotlib/3dplotting/","title":"Example 4: 3D Plotting","summary":"Example 4: 3D Plotting Introduction In this tutorial, we will equip the macro module we created in the Example 1: Module Setup and later on adapted by enabling it to plot grayscale distributions of single slices and sequences in 2D in Example 2: 2D Plotting with a three dimensional plotting functionality.\nSteps to do The fields and commands needed have already been prepared in the second tutorial. We will just have to modify our .","content":"Example 4: 3D Plotting Introduction In this tutorial, we will equip the macro module we created in the Example 1: Module Setup and later on adapted by enabling it to plot grayscale distributions of single slices and sequences in 2D in Example 2: 2D Plotting with a three dimensional plotting functionality.\nSteps to do The fields and commands needed have already been prepared in the second tutorial. We will just have to modify our .py file a little to make them usable. Integrate the following code into your .py file and import numpy.\nBaseNetwork.py\ndef click3D(): clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() values = [i for i in range(startSlice, endSlice + 1)] if startSlice == endSlice: subplot = figure.add_subplot(111, projection=\u0026#39;3d\u0026#39;) subplot.bar3d(x=getX(), y=startSlice, z=0, dx=1, dy=1, dz=getY()) subplot.set_yticks(np.arange(startSlice, endSlice)) subplot.set_title(f\u0026#39;Slice {startSlice}\u0026#39;) figure.canvas.draw() else: clearFigure() figure = ctx.control(\u0026#34;canvas\u0026#34;).object().figure() subplot = figure.add_subplot(111, projection=\u0026#39;3d\u0026#39;) for i in values: ctx.field(\u0026#34;SubImage.z\u0026#34;).value = i ctx.field(\u0026#34;SubImage.sz\u0026#34;).value = i subplot.bar3d(x=getX(), y=i, z=0, dx=1, dy=1, dz=getY()) subplot.set_yticks(values) subplot.set_title(f\u0026#39;Sequence from {values[0]} to {endSlice}\u0026#39;) ctx.field(\u0026#34;SubImage.z\u0026#34;).value = values[0] figure.canvas.draw() After saving, you should be able to reproduce results like these:\nWarning:\u0026nbsp; You cannot zoom into 3D plots on a Matplotlib canvas. Try changing the viewing angle instead. You can download the .py file below if you want. \u0026nbsp;\u0026nbsp;\u0026nbsp;Download Python file here. ","tags":["Advanced","Tutorial","Matplotlib","Visualization"],"section":"tutorials"},{"date":"1701216000","url":"https://mevislab.github.io/examples/tutorials/shorts/","title":"Tips and Tricks","summary":"MeVisLab Tips and Tricks This chapter shows some features and functionalities which are helpful but do not provide its own tutorial.\nKeyboard Shortcuts Using Snippets Scripting Assistant User Scripts Show status of module in- and output Keyboard Shortcuts This is a collection of useful keyboard shortcuts in MeVisLab, hopefully it grows continuously.\nShortcut Functionality Ctrl+1 Automatically arrange selection of modules / in the current network Ctrl+2 Open most recent network file Ctrl+3 Run most recent test case (extremely useful for developers) Ctrl+A then Ctrl+1 Layout network Ctrl+A then Tab Layout .","content":"MeVisLab Tips and Tricks This chapter shows some features and functionalities which are helpful but do not provide its own tutorial.\nKeyboard Shortcuts Using Snippets Scripting Assistant User Scripts Show status of module in- and output Keyboard Shortcuts This is a collection of useful keyboard shortcuts in MeVisLab, hopefully it grows continuously.\nShortcut Functionality Ctrl+1 Automatically arrange selection of modules / in the current network Ctrl+2 Open most recent network file Ctrl+3 Run most recent test case (extremely useful for developers) Ctrl+A then Ctrl+1 Layout network Ctrl+A then Tab Layout .script file (in MATE) Ctrl+D Duplicate currently selected module (including all field values) Ctrl+Left Mouse or Middle Mouse Button Show Internal Network Space Show hidden outputs of the currently selected module Ctrl+Alt+T Start test center Ctrl+K\tRestart MeVisLab Using Snippets \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. Sometimes you have to create the same network over and over again \u0026ndash; for example, to quickly preview DICOM files. Generally, you will at least add one module to load and another module to display your images. Sometimes you may also want to view the DICOM header data. A network you possibly generate whenever opening DICOM files will be the following:\nOpen DICOM files Create a snippet of your commonly used networks by adding the snippets list from the main menu. Open [ View \u0026rarr; Views \u0026rarr; Snippets List ]. A new panel is shown. Select all modules of your network and double-click New\u0026hellip; in your Snippets List.\nEnter a name for your snippet like DICOM Viewer and click Add.\nA new snippet will be shown in your Snippets List. You can drag and drop the snippet to your workspace and the modules are re-used, including all defined field values.\nSnippets List Scripting Assistant \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;This example is also available on YouTube. If you are new to Python or don\u0026rsquo;t have experiences in accessing fields in MeVisLab via Python scripting, the Scripting Assistant might help you.\nOpen [ View \u0026rarr; Views \u0026rarr; Scripting Assistant ]. A new panel is shown.\nIf you now interact with a network, module or macro module, your user interactions are converted into Python calls. You can see the calls in the panel of the Scripting Assistant and copy and paste them for your Python script.\nScripting Assistant User Scripts User scripts allow you to call any Python code from the main menu entry [ Scripting ]. MeVisLab already comes with some user scripts you can try. You can also view the sources for example code via right-click on the menu entry under [ Scripting ].\nThis example shows you how to change the color of the MeVisLab IDE to a dark mode.\nRight-click menu entry [ Scripting \u0026rarr; Utilities \u0026rarr; Close Unselected Panels ] and select Edit User Script. The Python file opens in MATE. Right-click on the tab in the editor and select Show Enclosing Folder.\nThe opened directory contains all available user scripts. Add a new file MyScripts.def and open the file in MATE.\nEnter the following: MyScripts.def\nUserIDEActions { Action \u0026#34;Set Dark Theme\u0026#34; { name = changeTheme userScript = $(LOCAL)/changeTheme.py statusTip = \u0026#34;Change Theme to dark mode.\u0026#34; accel = \u0026#34;ctrl+F9\u0026#34; } } UserIDEMenus { SubMenu \u0026#34;Theme\u0026#34; { ActionReference = changeTheme } } We define an action Set Dark Theme, which is added to the submenu Theme in the MeVisLab IDE menu item [ Scripting ]. The action is named changeTheme and a reference to a Python script is added as $(LOCAL)/changeTheme.py. We also defined a keyboard shortcut ctrl\u0026#43;F9 .\nChange to MeVisLab IDE and select menu item [ Extras \u0026rarr; Reload Module Database (Clear Cache) ]. Open the menu item [ Scripting ]. You can see the new submenu [ Theme \u0026rarr; Set Dark Theme ]. If you select this entry, you get an error in MeVisLab console: Could not locate user script: \u0026hellip;/changeTheme.py\nWe did not yet create the Python file containing the code of your script.\nOpen the directory where your MyScripts.def file is located and create a new Python file changeTheme.py. Open the file in MATE and enter the following:\nchangeTheme.py\nfrom PythonQt.QtGui import QApplication, QColor, QPalette fgColor = QColor(\u0026#34;#888888\u0026#34;) bgColor = QColor(\u0026#34;#333333\u0026#34;) palette = QApplication.palette() palette.setColor(QPalette.Window, bgColor) palette.setColor(QPalette.Background, bgColor) palette.setColor(QPalette.Base, bgColor) palette.setColor(QPalette.Button, bgColor) palette.setColor(QPalette.WindowText, fgColor) palette.setColor(QPalette.Text, fgColor) QApplication.setPalette(palette) This script defines the color of the MeVisLab user interface elements. You can define other colors and more items, this is just an example of what you can do with user scripts.\nSwitch back to the MeVisLab IDE and select the menu item [ Extras \u0026rarr; Reload Module Database (Clear Cache) ] again. The colors of the MeVisLab IDE change as defined in our Python script. This change persists until you restart MeVisLab and can always be repeated by selecting the menu entry or the keyboard shortcut ctrl\u0026#43;F9 .\nShow status of module in- and output Especially in large networks it is useful to see the state of the input and output connectors of a module. By default, the module connectors do not show if data is available. Below image shows a DicomImport module and a View2D module where no data is loaded.\nNo status on connector In the MeVisLab preferences dialog, you can see a checkbox Show ML image state. By default, the setting is Off.\nShow ML image state After enabling Show ML image state, your network changes and the input and output connectors appear red in case no data is available at the output.\nNo data on connector After loading a valid DICOM directory, the connectors providing a valid ML image appear green. The previously red outputs are beige again, showing there is data available.\nNo data on connector ","tags":["Basic","Tutorial"],"section":"tutorials"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/basic_mechanisms/","title":"Basic Mechanisms","summary":"Basic Mechanism Examples: The following examples are available:\n[1] Contour Filter [2] Creating a simple application [3] Panel for the contour filter [4] Python scripting ","content":"Basic Mechanism Examples: The following examples are available:\n[1] Contour Filter [2] Creating a simple application [3] Panel for the contour filter [4] Python scripting ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/data_objects/","title":"Data Objects","summary":"Data Object Examples: The following examples are available:\n[1] 2D and 3D visualization of contours [2] Annotation of images [3] Apply transformations on a 3D WEM object via mouse interactions [4] Contour interpolation [5] Contours and ghosting [6] Creation of Contours [7] Creation of WEMs [8] Distance between markers [9] Drawing curves [10] Interactively moving WEM [11] Processing and modification of WEMs [12] WEM - Primitive Value Lists ","content":"Data Object Examples: The following examples are available:\n[1] 2D and 3D visualization of contours [2] Annotation of images [3] Apply transformations on a 3D WEM object via mouse interactions [4] Contour interpolation [5] Contours and ghosting [6] Creation of Contours [7] Creation of WEMs [8] Distance between markers [9] Drawing curves [10] Interactively moving WEM [11] Processing and modification of WEMs [12] WEM - Primitive Value Lists ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/thirdparty/example2/","title":"Face detection in OpenCV","summary":"ThirdParty Example 2: Face detection in OpenCV This Python file shows how to access the webcam and detect faces in the video stream via OpenCV.\nDownload You can download the Python files here","content":"ThirdParty Example 2: Face detection in OpenCV This Python file shows how to access the webcam and detect faces in the video stream via OpenCV.\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/image_processing/","title":"Image Processing","summary":"Image Processing Examples: The following examples are available:\n[1] Arithmetic operations on two images [2] Clip Planes [3] Masking images [4] Region Growing (Segmentation) [5] Subtract 3D objects ","content":"Image Processing Examples: The following examples are available:\n[1] Arithmetic operations on two images [2] Clip Planes [3] Masking images [4] Region Growing (Segmentation) [5] Subtract 3D objects ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/testing/example3/","title":"Iterative tests in MeVisLab with Screenshots","summary":"Testing Example 3: Iterative tests in MeVisLab with Screenshots In this example you will learn how to write iterative tests in MeVisLab. In addition to that, we create a screenshot of a viewer and add the image to the test report.\nDownload n.a.","content":"Testing Example 3: Iterative tests in MeVisLab with Screenshots In this example you will learn how to write iterative tests in MeVisLab. In addition to that, we create a screenshot of a viewer and add the image to the test report.\nDownload n.a.\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/open_inventor/","title":"Open Inventor","summary":"Open Inventor Examples: The following examples are available:\n[1] Camera interaction with collision detection [2] Camera Interactions in Open Inventor [3] Mouse interactions in an Open Inventor scene [4] Open Inventor objects ","content":"Open Inventor Examples: The following examples are available:\n[1] Camera interaction with collision detection [2] Camera Interactions in Open Inventor [3] Mouse interactions in an Open Inventor scene [4] Open Inventor objects ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/thirdparty/example1/","title":"OpenCV Webcam access","summary":"ThirdParty Example 1: OpenCV Webcam access This Python file shows how to access the webcam via OpenCV and use the video via PythonImage module in MeVisLab.\nDownload You can download the Python files here","content":"ThirdParty Example 1: OpenCV Webcam access This Python file shows how to access the webcam via OpenCV and use the video via PythonImage module in MeVisLab.\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/testing/example2/","title":"Profiling in MeVisLab","summary":"Testing Example 2: Profiling in MeVisLab This example shows how to use the Profiling View in MeVisLab.\nDownload n.a.","content":"Testing Example 2: Profiling in MeVisLab This example shows how to use the Profiling View in MeVisLab.\nDownload n.a.\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/thirdparty/pytorch1/","title":"PyTorch segmentation","summary":"ThirdParty Example 5: Segmentation in webcam stream by using PyTorch This macro module segments a person shown in a webcam stream by using a pre-trained network from PyTorch (torchvision).\nDownload You can download the Python files here","content":"ThirdParty Example 5: Segmentation in webcam stream by using PyTorch This macro module segments a person shown in a webcam stream by using a pre-trained network from PyTorch (torchvision).\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/testing/","title":"Testing Examples","summary":"Testing Examples: The following examples are available:\n[1] Iterative tests in MeVisLab with Screenshots [2] Profiling in MeVisLab [3] Writing a simple test case in MeVisLab ","content":"Testing Examples: The following examples are available:\n[1] Iterative tests in MeVisLab with Screenshots [2] Profiling in MeVisLab [3] Writing a simple test case in MeVisLab ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/thirdparty/","title":"ThirdParty Examples","summary":"ThirdParty Examples: The following examples are available:\n[1] Face detection in OpenCV [2] OpenCV Webcam access [3] PyTorch segmentation ","content":"ThirdParty Examples: The following examples are available:\n[1] Face detection in OpenCV [2] OpenCV Webcam access [3] PyTorch segmentation ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/howto/","title":"Using provided examples","summary":"Structure Each tutorial chapter was used as an umbrella theme to structure related examples, that are linked in a list. After clicking any of the linked examples, you will be forwarded to a short description of the feature and have the option to download the resource that produces your desired effect.\nThe provided files are usually either *.mlab files or *.zip archives. You will find a short tutorial on how to add those files into your MeVisLab application, to work with them, below.","content":"Structure Each tutorial chapter was used as an umbrella theme to structure related examples, that are linked in a list. After clicking any of the linked examples, you will be forwarded to a short description of the feature and have the option to download the resource that produces your desired effect.\nThe provided files are usually either *.mlab files or *.zip archives. You will find a short tutorial on how to add those files into your MeVisLab application, to work with them, below.\nMeVisLab (*.mlab) files MeVisLab files are networks stored as *.mlab files. Info:\u0026nbsp; Double clicking the left mouse button within your MeVisLab workspace works as a shortcut to open files. Files can also be opened using the menu option [ File \u0026rarr; Open ].\nArchives (*.zip files) Archives mostly contain macro modules. To use those macro modules, you will need to know how to handle user packages.\nCheck:\u0026nbsp; See Example 2.1: Package creation for more information on packages in MeVisLab. The contents can be extracted into the directory of your package. Make sure to keep the directory\u0026rsquo;s structure for the examples to be loaded and displayed correctly.\nThe typical directory structure of a MeVisLab package looks like this: Package directory structure The package TutorialSummary within the package group MeVis is shown above. A package normally at least contains a Projects directory, which is where the macro modules are located. When extracting the contents of a *.zip file, the Projects folder of your package should be the target directory.\nSometimes we even provide test cases. Extract them into the TestCases directory. Package directory structure Notice:\u0026nbsp; Feel free to create certain directories if they do not exist yet, but make sure to name them conforming the directory structure shown above. Continuing on your MeVisLab workspace: You might need to reload the module cache after adding macro modules out of *.zip archives for them to be displayed and ready to be used. To do so, open [ Extras \u0026rarr; Reload Module Database (Clear Cache) ].\nPython (*.py) or Script (*.script) files In the rare case that a *.py or *.script file is provided, make sure to firstly follow the tutorials related to macro modules and test cases.\nWarning:\u0026nbsp; The integration of Python scripts might not add a lot of value for someone that lacks the knowledge conveyed by the tutorials. ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/visualization/","title":"Visualization Examples","summary":"Visualization Examples: The following examples are available:\n[1] Creating a magnifier [2] Display images converted to Open Inventor scene objects [3] Image overlays [4] Synchronous view of two images [5] Volume rendering and interactions [6] Volume Rendering vs. Path Tracing ","content":"Visualization Examples: The following examples are available:\n[1] Creating a magnifier [2] Display images converted to Open Inventor scene objects [3] Image overlays [4] Synchronous view of two images [5] Volume rendering and interactions [6] Volume Rendering vs. Path Tracing ","tags":[],"section":"examples"},{"date":"1655276193","url":"https://mevislab.github.io/examples/examples/testing/example1/","title":"Writing a simple test case in MeVisLab","summary":"Testing Example 1: Writing a simple test case in MeVisLab This example shows how to write and execute test cases in MeVisLab. The Python files can be downloaded below.\nDownload You can download the Python files here","content":"Testing Example 1: Writing a simple test case in MeVisLab This example shows how to write and execute test cases in MeVisLab. The Python files can be downloaded below.\nDownload You can download the Python files here\n","tags":[],"section":"examples"},{"date":"1655276093","url":"https://mevislab.github.io/examples/about/about/","title":"Overview","summary":"Symbols We embedded three symbols, referencing additional info, tasks and warnings: Info:\u0026nbsp; Provides additional links or info on the current topic. Check:\u0026nbsp; Points out a related task. Warning:\u0026nbsp; Hints common mistakes or steps you should consider beforehand. Keyboard Shortcuts Keyboard shortcuts are incorporated like this: CTRL + ALT + 2 .\nNetworks The networks shown and used in the tutorials can be found in the Examples section of this page. They are usually embedded like this: ","content":"Symbols We embedded three symbols, referencing additional info, tasks and warnings: Info:\u0026nbsp; Provides additional links or info on the current topic. Check:\u0026nbsp; Points out a related task. Warning:\u0026nbsp; Hints common mistakes or steps you should consider beforehand. Keyboard Shortcuts Keyboard shortcuts are incorporated like this: CTRL + ALT + 2 .\nNetworks The networks shown and used in the tutorials can be found in the Examples section of this page. They are usually embedded like this: ","tags":["Symbols","Glossary","Overview"],"section":"about"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/contours/example3/","title":"2D and 3D visualization of contours","summary":"Contour Example 3: 2D and 3D visualization of contours This example shows how to display CSOs in 2D as an overlay and additionally how the CSOs are displayed in 3D.\nSummary Images are loaded by using a LocalImage module and displayed in a 2D viewer. A SoCSOLiveWireEditor is added to draw contours on the images. The CSOSliceInterpolator generates additional contours between the manual CSOs by using linear interpolation.\nThe module VoxelizeCSO is used to create a three-dimensional voxel mask of the contours which can be used as an overlay on the images in a View2D panel.","content":"Contour Example 3: 2D and 3D visualization of contours This example shows how to display CSOs in 2D as an overlay and additionally how the CSOs are displayed in 3D.\nSummary Images are loaded by using a LocalImage module and displayed in a 2D viewer. A SoCSOLiveWireEditor is added to draw contours on the images. The CSOSliceInterpolator generates additional contours between the manual CSOs by using linear interpolation.\nThe module VoxelizeCSO is used to create a three-dimensional voxel mask of the contours which can be used as an overlay on the images in a View2D panel. The SoView2DOverlay module defines the color and opacity of the overlay.\nLastly the panel of the View3D module is used to visualize the voxel mask in 3D.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/contours/example4/","title":"Annotation of images","summary":"Contour Example 4: Annotation of images This example shows how to add annotations to an image.\nSummary In this example, the network of Contour Example 3 is extended so that the volume of the 3D mask generated by the VoxelizeCSO module is calculated. The CalculateVolume module counts the number of voxels in the given mask and returns the correct volume in ml. The calculated volume will be used for a custom SoView2DAnnotation displayed in the View2D.","content":"Contour Example 4: Annotation of images This example shows how to add annotations to an image.\nSummary In this example, the network of Contour Example 3 is extended so that the volume of the 3D mask generated by the VoxelizeCSO module is calculated. The CalculateVolume module counts the number of voxels in the given mask and returns the correct volume in ml. The calculated volume will be used for a custom SoView2DAnnotation displayed in the View2D.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/surface_objects/example3/","title":"Apply transformations on a 3D WEM object via mouse interactions","summary":"Surface Example 3: Interactions with WEM Scale, rotate and move a WEM in a scene In this example, we are using a SoTransformerDragger module to apply transformations on a 3D WEM object via mouse interactions. Download You can download the example network here\nInteractively modify WEMs In this example, we are using a SoWEMBulgeEditor module to modify a WEM using the mouse. Download You can download the example network here","content":"Surface Example 3: Interactions with WEM Scale, rotate and move a WEM in a scene In this example, we are using a SoTransformerDragger module to apply transformations on a 3D WEM object via mouse interactions. Download You can download the example network here\nInteractively modify WEMs In this example, we are using a SoWEMBulgeEditor module to modify a WEM using the mouse. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/image_processing/example1/","title":"Arithmetic operations on two images","summary":"Image Processing Example 1: Arithmetic operations on two images In this example, we apply scalar functions on two images like Add, Multiply, Subtract, etc.\nSummary We are loading two images by using the LocalImage module and show them in a SynchroView2D. In addition to that, both images are used for arithmetic processing in the module Arithmetic2.\nDownload You can download the example network here","content":"Image Processing Example 1: Arithmetic operations on two images In this example, we apply scalar functions on two images like Add, Multiply, Subtract, etc.\nSummary We are loading two images by using the LocalImage module and show them in a SynchroView2D. In addition to that, both images are used for arithmetic processing in the module Arithmetic2.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/open_inventor/example4/","title":"Camera interaction with collision detection","summary":"Open Inventor Example 4: Camera interaction with collision detection This example shows how to implement a camera flight using keyboard shortcuts. Collisions with anatomical structures are detected and the flight stops. In addition to that, the camera object and direction is rendered in another viewer.\nThis example has been taken from the MeVisLab forum.\nSummary A local macro flightControl allows you to navigate with the camera through the scene.\nDownload You can download the example network here","content":"Open Inventor Example 4: Camera interaction with collision detection This example shows how to implement a camera flight using keyboard shortcuts. Collisions with anatomical structures are detected and the flight stops. In addition to that, the camera object and direction is rendered in another viewer.\nThis example has been taken from the MeVisLab forum.\nSummary A local macro flightControl allows you to navigate with the camera through the scene.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/open_inventor/example3/","title":"Camera Interactions in Open Inventor","summary":"Open Inventor Example 3: Camera Interactions in Open Inventor This example shows different options for using a camera and different viewers in Open Inventor.\nSummary We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nDownload You can download the example network here","content":"Open Inventor Example 3: Camera Interactions in Open Inventor This example shows different options for using a camera and different viewers in Open Inventor.\nSummary We will show the difference between a SoRenderArea and a SoExaminerViewer and use different modules of the SoCamera* group.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/image_processing/example5/","title":"Clip Planes","summary":"Image Processing Example 5: Clip Planes In this example, we are using the currently visible slice from a 2D view as a clip plane in 3D.\nSummary We are loading images by using the LocalImage module and render them as a 2-dimensional image stack SoRenderArea. The displayed slice is used to create a 3D plane/clip plane in a SoExaminerViewer.\nDownload You can download the example network here","content":"Image Processing Example 5: Clip Planes In this example, we are using the currently visible slice from a 2D view as a clip plane in 3D.\nSummary We are loading images by using the LocalImage module and render them as a 2-dimensional image stack SoRenderArea. The displayed slice is used to create a 3D plane/clip plane in a SoExaminerViewer.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/basic_mechanisms/contour_filter/","title":"Contour Filter","summary":"Example 1: Contour Filter This example shows how to create a contour filter.\nSummary Images are loaded via ImageLoad module and visualized unchanged in a View2D module View2D1. Additionally the images are modified by a local macro module Filter and shown in another View2D viewer View2D.\nIn order to display the same slice (unchanged and changed), the module SyncFloat is used to synchronize the field value startSlice in both viewers. The SyncFloat module duplicates the value Float1 to the field Float2.","content":"Example 1: Contour Filter This example shows how to create a contour filter.\nSummary Images are loaded via ImageLoad module and visualized unchanged in a View2D module View2D1. Additionally the images are modified by a local macro module Filter and shown in another View2D viewer View2D.\nIn order to display the same slice (unchanged and changed), the module SyncFloat is used to synchronize the field value startSlice in both viewers. The SyncFloat module duplicates the value Float1 to the field Float2.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/contours/example2/","title":"Contour interpolation","summary":"Contour Example 2: Contour interpolation This example shows how to interpolate CSOs across slices.\nSummary In this example, semi-automatic countours are created using the SoCSOLiveWireEditor module and their depiction is modified using the SoCSOVisualizationSettings module.\nAdditional contours between the manually created ones are generated by the CSOSliceInterpolator and added to the CSOManager. Different groups of contours are created for the left and right lobe of the lung and colored respectively.","content":"Contour Example 2: Contour interpolation This example shows how to interpolate CSOs across slices.\nSummary In this example, semi-automatic countours are created using the SoCSOLiveWireEditor module and their depiction is modified using the SoCSOVisualizationSettings module.\nAdditional contours between the manually created ones are generated by the CSOSliceInterpolator and added to the CSOManager. Different groups of contours are created for the left and right lobe of the lung and colored respectively.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/contours/example5/","title":"Contours and ghosting","summary":"Contour Example 5: Contours and ghosting This image shows how to automatically create CSOs based on ISO values. In addition the visualization of CSOs of previous and subsequent slices is shown.\nSummary In this example, the CSOIsoGenerator is used to generate contours based on a given ISO value of the image. Contours are generated in the image where the given ISO value is close to the one configured. These contours are stored in the CSOManager and ghosting is activated in the SoCSOVisualizationSettings.","content":"Contour Example 5: Contours and ghosting This image shows how to automatically create CSOs based on ISO values. In addition the visualization of CSOs of previous and subsequent slices is shown.\nSummary In this example, the CSOIsoGenerator is used to generate contours based on a given ISO value of the image. Contours are generated in the image where the given ISO value is close to the one configured. These contours are stored in the CSOManager and ghosting is activated in the SoCSOVisualizationSettings.\nGhosting means not only showing contours available on the currently visible slice but also contours of the neighbouring slices with increasing transparency.\nThe contours are also displayed in a three-dimensionsl SoExaminerViewer by using the SoCSO3DRenderer.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/visualization/example2/","title":"Creating a magnifier","summary":"Visualization Example 2: Creating a magnifier This example shows how to create a magnifier. Using the module SubImage a fraction of the original image can be extracted and enlarged. Download You can download the example network here","content":"Visualization Example 2: Creating a magnifier This example shows how to create a magnifier. Using the module SubImage a fraction of the original image can be extracted and enlarged. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/basic_mechanisms/viewer_application/","title":"Creating a simple application","summary":"Example 3: Creating a simple application In this example, you will learn how to create a simple prototype application in MeVisLab including a user interface (UI) with 2D and 3D viewers.\nDownload You can download the example network here","content":"Example 3: Creating a simple application In this example, you will learn how to create a simple prototype application in MeVisLab including a user interface (UI) with 2D and 3D viewers.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/contours/example1/","title":"Creation of Contours","summary":"Contour Example 1: Creation of contours Contours are stored as Contour Segmented Objects (CSOs) in MeVisLab. This example highlights ways of creating CSOs using modules of the SoCSOEditor group.\nInfo:\u0026nbsp; You may want to look at the glossary entry on CSOs. The SoCSOEditor module group contains several modules, some of which are listed right below:\nSoCSOPointEditor SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor SoCSODistancePolylineEditor SoCSOEllipseEditor SoCSORectangleEditor SoCSOSplineEditor SoCSOPolygonEditor SoCSOIsoEditor SoCSOLiveWireEditor Info:\u0026nbsp; Whenever Contour Segmented Objects are created, they are temporarily stored by and can be managed with the CSOManager.","content":"Contour Example 1: Creation of contours Contours are stored as Contour Segmented Objects (CSOs) in MeVisLab. This example highlights ways of creating CSOs using modules of the SoCSOEditor group.\nInfo:\u0026nbsp; You may want to look at the glossary entry on CSOs. The SoCSOEditor module group contains several modules, some of which are listed right below:\nSoCSOPointEditor SoCSOAngleEditor SoCSOArrowEditor SoCSODistanceLineEditor SoCSODistancePolylineEditor SoCSOEllipseEditor SoCSORectangleEditor SoCSOSplineEditor SoCSOPolygonEditor SoCSOIsoEditor SoCSOLiveWireEditor Info:\u0026nbsp; Whenever Contour Segmented Objects are created, they are temporarily stored by and can be managed with the CSOManager. In this example, contours are created and colors and styles of these CSOs are customized by using the SoCSOVisualizationSettings module.\nSummary Contours are stored as their own abstract data type called Contour Segmented Objects (often abbreviated to CSO). The SoEditor module group contains several useful modules to create, interact with or modify CSOs. Created CSOs are temporarily stored and can be managed using the CSOManager. Download The example network can be downloaded here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/surface_objects/example1/","title":"Creation of WEMs","summary":"Surface Example 1: Creation of WEMs This example shows how to create WEMs out of voxel images and CSOs. Download You can download the example network here","content":"Surface Example 1: Creation of WEMs This example shows how to create WEMs out of voxel images and CSOs. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/visualization/example4/","title":"Display images converted to Open Inventor scene objects","summary":"Visualization Example 4: Display images converted to Open Inventor scene objects This example shows how to convert images to Open Inventor scene objects using the module SoView2D and modules based on SoView2D. Download You can download the example network here","content":"Visualization Example 4: Display images converted to Open Inventor scene objects This example shows how to convert images to Open Inventor scene objects using the module SoView2D and modules based on SoView2D. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/markers/example1/","title":"Distance between markers","summary":"Marker Example 1: Distance between markers This examples shows how to create markers in a viewer and measure their distance. Download You can download the example network here","content":"Marker Example 1: Distance between markers This examples shows how to create markers in a viewer and measure their distance. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/curves/example1/","title":"Drawing curves","summary":"Curves Example: Drawing curves This examples shows how to create and render curves. Download You can download the example network here","content":"Curves Example: Drawing curves This examples shows how to create and render curves. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/visualization/example3/","title":"Image overlays","summary":"Visualization Example 3: Image overlays This example shows the creation of an overlay. Using the module SoView2DOverlay, an overlay can be blended over a 2D image. Download You can download the example network here","content":"Visualization Example 3: Image overlays This example shows the creation of an overlay. Using the module SoView2DOverlay, an overlay can be blended over a 2D image. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/surface_objects/example4/","title":"Interactively moving WEM","summary":"Surface Example 4: Interactively moving WEM This example shows how to use dragger modules, to modify objects in a 3D viewer. Download You can download the example network here","content":"Surface Example 4: Interactively moving WEM This example shows how to use dragger modules, to modify objects in a 3D viewer. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/image_processing/example2/","title":"Masking images","summary":"Image Processing Example 2: Masking images In this example, we create a simple mask on an image, so that background pixels are not affected by changes of the window/level values.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is shown in the right viewer of the SynchroView2D but with a Threshold based Mask.\nDownload You can download the example network here","content":"Image Processing Example 2: Masking images In this example, we create a simple mask on an image, so that background pixels are not affected by changes of the window/level values.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is shown in the right viewer of the SynchroView2D but with a Threshold based Mask.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/open_inventor/example2/","title":"Mouse interactions in an Open Inventor scene","summary":"Open Inventor Example 2: Mouse interactions in an Open Inventor scene This example shows how to implement object interactions.\nSummary A SoExaminerViewer is used to render a SoCube object. The SoMouseGrabber is used to identify mouse interactions in the viewer and to resize the cube.\nDownload You can download the example network here","content":"Open Inventor Example 2: Mouse interactions in an Open Inventor scene This example shows how to implement object interactions.\nSummary A SoExaminerViewer is used to render a SoCube object. The SoMouseGrabber is used to identify mouse interactions in the viewer and to resize the cube.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/open_inventor/example1/","title":"Open Inventor objects","summary":"Open Inventor Example 1: Open Inventor objects In this example a simple Open Inventor scene is created. The Open Inventor scene shows three objects of different color and shape.\nSummary A SoExaminerViewer is used to render open inventor scenes in 3D. The SoBackground module defines the background of the whole scene.\nThree 3D objects are created (SoCone, SoSphere and SoCube) having a defined SoMaterial module for setting the DiffuseColor of the object.","content":"Open Inventor Example 1: Open Inventor objects In this example a simple Open Inventor scene is created. The Open Inventor scene shows three objects of different color and shape.\nSummary A SoExaminerViewer is used to render open inventor scenes in 3D. The SoBackground module defines the background of the whole scene.\nThree 3D objects are created (SoCone, SoSphere and SoCube) having a defined SoMaterial module for setting the DiffuseColor of the object. The cube and the cone are also transformed by a SoTransform module so that they are located next to the centered sphere.\nIn the end, all three objects including their materials and transformations are added to the SoExaminerViewer by a SoGroup.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/basic_mechanisms/macro_modules_and_module_interaction/example1/","title":"Panel for the contour filter","summary":"Example 1: Panel for the contour filter This example contains a whole package structure. Inside you can find the example contour filter for which a panel was created.\nSummary A new macro module Filter has been created. Initially macro modules do not provide an own panel containing user interface elements such as buttons. The Automatic Panel is shown on double-clicking the module providing the name of the module.\nIn this example we update the *.","content":"Example 1: Panel for the contour filter This example contains a whole package structure. Inside you can find the example contour filter for which a panel was created.\nSummary A new macro module Filter has been created. Initially macro modules do not provide an own panel containing user interface elements such as buttons. The Automatic Panel is shown on double-clicking the module providing the name of the module.\nIn this example we update the *.script file of the Filter module to display the kernel selection field of the Convolution module within its network.\nInfo:\u0026nbsp; Changes applied to fields in the macro module\u0026rsquo;s panel are applied to their internal network as well. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/surface_objects/example2/","title":"Processing and modification of WEMs","summary":"Surface Example 2: Processing and modification of WEMs This example shows how to process and modify WEMs using the modules WEMModify, WEMSmooth and WEMSurfaceDistance. Download You can download the example network here","content":"Surface Example 2: Processing and modification of WEMs This example shows how to process and modify WEMs using the modules WEMModify, WEMSmooth and WEMSurfaceDistance. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/basic_mechanisms/macro_modules_and_module_interaction/example2/","title":"Python scripting","summary":"Example 2: Python scripting This example shows how to create module interactions via Python scripting.\nSummary A new macro module IsoCSOs is created providing two viewers in its internal network, View2D and SoExaminerViewer. Both viewers are included in the panel of the module.\nTo showcase how Python functions can be implemented in MeVisLab and called from within a module, additional buttons to browse directories and create contours via the CSOIsoGenerator are added.","content":"Example 2: Python scripting This example shows how to create module interactions via Python scripting.\nSummary A new macro module IsoCSOs is created providing two viewers in its internal network, View2D and SoExaminerViewer. Both viewers are included in the panel of the module.\nTo showcase how Python functions can be implemented in MeVisLab and called from within a module, additional buttons to browse directories and create contours via the CSOIsoGenerator are added. Lastly a field listener is implemented reacting to field changes by colorizing contours when the user hovers over them with the mouse.\nDownload The files need to be added to a package. You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/image_processing/example3/","title":"Region Growing (Segmentation)","summary":"Image Processing Example 3: Region Growing (Segmentation) In this example, we create a simple mask on an image by using the RegionGrowing module.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is used as input for the RegionGrowing module. The starting point for the algorithm is a list of markers created by the SoView2DMarkerEditor. As the RegionGrowing may leave gaps, an additional CloseGap module is added.","content":"Image Processing Example 3: Region Growing (Segmentation) In this example, we create a simple mask on an image by using the RegionGrowing module.\nSummary We are loading images by using the LocalImage module and show them in a SynchroView2D. The same image is used as input for the RegionGrowing module. The starting point for the algorithm is a list of markers created by the SoView2DMarkerEditor. As the RegionGrowing may leave gaps, an additional CloseGap module is added. The resulting segmentation mask is shown as an overlay on the original image via SoView2DOverlay.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/image_processing/example4/","title":"Subtract 3D objects","summary":"Image Processing Example 4: Subtract 3D objects In this example, we subtract a sphere from another WEM.\nSummary We are loading images by using the LocalImage module and render them as a 3D scene in a SoExaminerViewer. We also add a sphere which is then subtracted from the original image.\nDownload You can download the example network here","content":"Image Processing Example 4: Subtract 3D objects In this example, we subtract a sphere from another WEM.\nSummary We are loading images by using the LocalImage module and render them as a 3D scene in a SoExaminerViewer. We also add a sphere which is then subtracted from the original image.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/visualization/example1/","title":"Synchronous view of two images","summary":"Visualization Example 1: Synchronous view of two images This very simple example shows how to load an image and apply a basic Convolution filter to the image. The image with and without filter is shown in a Viewer and scrolling is synchronized so that the same slice is shown for both images.\nDownload You can download the example network here","content":"Visualization Example 1: Synchronous view of two images This very simple example shows how to load an image and apply a basic Convolution filter to the image. The image with and without filter is shown in a Viewer and scrolling is synchronized so that the same slice is shown for both images.\nDownload You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/visualization/example5/","title":"Volume rendering and interactions","summary":"Visualization Example 5: Volume rendering and interactions This example shows the volume rendering of a scan. The texture of the volume is edited and animations are implemented. Download You can download the example network here","content":"Visualization Example 5: Volume rendering and interactions This example shows the volume rendering of a scan. The texture of the volume is edited and animations are implemented. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/visualization/example6/","title":"Volume Rendering vs. Path Tracing","summary":"Visualization Example 6.1: Volume Rendering vs. Path Tracing This example shows a comparison between Volume Rendering and Path Tracing. The same scene is rendered and the camera interactions in both viewers are synchronized. Download You can download the example network here","content":"Visualization Example 6.1: Volume Rendering vs. Path Tracing This example shows a comparison between Volume Rendering and Path Tracing. The same scene is rendered and the camera interactions in both viewers are synchronized. Download You can download the example network here\n","tags":[],"section":"examples"},{"date":"-62135596800","url":"https://mevislab.github.io/examples/examples/data_objects/surface_objects/example5/","title":"WEM - Primitive Value Lists","summary":"Surface Example 5: WEM - Primitive Value Lists This example shows how to use Primitive Value Lists (PVLs). With the help of PVLs the distance between the surfaces of WEMs is color coded. Download You can download the example network here","content":"Surface Example 5: WEM - Primitive Value Lists This example shows how to use Primitive Value Lists (PVLs). With the help of PVLs the distance between the surfaces of WEMs is color coded. Download You can download the example network here\n","tags":[],"section":"examples"}]